{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afb6e8a",
   "metadata": {
    "_cell_guid": "8eaa51fb-b150-49f6-bc5d-8a18f4ae6951",
    "_uuid": "b8a4e7ae-075b-4b87-9407-f739a7928bb7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005191,
     "end_time": "2025-09-15T20:15:50.979493",
     "exception": false,
     "start_time": "2025-09-15T20:15:50.974302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Interpretable ML Benchmark\n",
    "NousNet (Fixed + Softmax Prototypes) vs XGBoost, EBM, MLP, KAN\n",
    "\n",
    "#\n",
    "Datasets:\n",
    "- FICO HELOC (classification)\n",
    "- Adult Income (classification)\n",
    "- Breast Cancer (classification)\n",
    "- California Housing (regression)\n",
    "\n",
    "#\n",
    "Protocol:\n",
    "- 5-fold cross-validation (Stratified for classification, KFold for regression)\n",
    "- Early stopping & best restore for neural models\n",
    "- Fixed seeds (42)\n",
    "\n",
    "#\n",
    "Models:\n",
    "- NousNet (Fixed rules)\n",
    "- NousNet (Softmax rules + calibrators + prototype head for classification)\n",
    "- XGBoost (trees + SHAP)\n",
    "- EBM (InterpretML glassbox)\n",
    "- MLP (PyTorch)\n",
    "- KAN: piecewise-linear univariate bases + shallow Kolmogorov-Arnold composition\n",
    "\n",
    "#\n",
    "Interpretability:\n",
    "- NousNet: honest LOO, MSE, fidelity-driven pruning, prototype analytics\n",
    "- XGBoost: SHAP (global/local)\n",
    "- EBM: global and local explanations\n",
    "- MLP: permutation importance\n",
    "- KAN: global importance from learned univariate bases + mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a37a21",
   "metadata": {
    "papermill": {
     "duration": 117.915844,
     "end_time": "2025-09-15T20:17:48.899669",
     "exception": false,
     "start_time": "2025-09-15T20:15:50.983825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nous@ git+https://github.com/EmotionEngineer/nous@main (from nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\n",
      "  Cloning https://github.com/EmotionEngineer/nous (to revision main) to /tmp/pip-install-hqtahse7/nous_024add0fe8744e379f47cf43f320c66f\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/EmotionEngineer/nous /tmp/pip-install-hqtahse7/nous_024add0fe8744e379f47cf43f320c66f\n",
      "  Resolved https://github.com/EmotionEngineer/nous to commit 608328f967895c47cbc511655b6c31d6fbe5c321\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.1 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.22 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.5 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=3.6 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.8.2)\n",
      "Requirement already satisfied: seaborn>=0.12 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.65 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (4.66.1)\n",
      "Requirement already satisfied: ucimlrepo>=0.0.5 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.0.7)\n",
      "Requirement already satisfied: pytest>=7.0 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (8.3.3)\n",
      "Requirement already satisfied: pytest-cov>=4.0 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (7.0.0)\n",
      "Requirement already satisfied: mypy>=1.5 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.18.1)\n",
      "Requirement already satisfied: ruff>=0.5 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.13.0)\n",
      "Requirement already satisfied: black>=23.0 in ./.local/lib/python3.10/site-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (25.1.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.local/lib/python3.10/site-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in ./.local/lib/python3.10/site-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in ./.local/lib/python3.10/site-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (23.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in ./.local/lib/python3.10/site-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in ./.local/lib/python3.10/site-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.10.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in ./.local/lib/python3.10/site-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in ./.local/lib/python3.10/site-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (4.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.5->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas>=1.5->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2023.3)\n",
      "Requirement already satisfied: iniconfig in ./.local/lib/python3.10/site-packages (from pytest>=7.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./.local/lib/python3.10/site-packages (from pytest>=7.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in ./.local/lib/python3.10/site-packages (from pytest>=7.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.1.3)\n",
      "Requirement already satisfied: coverage>=7.10.6 in ./.local/lib/python3.10/site-packages (from coverage[toml]>=7.10.6->pytest-cov>=4.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (7.10.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.local/lib/python3.10/site-packages (from scikit-learn>=1.2->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn>=1.2->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn>=1.2->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.2.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.local/lib/python3.10/site-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.local/lib/python3.10/site-packages (from triton==3.4.0->torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (80.9.0)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in ./.local/lib/python3.10/site-packages (from ucimlrepo>=0.0.5->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.1.3)\n",
      "\u001b[33mWARNING: Error parsing dependencies of gym: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    opencv-python (>=3.) ; extra == 'all'\n",
      "                  ~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: interpret in ./.local/lib/python3.10/site-packages (0.7.2)\n",
      "Requirement already satisfied: interpret-core==0.7.2 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.25 in ./.local/lib/python3.10/site-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19.2 in ./.local/lib/python3.10/site-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in ./.local/lib/python3.10/site-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in ./.local/lib/python3.10/site-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.2.0)\n",
      "Requirement already satisfied: dash<3.0.0,>=2.0.0 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.18.2)\n",
      "Requirement already satisfied: dash-cytoscape>=0.1.1 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.0.2)\n",
      "Requirement already satisfied: gevent>=1.3.6 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (25.8.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.6.2 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.9.4)\n",
      "Requirement already satisfied: plotly>=3.8.1 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.18.0)\n",
      "Requirement already satisfied: aplr>=10.6.1 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (10.11.1)\n",
      "Requirement already satisfied: ipykernel>=4.10.0 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (6.25.1)\n",
      "Requirement already satisfied: ipython>=5.5.0 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.15.0)\n",
      "Requirement already satisfied: SALib>=1.3.3 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.5.1)\n",
      "Requirement already satisfied: shap>=0.28.5 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.48.0)\n",
      "Requirement already satisfied: dill>=0.2.5 in ./.local/lib/python3.10/site-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.3.7)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.2.5)\n",
      "Requirement already satisfied: Werkzeug<3.1 in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.0.1)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.0.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.0.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.6.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.11.0)\n",
      "Requirement already satisfied: retrying in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.4.2)\n",
      "Requirement already satisfied: nest-asyncio in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.5.7)\n",
      "Requirement already satisfied: setuptools in ./.local/lib/python3.10/site-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (80.9.0)\n",
      "Requirement already satisfied: greenlet>=3.2.2 in ./.local/lib/python3.10/site-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.2.4)\n",
      "Requirement already satisfied: zope.event in ./.local/lib/python3.10/site-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (6.0)\n",
      "Requirement already satisfied: zope.interface in ./.local/lib/python3.10/site-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.6.7.post1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.3.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.3.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.1.6)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (23.1)\n",
      "Requirement already satisfied: pyzmq>=20 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (25.1.1)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./.local/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.9.0)\n",
      "Requirement already satisfied: backcall in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.19.0)\n",
      "Requirement already satisfied: pickleshare in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.local/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.local/lib/python3.10/site-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2024.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.5 in ./.local/lib/python3.10/site-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.8.2)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.70.15)\n",
      "Requirement already satisfied: scipy>=1.9.3 in ./.local/lib/python3.10/site-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn>=0.18.1->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in ./.local/lib/python3.10/site-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.66.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in ./.local/lib/python3.10/site-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in ./.local/lib/python3.10/site-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.58.1)\n",
      "Requirement already satisfied: cloudpickle in ./.local/lib/python3.10/site-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.6.0)\n",
      "Requirement already satisfied: Jinja2>=3.0 in ./.local/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in ./.local/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in ./.local/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.1.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.local/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in ./.local/lib/python3.10/site-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.4.7)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in ./.local/lib/python3.10/site-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.41.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.10/site-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.3.0)\n",
      "Requirement already satisfied: pure-eval in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.2.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of gym: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    opencv-python (>=3.) ; extra == 'all'\n",
      "                  ~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in ./.local/lib/python3.10/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.local/lib/python3.10/site-packages (from xgboost) (2.27.3)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.10/site-packages (from xgboost) (1.13.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of gym: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    opencv-python (>=3.) ; extra == 'all'\n",
      "                  ~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-cov>=4.0 (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mypy>=1.5 (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading mypy-1.18.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: ruff>=0.5 in /usr/local/lib/python3.11/dist-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.12.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting black>=23.0 (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.7.2)\r\n",
      "Requirement already satisfied: seaborn>=0.12 in /usr/local/lib/python3.11/dist-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.12.2)\r\n",
      "Requirement already satisfied: tqdm>=4.65 in /usr/local/lib/python3.11/dist-packages (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (4.67.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo>=0.0.5 (from nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (8.2.1)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.1.0)\r\n",
      "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (25.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathspec>=0.9.0 (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\r\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black>=23.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (4.3.8)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (4.58.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.4.8)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.9.0.post0)\r\n",
      "Requirement already satisfied: typing_extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from mypy>=1.5->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (4.14.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.4.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2025.2)\r\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=7.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.1.0)\r\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=7.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov>=4.0->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading coverage-7.10.6-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.9 kB)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.18.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2025.5.1)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.3.0)\r\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo>=0.0.5->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2025.6.15)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22->nous@ git+https://github.com/EmotionEngineer/nous@main->nous[dev,examples]@ git+https://github.com/EmotionEngineer/nous@main) (2024.2.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mypy-1.18.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/13.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/13.2 MB\u001b[0m \u001b[31m188.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m196.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m196.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytest_cov-7.0.0-py3-none-any.whl (22 kB)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/363.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/363.4 MB\u001b[0m \u001b[31m227.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/363.4 MB\u001b[0m \u001b[31m173.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/363.4 MB\u001b[0m \u001b[31m167.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/363.4 MB\u001b[0m \u001b[31m211.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/363.4 MB\u001b[0m \u001b[31m217.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/363.4 MB\u001b[0m \u001b[31m217.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/363.4 MB\u001b[0m \u001b[31m213.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/363.4 MB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/363.4 MB\u001b[0m \u001b[31m150.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/363.4 MB\u001b[0m \u001b[31m208.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/363.4 MB\u001b[0m \u001b[31m209.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.6/363.4 MB\u001b[0m \u001b[31m219.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/363.4 MB\u001b[0m \u001b[31m216.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/363.4 MB\u001b[0m \u001b[31m216.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/363.4 MB\u001b[0m \u001b[31m215.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/363.4 MB\u001b[0m \u001b[31m219.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/363.4 MB\u001b[0m \u001b[31m220.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/363.4 MB\u001b[0m \u001b[31m220.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/363.4 MB\u001b[0m \u001b[31m220.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/363.4 MB\u001b[0m \u001b[31m221.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/363.4 MB\u001b[0m \u001b[31m223.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/363.4 MB\u001b[0m \u001b[31m203.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/363.4 MB\u001b[0m \u001b[31m192.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.2/363.4 MB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/363.4 MB\u001b[0m \u001b[31m218.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.5/363.4 MB\u001b[0m \u001b[31m219.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/363.4 MB\u001b[0m \u001b[31m220.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.9/363.4 MB\u001b[0m \u001b[31m221.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/363.4 MB\u001b[0m \u001b[31m220.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.1/363.4 MB\u001b[0m \u001b[31m219.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/363.4 MB\u001b[0m \u001b[31m218.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.3/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/363.4 MB\u001b[0m \u001b[31m209.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/363.4 MB\u001b[0m \u001b[31m216.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m251.8/363.4 MB\u001b[0m \u001b[31m211.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m251.9/363.4 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/363.4 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m252.1/363.4 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m252.3/363.4 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m253.3/363.4 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m256.2/363.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m258.8/363.4 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m265.5/363.4 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m272.1/363.4 MB\u001b[0m \u001b[31m192.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m278.9/363.4 MB\u001b[0m \u001b[31m193.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m285.5/363.4 MB\u001b[0m \u001b[31m193.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m292.4/363.4 MB\u001b[0m \u001b[31m191.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m299.1/363.4 MB\u001b[0m \u001b[31m194.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m305.8/363.4 MB\u001b[0m \u001b[31m195.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m312.6/363.4 MB\u001b[0m \u001b[31m193.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m318.9/363.4 MB\u001b[0m \u001b[31m184.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m325.4/363.4 MB\u001b[0m \u001b[31m181.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m331.8/363.4 MB\u001b[0m \u001b[31m193.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m335.8/363.4 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m342.5/363.4 MB\u001b[0m \u001b[31m177.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m349.3/363.4 MB\u001b[0m \u001b[31m197.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m356.3/363.4 MB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/13.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/13.8 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10.7/13.8 MB\u001b[0m \u001b[31m155.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m12.3/13.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/24.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/24.6 MB\u001b[0m \u001b[31m221.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/24.6 MB\u001b[0m \u001b[31m202.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m19.9/24.6 MB\u001b[0m \u001b[31m178.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m188.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m188.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m188.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/883.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/664.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/664.8 MB\u001b[0m \u001b[31m213.8 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/664.8 MB\u001b[0m \u001b[31m209.9 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/664.8 MB\u001b[0m \u001b[31m206.8 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.7/664.8 MB\u001b[0m \u001b[31m207.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/664.8 MB\u001b[0m \u001b[31m195.9 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/664.8 MB\u001b[0m \u001b[31m212.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/664.8 MB\u001b[0m \u001b[31m216.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/664.8 MB\u001b[0m \u001b[31m215.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/664.8 MB\u001b[0m \u001b[31m218.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.8/664.8 MB\u001b[0m \u001b[31m211.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/664.8 MB\u001b[0m \u001b[31m213.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/664.8 MB\u001b[0m \u001b[31m196.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/664.8 MB\u001b[0m \u001b[31m193.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/664.8 MB\u001b[0m \u001b[31m194.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/664.8 MB\u001b[0m \u001b[31m188.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/664.8 MB\u001b[0m \u001b[31m188.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/664.8 MB\u001b[0m \u001b[31m188.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/664.8 MB\u001b[0m \u001b[31m192.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/664.8 MB\u001b[0m \u001b[31m194.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/664.8 MB\u001b[0m \u001b[31m194.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/664.8 MB\u001b[0m \u001b[31m194.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.4/664.8 MB\u001b[0m \u001b[31m191.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/664.8 MB\u001b[0m \u001b[31m194.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.1/664.8 MB\u001b[0m \u001b[31m197.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/664.8 MB\u001b[0m \u001b[31m196.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/664.8 MB\u001b[0m \u001b[31m197.2 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/664.8 MB\u001b[0m \u001b[31m194.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/664.8 MB\u001b[0m \u001b[31m190.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.7/664.8 MB\u001b[0m \u001b[31m194.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/664.8 MB\u001b[0m \u001b[31m192.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.8/664.8 MB\u001b[0m \u001b[31m191.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/664.8 MB\u001b[0m \u001b[31m192.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/664.8 MB\u001b[0m \u001b[31m193.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/664.8 MB\u001b[0m \u001b[31m192.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.8/664.8 MB\u001b[0m \u001b[31m193.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/664.8 MB\u001b[0m \u001b[31m198.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.3/664.8 MB\u001b[0m \u001b[31m193.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/664.8 MB\u001b[0m \u001b[31m193.3 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/664.8 MB\u001b[0m \u001b[31m195.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/664.8 MB\u001b[0m \u001b[31m195.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/664.8 MB\u001b[0m \u001b[31m194.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/664.8 MB\u001b[0m \u001b[31m195.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.7/664.8 MB\u001b[0m \u001b[31m190.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/664.8 MB\u001b[0m \u001b[31m189.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.5/664.8 MB\u001b[0m \u001b[31m182.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/664.8 MB\u001b[0m \u001b[31m177.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/664.8 MB\u001b[0m \u001b[31m180.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/664.8 MB\u001b[0m \u001b[31m181.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.6/664.8 MB\u001b[0m \u001b[31m183.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.0/664.8 MB\u001b[0m \u001b[31m184.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.3/664.8 MB\u001b[0m \u001b[31m182.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.6/664.8 MB\u001b[0m \u001b[31m182.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.9/664.8 MB\u001b[0m \u001b[31m181.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.3/664.8 MB\u001b[0m \u001b[31m185.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.7/664.8 MB\u001b[0m \u001b[31m184.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.9/664.8 MB\u001b[0m \u001b[31m184.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.1/664.8 MB\u001b[0m \u001b[31m182.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.4/664.8 MB\u001b[0m \u001b[31m181.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.6/664.8 MB\u001b[0m \u001b[31m180.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.1/664.8 MB\u001b[0m \u001b[31m186.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/664.8 MB\u001b[0m \u001b[31m180.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.4/664.8 MB\u001b[0m \u001b[31m175.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.6/664.8 MB\u001b[0m \u001b[31m180.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/664.8 MB\u001b[0m \u001b[31m181.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m433.4/664.8 MB\u001b[0m \u001b[31m186.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m439.7/664.8 MB\u001b[0m \u001b[31m184.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m445.9/664.8 MB\u001b[0m \u001b[31m181.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m452.3/664.8 MB\u001b[0m \u001b[31m182.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m458.6/664.8 MB\u001b[0m \u001b[31m182.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m465.0/664.8 MB\u001b[0m \u001b[31m184.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m471.4/664.8 MB\u001b[0m \u001b[31m183.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m477.5/664.8 MB\u001b[0m \u001b[31m181.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m483.9/664.8 MB\u001b[0m \u001b[31m182.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m490.2/664.8 MB\u001b[0m \u001b[31m184.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m496.5/664.8 MB\u001b[0m \u001b[31m183.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m502.6/664.8 MB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m508.9/664.8 MB\u001b[0m \u001b[31m180.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m515.4/664.8 MB\u001b[0m \u001b[31m185.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m521.5/664.8 MB\u001b[0m \u001b[31m180.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m527.7/664.8 MB\u001b[0m \u001b[31m178.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m534.3/664.8 MB\u001b[0m \u001b[31m185.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m540.7/664.8 MB\u001b[0m \u001b[31m185.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m547.1/664.8 MB\u001b[0m \u001b[31m186.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m553.6/664.8 MB\u001b[0m \u001b[31m187.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m560.1/664.8 MB\u001b[0m \u001b[31m188.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m566.5/664.8 MB\u001b[0m \u001b[31m186.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m572.6/664.8 MB\u001b[0m \u001b[31m180.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m578.8/664.8 MB\u001b[0m \u001b[31m177.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m585.2/664.8 MB\u001b[0m \u001b[31m182.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m591.5/664.8 MB\u001b[0m \u001b[31m181.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m599.2/664.8 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m606.7/664.8 MB\u001b[0m \u001b[31m215.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m614.0/664.8 MB\u001b[0m \u001b[31m212.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m621.8/664.8 MB\u001b[0m \u001b[31m218.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m629.4/664.8 MB\u001b[0m \u001b[31m219.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m637.1/664.8 MB\u001b[0m \u001b[31m221.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m644.7/664.8 MB\u001b[0m \u001b[31m218.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m652.3/664.8 MB\u001b[0m \u001b[31m218.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m659.8/664.8 MB\u001b[0m \u001b[31m216.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/211.5 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/211.5 MB\u001b[0m \u001b[31m145.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/211.5 MB\u001b[0m \u001b[31m189.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/211.5 MB\u001b[0m \u001b[31m187.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.3/211.5 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/211.5 MB\u001b[0m \u001b[31m150.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/211.5 MB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/211.5 MB\u001b[0m \u001b[31m183.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/211.5 MB\u001b[0m \u001b[31m185.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/211.5 MB\u001b[0m \u001b[31m183.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/211.5 MB\u001b[0m \u001b[31m184.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/211.5 MB\u001b[0m \u001b[31m184.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/211.5 MB\u001b[0m \u001b[31m199.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/211.5 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/211.5 MB\u001b[0m \u001b[31m141.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/211.5 MB\u001b[0m \u001b[31m183.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/211.5 MB\u001b[0m \u001b[31m190.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/211.5 MB\u001b[0m \u001b[31m190.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/211.5 MB\u001b[0m \u001b[31m180.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/211.5 MB\u001b[0m \u001b[31m170.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.5/211.5 MB\u001b[0m \u001b[31m181.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/211.5 MB\u001b[0m \u001b[31m185.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/211.5 MB\u001b[0m \u001b[31m183.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/211.5 MB\u001b[0m \u001b[31m181.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m151.0/211.5 MB\u001b[0m \u001b[31m185.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m157.3/211.5 MB\u001b[0m \u001b[31m184.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m164.5/211.5 MB\u001b[0m \u001b[31m200.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m167.3/211.5 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m174.7/211.5 MB\u001b[0m \u001b[31m144.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m182.1/211.5 MB\u001b[0m \u001b[31m216.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m189.6/211.5 MB\u001b[0m \u001b[31m216.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m197.1/211.5 MB\u001b[0m \u001b[31m215.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m199.2/211.5 MB\u001b[0m \u001b[31m140.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m202.4/211.5 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m210.0/211.5 MB\u001b[0m \u001b[31m154.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/56.3 MB\u001b[0m \u001b[31m202.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/56.3 MB\u001b[0m \u001b[31m189.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/56.3 MB\u001b[0m \u001b[31m193.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/56.3 MB\u001b[0m \u001b[31m194.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/56.3 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/56.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m39.5/56.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m46.1/56.3 MB\u001b[0m \u001b[31m190.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m53.1/56.3 MB\u001b[0m \u001b[31m202.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/127.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/127.9 MB\u001b[0m \u001b[31m200.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/127.9 MB\u001b[0m \u001b[31m210.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/127.9 MB\u001b[0m \u001b[31m208.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/127.9 MB\u001b[0m \u001b[31m218.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/127.9 MB\u001b[0m \u001b[31m218.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/127.9 MB\u001b[0m \u001b[31m222.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/127.9 MB\u001b[0m \u001b[31m220.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/127.9 MB\u001b[0m \u001b[31m224.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/127.9 MB\u001b[0m \u001b[31m220.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/127.9 MB\u001b[0m \u001b[31m192.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/127.9 MB\u001b[0m \u001b[31m188.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/127.9 MB\u001b[0m \u001b[31m211.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m95.4/127.9 MB\u001b[0m \u001b[31m218.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m100.5/127.9 MB\u001b[0m \u001b[31m174.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m107.1/127.9 MB\u001b[0m \u001b[31m172.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m112.0/127.9 MB\u001b[0m \u001b[31m166.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m116.0/127.9 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m120.6/127.9 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/207.5 MB\u001b[0m \u001b[31m207.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/207.5 MB\u001b[0m \u001b[31m210.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/207.5 MB\u001b[0m \u001b[31m211.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/207.5 MB\u001b[0m \u001b[31m212.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.5/207.5 MB\u001b[0m \u001b[31m215.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/207.5 MB\u001b[0m \u001b[31m217.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/207.5 MB\u001b[0m \u001b[31m217.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/207.5 MB\u001b[0m \u001b[31m214.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/207.5 MB\u001b[0m \u001b[31m217.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/207.5 MB\u001b[0m \u001b[31m217.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/207.5 MB\u001b[0m \u001b[31m217.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/207.5 MB\u001b[0m \u001b[31m224.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.0/207.5 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.3/207.5 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/207.5 MB\u001b[0m \u001b[31m218.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/207.5 MB\u001b[0m \u001b[31m220.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/207.5 MB\u001b[0m \u001b[31m216.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/207.5 MB\u001b[0m \u001b[31m217.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.4/207.5 MB\u001b[0m \u001b[31m219.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/207.5 MB\u001b[0m \u001b[31m218.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m147.4/207.5 MB\u001b[0m \u001b[31m217.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m155.1/207.5 MB\u001b[0m \u001b[31m219.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m162.5/207.5 MB\u001b[0m \u001b[31m214.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m169.8/207.5 MB\u001b[0m \u001b[31m211.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m177.5/207.5 MB\u001b[0m \u001b[31m212.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m185.0/207.5 MB\u001b[0m \u001b[31m216.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m192.5/207.5 MB\u001b[0m \u001b[31m216.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m200.0/207.5 MB\u001b[0m \u001b[31m219.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/21.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/21.1 MB\u001b[0m \u001b[31m205.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/21.1 MB\u001b[0m \u001b[31m204.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m217.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m217.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading coverage-7.10.6-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (249 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/249.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: nous\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for nous (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for nous: filename=nous-0.2.0-py3-none-any.whl size=43789 sha256=a986bbe8ead0ecaa5feeb0777062dd2c0a52a2d633a5d0208e0e6368d891a006\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dcv6fscu/wheels/5c/d9/99/a5c8f301bd5928ee051cc5c7535892bdf567557bb0affb43ab\r\n",
      "Successfully built nous\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pathspec, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, coverage, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mypy, black, pytest-cov, nvidia-cusolver-cu12, ucimlrepo, nous\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cufft-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: coverage\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: coverage 7.9.1\r\n",
      "    Uninstalling coverage-7.9.1:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled coverage-7.9.1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed black-25.1.0 coverage-7.10.6 mypy-1.18.1 nous-0.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pathspec-0.12.1 pytest-cov-7.0.0 ucimlrepo-0.0.7\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting interpret\r\n",
      "  Downloading interpret-0.7.2-py3-none-any.whl.metadata (1.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting interpret-core==0.7.2 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading interpret_core-0.7.2-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.26.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.2.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.5.1)\r\n",
      "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (7.0.0)\r\n",
      "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (6.17.1)\r\n",
      "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (7.34.0)\r\n",
      "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.24.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SALib>=1.3.3 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading salib-1.5.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.44.1)\r\n",
      "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.3.8)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aplr>=10.6.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading aplr-10.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dash<3.0.0,>=2.0.0 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dash-cytoscape>=0.1.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading dash_cytoscape-1.0.2.tar.gz (4.0 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m179.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gevent>=1.3.6 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading gevent-25.8.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.32.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask<3.1,>=1.0.4 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Werkzeug<3.1 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.7.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.14.0)\r\n",
      "Collecting retrying (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.6.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (75.2.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.2.3)\r\n",
      "Collecting zope.event (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Downloading zope_event-6.0-py3-none-any.whl.metadata (5.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zope.interface (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading zope_interface-8.0-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (44 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.8.0)\r\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.6.3)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.1.7)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (25.0)\r\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (24.0.1)\r\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (6.5.1)\r\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.7.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.19.2)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.4.2)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.0.51)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.19.2)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.9.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2025.2)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.5.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2025.6.15)\r\n",
      "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.7.2)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.70.16)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.15.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.6.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.67.1)\r\n",
      "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.0.7)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.60.0)\r\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.1.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.1.6)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2.2.0)\r\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (8.2.1)\r\n",
      "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.9.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.8.4)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (5.8.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.4.8)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.0.9)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.2.13)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.0.2)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (3.23.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2022.2.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2024.2.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (0.43.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret)\r\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.25->interpret-core==0.7.2->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (2024.2.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.2->interpret) (4.3.8)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading interpret-0.7.2-py3-none-any.whl (1.4 kB)\r\n",
      "Downloading interpret_core-0.7.2-py3-none-any.whl (16.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/16.5 MB\u001b[0m \u001b[31m150.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/16.5 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/16.5 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/16.5 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/16.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/16.5 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/16.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/16.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m15.9/16.5 MB\u001b[0m \u001b[31m174.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m174.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading aplr-10.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.0 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.0 MB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.0 MB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.0 MB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.0 MB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.0 MB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.0 MB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/7.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/7.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/7.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/7.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/7.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/7.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/7.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/7.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.2/7.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.2/7.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.2/7.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.2/7.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.2/7.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.2/7.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.2/7.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.3/7.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m7.3/7.8 MB\u001b[0m \u001b[31m221.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m211.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\r\n",
      "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\r\n",
      "Downloading gevent-25.8.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.1 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading salib-1.5.1-py3-none-any.whl (778 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/778.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/228.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zope_event-6.0-py3-none-any.whl (6.4 kB)\r\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\r\n",
      "Downloading zope_interface-8.0-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (259 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/259.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: dash-cytoscape\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for dash-cytoscape (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created wheel for dash-cytoscape: filename=dash_cytoscape-1.0.2-py3-none-any.whl size=4010717 sha256=b9b0beb7129cef3e56f1d7037c7f33516b11c586f8dd3104957635595b308364\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/b1/ab/6c999ab288b4849d372e23c0a8f6ece7edb7ffeb8c97959ab0\r\n",
      "Successfully built dash-cytoscape\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, setuptools, retrying, zope.interface, zope.event, Flask, gevent, dash, dash-cytoscape, SALib, interpret-core, aplr, interpret\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: Werkzeug\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: Werkzeug 3.1.3\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling Werkzeug-3.1.3:\r\n",
      "      Successfully uninstalled Werkzeug-3.1.3\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: setuptools\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: setuptools 75.2.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling setuptools-75.2.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled setuptools-75.2.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: Flask\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: Flask 3.1.1\r\n",
      "    Uninstalling Flask-3.1.1:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled Flask-3.1.1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed Flask-3.0.3 SALib-1.5.1 Werkzeug-3.0.6 aplr-10.11.1 dash-2.18.2 dash-core-components-2.0.0 dash-cytoscape-1.0.2 dash-html-components-2.0.0 dash-table-5.0.0 gevent-25.8.2 interpret-0.7.2 interpret-core-0.7.2 retrying-1.4.2 setuptools-80.9.0 zope.event-6.0 zope.interface-8.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pykan\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pykan-0.2.8-py3-none-any.whl.metadata (11 kB)\r\n",
      "Downloading pykan-0.2.8-py3-none-any.whl (78 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pykan\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed pykan-0.2.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"nous[dev,examples] @ git+https://github.com/EmotionEngineer/nous@main\"\n",
    "!pip install interpret\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af09f0e5",
   "metadata": {
    "_cell_guid": "63408444-f123-4fab-bba3-014f5dab491c",
    "_uuid": "6bfdbe70-56a4-49a0-9ed2-ce9b22453959",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.548957,
     "end_time": "2025-09-15T20:17:57.476025",
     "exception": false,
     "start_time": "2025-09-15T20:17:48.927068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, Dict, Any, List\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    ")\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Nous\n",
    "from nous import (\n",
    "    NousNet,\n",
    "    rule_impact_df,\n",
    "    minimal_sufficient_explanation,\n",
    "    select_pruning_threshold_global_bs,\n",
    "    generate_enhanced_explanation,\n",
    "    explanation_fidelity_metrics,\n",
    "    aggregator_mixture_report,\n",
    "    set_global_seed,\n",
    ")\n",
    "from nous.explain.traces import (\n",
    "    prototype_report_global,\n",
    "    prototype_contribution_df,\n",
    "    describe_prototype,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1281c15",
   "metadata": {
    "_cell_guid": "f31c4c37-7d3c-4be8-b50e-50556f1da4ba",
    "_uuid": "f51c2bce-c69e-4f94-9b3a-c227d050c2f9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026397,
     "end_time": "2025-09-15T20:17:57.530078",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.503681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b32f4a",
   "metadata": {
    "_cell_guid": "b7aef450-774d-47ed-b924-c69ab1dfc4d7",
    "_uuid": "ff0a650a-1241-49c7-8329-d42a40142351",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.04545,
     "end_time": "2025-09-15T20:17:57.601925",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.556475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    set_global_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def safe_auc(y_true: np.ndarray, proba: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Robust ROC AUC:\n",
    "      - Binary: use positive-class prob (proba[:, 1]) if available or 1D scores.\n",
    "      - Multiclass: use multi_class='ovr' with 2D proba.\n",
    "      - Returns np.nan on any mismatch or failure.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    if proba is None:\n",
    "        return np.nan\n",
    "    proba = np.asarray(proba)\n",
    "    try:\n",
    "        classes = np.unique(y_true)\n",
    "        if len(classes) == 2:\n",
    "            if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "                return roc_auc_score(y_true, proba[:, 1])\n",
    "            elif proba.ndim == 1:\n",
    "                return roc_auc_score(y_true, proba)\n",
    "            else:\n",
    "                return np.nan\n",
    "        else:\n",
    "            if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "                return roc_auc_score(y_true, proba, multi_class=\"ovr\")\n",
    "            return np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Simple PyTorch MLP with early stopping\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden: Tuple[int, ...] = (128, 64), task_type: str = \"classification\"):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [input_dim] + list(hidden)\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.extend([nn.Linear(dims[i], dims[i+1]), nn.ReLU()])\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.task = task_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp(\n",
    "    X_train: np.ndarray, y_train: np.ndarray,\n",
    "    X_val: np.ndarray, y_val: np.ndarray,\n",
    "    task_type: str, max_epochs: int = 1000, patience: int = 200, lr: float = 1e-3, weight_decay: float = 1e-4,\n",
    "    batch_size: int = 128\n",
    ") -> Tuple[MLPNet, float]:\n",
    "    seed_everything(42)\n",
    "    in_dim = X_train.shape[1]\n",
    "    out_dim = len(np.unique(y_train)) if task_type == \"classification\" else 1\n",
    "\n",
    "    model = MLPNet(in_dim, out_dim, hidden=(128, 64), task_type=task_type).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        ytr_t = torch.tensor(y_train, dtype=torch.long)\n",
    "        yval_t = torch.tensor(y_val, dtype=torch.long)\n",
    "    else:\n",
    "        criterion = nn.SmoothL1Loss(beta=1.0)\n",
    "        ytr_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "        yval_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    Xtr_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Xval_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(Xval_t, yval_t), batch_size=batch_size)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            if task_type == \"classification\":\n",
    "                loss = criterion(out, yb)\n",
    "            else:\n",
    "                out = out.squeeze(-1)\n",
    "                loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        total_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "                if task_type == \"classification\":\n",
    "                    vloss = criterion(out, yb)\n",
    "                else:\n",
    "                    vloss = criterion(out.squeeze(-1), yb)\n",
    "                total_val += float(vloss.item())\n",
    "        avg_val = total_val / max(1, len(val_loader))\n",
    "\n",
    "        if avg_val < best_val - 1e-6:\n",
    "            best_val = avg_val\n",
    "            epochs_no_improve = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_val\n",
    "\n",
    "def predict_mlp(model: MLPNet, X: np.ndarray, task_type: str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "        logits = model(X_t)\n",
    "        if task_type == \"classification\":\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "            return probs\n",
    "        else:\n",
    "            preds = logits.squeeze(-1).cpu().numpy()\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8960d",
   "metadata": {
    "_cell_guid": "d6906e04-1333-4374-a5e5-f3179a4398dd",
    "_uuid": "4be176c0-b724-499b-9185-2507bc229880",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026456,
     "end_time": "2025-09-15T20:17:57.655089",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.628633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "- HELOC: replace -9 with NaN, median-impute, standardize.\n",
    "- Adult: one-hot categoricals, impute, standardize.\n",
    "- Breast Cancer: standardize.\n",
    "- California Housing: standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0b7064",
   "metadata": {
    "_cell_guid": "bd3d498b-145b-444e-8371-b15b65e9393a",
    "_uuid": "357a7dfc-ccbd-4d3a-be17-6762cd7649e3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.041713,
     "end_time": "2025-09-15T20:17:57.772081",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.730368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_heloc() -> Tuple[pd.DataFrame, pd.Series, List[str], List[str]]:\n",
    "    urls = [\n",
    "        \"https://raw.githubusercontent.com/benoitparis/explainable-challenge/refs/heads/master/heloc_dataset_v1.csv\",\n",
    "    ]\n",
    "    df = None\n",
    "    for u in urls:\n",
    "        try:\n",
    "            df = pd.read_csv(u)\n",
    "            print(f\"Loaded HELOC from {u}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load from {u}: {e}\")\n",
    "    if df is None:\n",
    "        local = os.environ.get(\"HELCO_CSV_PATH\") or os.environ.get(\"HELOC_CSV_PATH\")\n",
    "        if local and os.path.exists(local):\n",
    "            df = pd.read_csv(local)\n",
    "            print(f\"Loaded HELOC from local file {local}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Cannot load HELOC dataset from mirrors. Set HELCO_CSV_PATH to local csv.\")\n",
    "\n",
    "    target_col = \"RiskPerformance\"\n",
    "    y_raw = df[target_col].astype(str).values\n",
    "    y = (y_raw == \"Bad\").astype(int)  # Bad=1, Good=0\n",
    "    X = df.drop(columns=[target_col]).copy()\n",
    "    X = X.replace(-9, np.nan)\n",
    "\n",
    "    num_cols = X.columns.tolist()\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[(\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    X_proc = ct.fit_transform(X)\n",
    "    Xp = pd.DataFrame(X_proc, columns=num_cols)\n",
    "    class_names = [\"Good\", \"Bad\"]\n",
    "    return Xp, pd.Series(y), num_cols, class_names\n",
    "\n",
    "def load_adult() -> Tuple[pd.DataFrame, pd.Series, List[str], List[str]]:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    adult = fetch_openml(\"adult\", version=2, as_frame=True, parser=\"auto\")\n",
    "    df = adult.frame.copy()\n",
    "    target = \"class\"\n",
    "    y_raw = df[target].astype(str)\n",
    "    y = (y_raw.str.contains(\">50K\")).astype(int)\n",
    "    X = df.drop(columns=[target])\n",
    "\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "            ]), cat_cols)\n",
    "        ]\n",
    "    )\n",
    "    Xp = preprocess.fit_transform(X)\n",
    "    cat_features = preprocess.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(cat_cols) if cat_cols else []\n",
    "    feature_names = num_cols + list(cat_features)\n",
    "    Xp = pd.DataFrame(Xp, columns=feature_names)\n",
    "    class_names = [\"<=50K\", \">50K\"]\n",
    "    return Xp, pd.Series(y), feature_names, class_names\n",
    "\n",
    "def load_breast_cancer_ds() -> Tuple[pd.DataFrame, pd.Series, List[str], List[str]]:\n",
    "    data = load_breast_cancer(as_frame=True)\n",
    "    X = data.frame.drop(columns=[\"target\"])\n",
    "    y = data.frame[\"target\"]\n",
    "    num_cols = X.columns.tolist()\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[(\"num\", Pipeline([\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols)]\n",
    "    )\n",
    "    Xp = ct.fit_transform(X)\n",
    "    Xp = pd.DataFrame(Xp, columns=num_cols)\n",
    "    class_names = list(data.target_names)\n",
    "    return Xp, y, num_cols, class_names\n",
    "\n",
    "def load_california() -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n",
    "    data = fetch_california_housing(as_frame=True)\n",
    "    X = data.frame.drop(columns=[\"MedHouseVal\"])\n",
    "    y = data.frame[\"MedHouseVal\"]\n",
    "    num_cols = X.columns.tolist()\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[(\"num\", Pipeline([\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols)]\n",
    "    )\n",
    "    Xp = ct.fit_transform(X)\n",
    "    Xp = pd.DataFrame(Xp, columns=num_cols)\n",
    "    return Xp, y, num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ddb57",
   "metadata": {
    "_cell_guid": "42f32541-9209-42df-b759-6be411f4ba56",
    "_uuid": "2244fb39-92ee-49db-bbea-ce671621c088",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026264,
     "end_time": "2025-09-15T20:17:57.825058",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.798794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## KAN\n",
    "- Two-stage Kolmogorov-Arnold-like network:\n",
    "  - Stage 1: univariate piecewise-linear bases per feature → u_j(x_j)\n",
    "  - Linear mixing s_r = Σ_j A[r,j] u_j\n",
    "  - Stage 2: univariate piecewise-linear bases over s_r → g_r(s_r)\n",
    "  - Output: y = W_out · g + b\n",
    "- Classification: logits dimension C; Regression: scalar.\n",
    "- Interpretability: report global feature importance via |W1| · |A| · |W_out|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5184da0a",
   "metadata": {
    "_cell_guid": "d71fec9f-7ba5-4423-8711-035b7da503aa",
    "_uuid": "66aff693-9b43-4ff4-8c11-b216d9b0be92",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.048859,
     "end_time": "2025-09-15T20:17:57.900186",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.851327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TentBasis1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Triangular (tent) basis over 1D input:\n",
    "      phi_k(x) = relu(1 - |x - c_k| / h), with centers c_k and bandwidth h.\n",
    "    \"\"\"\n",
    "    def __init__(self, centers: torch.Tensor, bandwidth: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"centers\", centers.clone().detach())  # [K]\n",
    "        self.bandwidth = float(bandwidth)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B] -> returns [B, K]\n",
    "        x = x.unsqueeze(-1)  # [B, 1]\n",
    "        dist = torch.abs(x - self.centers)  # [B, K]\n",
    "        return torch.relu(1.0 - dist / (self.bandwidth + 1e-8))\n",
    "\n",
    "\n",
    "class ManualKAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Manual KAN-like model with:\n",
    "      - per-feature univariate tent bases (K1 centers over standardized features),\n",
    "      - linear mixing to M units,\n",
    "      - per-unit univariate tent bases over s_r (K2 centers),\n",
    "      - linear head to C classes (classification) or scalar (regression).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        task_type: str = \"classification\",\n",
    "        num_classes: int = 2,\n",
    "        K1: int = 8,      # basis per feature\n",
    "        K2: int = 8,      # basis per hidden unit\n",
    "        M: int = 16,      # number of hidden units\n",
    "        span1: Tuple[float, float] = (-3.0, 3.0),  # feature range\n",
    "        span2: Tuple[float, float] = (-3.0, 3.0),  # s_r range\n",
    "        bandwidth1: float = 1.0,\n",
    "        bandwidth2: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.D = input_dim\n",
    "        self.task = task_type\n",
    "        self.C = num_classes if task_type == \"classification\" else 1\n",
    "        self.K1 = K1\n",
    "        self.K2 = K2\n",
    "        self.M = M\n",
    "\n",
    "        c1 = torch.linspace(span1[0], span1[1], K1)\n",
    "        self.bases1 = nn.ModuleList([TentBasis1D(c1, bandwidth=bandwidth1) for _ in range(self.D)])  # one basis per feature\n",
    "        self.W1 = nn.Parameter(torch.randn(self.D, K1) * 0.1)     # [D, K1] -> u_j\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(M, self.D) * 0.1)       # mixing to s_r\n",
    "\n",
    "        c2 = torch.linspace(span2[0], span2[1], K2)\n",
    "        self.basis2 = TentBasis1D(c2, bandwidth=bandwidth2)       # shared basis over s_r\n",
    "        self.W2 = nn.Parameter(torch.randn(M, K2) * 0.1)          # [M, K2] -> g_r\n",
    "\n",
    "        self.W_out = nn.Parameter(torch.randn(M, self.C) * 0.1)   # [M, C]\n",
    "        self.b_out = nn.Parameter(torch.zeros(self.C))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, D]\n",
    "        B = x.size(0)\n",
    "        # Stage 1: per-feature u_j\n",
    "        U = []\n",
    "        for j in range(self.D):\n",
    "            phi = self.bases1[j](x[:, j])  # [B, K1]\n",
    "            u_j = phi @ self.W1[j].unsqueeze(-1)  # [B, 1]\n",
    "            U.append(u_j.squeeze(-1))\n",
    "        U = torch.stack(U, dim=1)  # [B, D]\n",
    "\n",
    "        # Mix to s_r\n",
    "        S = U @ self.A.t()  # [B, M]\n",
    "\n",
    "        # Stage 2: per-unit g_r\n",
    "        G = []\n",
    "        for r in range(self.M):\n",
    "            phi2 = self.basis2(S[:, r])            # [B, K2]\n",
    "            g_r = (phi2 @ self.W2[r].unsqueeze(-1)).squeeze(-1)  # [B]\n",
    "            G.append(g_r)\n",
    "        G = torch.stack(G, dim=1)  # [B, M]\n",
    "\n",
    "        # Head\n",
    "        logits = G @ self.W_out + self.b_out  # [B, C]\n",
    "        if self.task == \"classification\":\n",
    "            return logits\n",
    "        else:\n",
    "            return logits.squeeze(-1)  # [B]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def global_importance(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Crude global importance per feature j:\n",
    "          |W1_j| summed across K1, multiplied by |A[:, j]| and |W_out| aggregated.\n",
    "        Returns: [D] importance.\n",
    "        \"\"\"\n",
    "        w1 = self.W1.abs().sum(dim=1).cpu().numpy()          # [D]\n",
    "        a = self.A.abs().cpu().numpy()                       # [M, D]\n",
    "        wout = self.W_out.abs().sum(dim=1, keepdim=True).cpu().numpy()  # [M,1]\n",
    "        # propagate importance through A then head\n",
    "        imp_m = (a * wout).sum(axis=0)   # [D]\n",
    "        return (w1 * imp_m)\n",
    "\n",
    "\n",
    "def train_manual_kan(\n",
    "    X_train: np.ndarray, y_train: np.ndarray,\n",
    "    X_val: np.ndarray, y_val: np.ndarray,\n",
    "    task_type: str,\n",
    "    num_classes: int,\n",
    "    K1: int = 8, K2: int = 8, M: int = 16,\n",
    "    lr: float = 3e-3, weight_decay: float = 1e-4,\n",
    "    max_epochs: int = 300, patience: int = 50, batch_size: int = 128\n",
    ") -> Tuple[ManualKAN, float]:\n",
    "    seed_everything(42)\n",
    "    D = X_train.shape[1]\n",
    "    model = ManualKAN(\n",
    "        input_dim=D, task_type=task_type, num_classes=num_classes,\n",
    "        K1=K1, K2=K2, M=M, span1=(-3,3), span2=(-3,3),\n",
    "        bandwidth1=1.0, bandwidth2=1.0\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        ytr_t = torch.tensor(y_train, dtype=torch.long)\n",
    "        yval_t = torch.tensor(y_val, dtype=torch.long)\n",
    "    else:\n",
    "        criterion = nn.SmoothL1Loss(beta=1.0)\n",
    "        ytr_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "        yval_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    Xtr_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Xval_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(Xval_t, yval_t), batch_size=batch_size)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            if task_type == \"classification\":\n",
    "                loss = criterion(out, yb)\n",
    "            else:\n",
    "                loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        total_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "                vloss = criterion(out, yb)\n",
    "                total_val += float(vloss.item())\n",
    "        avg_val = total_val / max(1, len(val_loader))\n",
    "\n",
    "        if avg_val < best_val - 1e-6:\n",
    "            best_val = avg_val\n",
    "            epochs_no_improve = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_val\n",
    "\n",
    "def predict_manual_kan(model: ManualKAN, X: np.ndarray, task_type: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Xt = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "        out = model(Xt)\n",
    "        if task_type == \"classification\":\n",
    "            probs = F.softmax(out, dim=-1).cpu().numpy()\n",
    "            return probs\n",
    "        else:\n",
    "            preds = out.cpu().numpy()\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e9797",
   "metadata": {
    "_cell_guid": "b9607a53-2be0-4c83-bb46-e6d1001aa6ab",
    "_uuid": "a302790b-b7ec-4975-826a-b24f2f857d31",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026188,
     "end_time": "2025-09-15T20:17:57.953162",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.926974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model runners (NousNet, XGBoost, EBM, MLP, KAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a59ec0e",
   "metadata": {
    "_cell_guid": "74f4ebc3-3e67-4715-910b-6d2c5c27d551",
    "_uuid": "bd473d3c-e434-4f5a-a93d-56a80fe59a14",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.042661,
     "end_time": "2025-09-15T20:17:58.022221",
     "exception": false,
     "start_time": "2025-09-15T20:17:57.979560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_nous(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_val: np.ndarray, y_val: np.ndarray,\n",
    "    X_te: np.ndarray,\n",
    "    feature_names: List[str],\n",
    "    task_type: str,\n",
    "    epochs: int = 1000, patience: int = 200, batch_size: int = 64, lr: float = 1e-3,\n",
    "    rule_method: str = \"softmax\",\n",
    "    use_calibrators: bool = True,\n",
    "    use_prototypes: bool = False,\n",
    ") -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    seed_everything(42)\n",
    "    input_dim = X_tr.shape[1]\n",
    "    if task_type == \"classification\":\n",
    "        num_outputs = int(len(np.unique(y_tr)))\n",
    "    else:\n",
    "        num_outputs = 1\n",
    "\n",
    "    model = NousNet(\n",
    "        input_dim=input_dim, num_outputs=num_outputs, task_type=task_type,\n",
    "        feature_names=feature_names,\n",
    "        num_facts=32 if task_type==\"classification\" else 64,\n",
    "        rules_per_layer=(16, 8) if task_type==\"classification\" else (32, 16),\n",
    "        rule_selection_method=rule_method, use_calibrators=use_calibrators,\n",
    "        use_prototypes=(use_prototypes and task_type==\"classification\")\n",
    "    )\n",
    "\n",
    "    Xtr_t = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    Xval_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "    Xte_t = torch.tensor(X_te, dtype=torch.float32)\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        ytr_t = torch.tensor(y_tr, dtype=torch.long)\n",
    "        yval_t = torch.tensor(y_val, dtype=torch.long)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        ytr_t = torch.tensor(y_tr, dtype=torch.float32)\n",
    "        yval_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "        criterion = nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(Xval_t, yval_t), batch_size=batch_size)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    from nous.training import train_model as nous_train\n",
    "    best_val = nous_train(\n",
    "        model, train_loader, val_loader, criterion, optimizer,\n",
    "        epochs=epochs, patience=patience, device=DEVICE,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(Xte_t.to(DEVICE))\n",
    "        if task_type == \"classification\":\n",
    "            probs = F.softmax(out, dim=-1).cpu().numpy()\n",
    "            return probs, {\"model\": model, \"best_val\": best_val}\n",
    "        else:\n",
    "            preds = out.cpu().numpy().ravel()\n",
    "            return preds, {\"model\": model, \"best_val\": best_val}\n",
    "\n",
    "def fit_xgboost(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_val: np.ndarray, y_val: np.ndarray,\n",
    "    X_te: np.ndarray, task_type: str\n",
    ") -> np.ndarray:\n",
    "    seed_everything(42)\n",
    "    if task_type == \"classification\":\n",
    "        num_classes = int(len(np.unique(y_tr)))\n",
    "        metric = \"mlogloss\" if num_classes > 2 else \"logloss\"\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=5000, learning_rate=0.02, max_depth=6,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=42, tree_method=\"hist\", n_jobs=0,\n",
    "            eval_metric=metric, early_stopping_rounds=100\n",
    "        )\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        return model.predict_proba(X_te)\n",
    "    else:\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=5000, learning_rate=0.02, max_depth=6,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=42, tree_method=\"hist\", n_jobs=0,\n",
    "            eval_metric=\"rmse\", early_stopping_rounds=100\n",
    "        )\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        return model.predict(X_te)\n",
    "\n",
    "def fit_ebm(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_val: np.ndarray, y_val: np.ndarray,\n",
    "    X_te: np.ndarray, task_type: str\n",
    ") -> Tuple[np.ndarray, Any]:\n",
    "    seed_everything(42)\n",
    "    if task_type == \"classification\":\n",
    "        ebm = ExplainableBoostingClassifier(\n",
    "            interactions=0,\n",
    "            outer_bags=8,\n",
    "            inner_bags=4,\n",
    "            learning_rate=0.05,\n",
    "            max_leaves=3,\n",
    "            max_bins=255,\n",
    "            random_state=42,\n",
    "        )\n",
    "        ebm.fit(X_tr, y_tr)\n",
    "        proba = ebm.predict_proba(X_te)\n",
    "        return proba, ebm\n",
    "    else:\n",
    "        ebm = ExplainableBoostingRegressor(\n",
    "            interactions=0,\n",
    "            outer_bags=8,\n",
    "            inner_bags=4,\n",
    "            learning_rate=0.05,\n",
    "            max_leaves=3,\n",
    "            max_bins=255,\n",
    "            random_state=42,\n",
    "        )\n",
    "        ebm.fit(X_tr, y_tr)\n",
    "        preds = ebm.predict(X_te)\n",
    "        return preds, ebm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dbfaa",
   "metadata": {
    "_cell_guid": "fa9ea2e5-95e7-4989-bad6-30a075162646",
    "_uuid": "2b9e2a75-0f0c-4b36-bd07-0571088c6a7c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026397,
     "end_time": "2025-09-15T20:17:58.075633",
     "exception": false,
     "start_time": "2025-09-15T20:17:58.049236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cross-validation (5-fold)\n",
    "NousNet variants:\n",
    "- NousNet (Fixed): rule_selection_method='fixed', use_calibrators=False\n",
    "- NousNet (Softmax+Proto): rule_selection_method='softmax', use_calibrators=True, use_prototypes=True (classification)\n",
    "Additional baselines: XGBoost, EBM, MLP, KAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad932667",
   "metadata": {
    "_cell_guid": "2923234f-9a8b-4ae3-b923-86798a3c60f0",
    "_uuid": "74ef32e6-3919-4fec-b032-2be204c981aa",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.053128,
     "end_time": "2025-09-15T20:17:58.155687",
     "exception": false,
     "start_time": "2025-09-15T20:17:58.102559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_cv_benchmark(\n",
    "    X: pd.DataFrame, y: pd.Series, feature_names: List[str], class_names: Optional[List[str]],\n",
    "    dataset_name: str, task_type: str,\n",
    "    n_splits: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    seed_everything(42)\n",
    "    Xv = X.values.astype(np.float32)\n",
    "    yv = y.values\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    rows = []\n",
    "    fold = 0\n",
    "    for tr_idx, te_idx in cv.split(Xv, yv if task_type==\"classification\" else None):\n",
    "        fold += 1\n",
    "        X_tr_full, X_te = Xv[tr_idx], Xv[te_idx]\n",
    "        y_tr_full, y_te = yv[tr_idx], yv[te_idx]\n",
    "\n",
    "        # Inner validation split\n",
    "        if task_type == \"classification\":\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X_tr_full, y_tr_full, test_size=0.15, random_state=42, stratify=y_tr_full\n",
    "            )\n",
    "        else:\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X_tr_full, y_tr_full, test_size=0.15, random_state=42\n",
    "            )\n",
    "\n",
    "        # NousNet (Fixed)\n",
    "        t0 = time.time()\n",
    "        if task_type == \"classification\":\n",
    "            y_nf_proba, _ = fit_nous(\n",
    "                X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"classification\",\n",
    "                epochs=1000, patience=200, batch_size=64, lr=1e-3,\n",
    "                rule_method=\"fixed\", use_calibrators=False, use_prototypes=False\n",
    "            )\n",
    "            y_nf = np.argmax(y_nf_proba, axis=1)\n",
    "            acc = accuracy_score(y_te, y_nf)\n",
    "            auc = safe_auc(y_te, y_nf_proba)\n",
    "            dur = time.time() - t0\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Fixed)\", \"metric\": \"Accuracy\", \"value\": acc, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Fixed)\", \"metric\": \"AUC\", \"value\": auc, \"time_s\": dur})\n",
    "        else:\n",
    "            y_nf, _ = fit_nous(\n",
    "                X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"regression\",\n",
    "                epochs=1000, patience=200, batch_size=64, lr=1e-3,\n",
    "                rule_method=\"fixed\", use_calibrators=False, use_prototypes=False\n",
    "            )\n",
    "            rm = rmse(y_te, y_nf)\n",
    "            mae = mean_absolute_error(y_te, y_nf)\n",
    "            r2 = r2_score(y_te, y_nf)\n",
    "            dur = time.time() - t0\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Fixed)\", \"metric\": \"RMSE\", \"value\": rm, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Fixed)\", \"metric\": \"MAE\", \"value\": mae, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Fixed)\", \"metric\": \"R2\", \"value\": r2, \"time_s\": dur})\n",
    "\n",
    "        # NousNet (Softmax+Proto for classification)\n",
    "        t0 = time.time()\n",
    "        if task_type == \"classification\":\n",
    "            y_ns_proba, _ = fit_nous(\n",
    "                X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"classification\",\n",
    "                epochs=1000, patience=200, batch_size=64, lr=1e-3,\n",
    "                rule_method=\"softmax\", use_calibrators=True, use_prototypes=True\n",
    "            )\n",
    "            y_ns = np.argmax(y_ns_proba, axis=1)\n",
    "            acc = accuracy_score(y_te, y_ns)\n",
    "            auc = safe_auc(y_te, y_ns_proba)\n",
    "            dur = time.time() - t0\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Softmax+Proto)\", \"metric\": \"Accuracy\", \"value\": acc, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Softmax+Proto)\", \"metric\": \"AUC\", \"value\": auc, \"time_s\": dur})\n",
    "        else:\n",
    "            y_ns, _ = fit_nous(\n",
    "                X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"regression\",\n",
    "                epochs=1000, patience=200, batch_size=64, lr=1e-3,\n",
    "                rule_method=\"softmax\", use_calibrators=True, use_prototypes=False\n",
    "            )\n",
    "            rm = rmse(y_te, y_ns)\n",
    "            mae = mean_absolute_error(y_te, y_ns)\n",
    "            r2 = r2_score(y_te, y_ns)\n",
    "            dur = time.time() - t0\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Softmax)\", \"metric\": \"RMSE\", \"value\": rm, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Softmax)\", \"metric\": \"MAE\", \"value\": mae, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"NousNet (Softmax)\", \"metric\": \"R2\", \"value\": r2, \"time_s\": dur})\n",
    "\n",
    "        # XGBoost\n",
    "        t0 = time.time()\n",
    "        yhat_xgb = fit_xgboost(X_tr, y_tr, X_val, y_val, X_te, task_type)\n",
    "        dur = time.time() - t0\n",
    "        if task_type == \"classification\":\n",
    "            y_pred = np.argmax(yhat_xgb, axis=1)\n",
    "            acc = accuracy_score(y_te, y_pred)\n",
    "            auc = safe_auc(y_te, yhat_xgb)\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"XGBoost\", \"metric\": \"Accuracy\", \"value\": acc, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"XGBoost\", \"metric\": \"AUC\", \"value\": auc, \"time_s\": dur})\n",
    "        else:\n",
    "            rm = rmse(y_te, yhat_xgb)\n",
    "            mae = mean_absolute_error(y_te, yhat_xgb)\n",
    "            r2 = r2_score(y_te, yhat_xgb)\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"XGBoost\", \"metric\": \"RMSE\", \"value\": rm, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"XGBoost\", \"metric\": \"MAE\", \"value\": mae, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"XGBoost\", \"metric\": \"R2\", \"value\": r2, \"time_s\": dur})\n",
    "\n",
    "        # EBM\n",
    "        t0 = time.time()\n",
    "        yhat_ebm, ebm_model = fit_ebm(X_tr, y_tr, X_val, y_val, X_te, task_type)\n",
    "        dur = time.time() - t0\n",
    "        if task_type == \"classification\":\n",
    "            y_pred = np.argmax(yhat_ebm, axis=1)\n",
    "            acc = accuracy_score(y_te, y_pred)\n",
    "            auc = safe_auc(y_te, yhat_ebm)\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"EBM\", \"metric\": \"Accuracy\", \"value\": acc, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"EBM\", \"metric\": \"AUC\", \"value\": auc, \"time_s\": dur})\n",
    "        else:\n",
    "            rm = rmse(y_te, yhat_ebm)\n",
    "            mae = mean_absolute_error(y_te, yhat_ebm)\n",
    "            r2 = r2_score(y_te, yhat_ebm)\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"EBM\", \"metric\": \"RMSE\", \"value\": rm, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"EBM\", \"metric\": \"MAE\", \"value\": mae, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"EBM\", \"metric\": \"R2\", \"value\": r2, \"time_s\": dur})\n",
    "\n",
    "        # MLP\n",
    "        t0 = time.time()\n",
    "        mlp_model, _ = train_mlp(X_tr, y_tr, X_val, y_val, task_type, max_epochs=1000, patience=200, lr=1e-3, batch_size=64)\n",
    "        yhat_mlp = predict_mlp(mlp_model, X_te, task_type)\n",
    "        dur = time.time() - t0\n",
    "        if task_type == \"classification\":\n",
    "            y_pred = np.argmax(yhat_mlp, axis=1)\n",
    "            acc = accuracy_score(y_te, y_pred)\n",
    "            auc = safe_auc(y_te, yhat_mlp)\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"MLP\", \"metric\": \"Accuracy\", \"value\": acc, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"MLP\", \"metric\": \"AUC\", \"value\": auc, \"time_s\": dur})\n",
    "        else:\n",
    "            y_pred = yhat_mlp\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"MLP\", \"metric\": \"RMSE\", \"value\": rmse(y_te, y_pred), \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"MLP\", \"metric\": \"MAE\", \"value\": mean_absolute_error(y_te, y_pred), \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"MLP\", \"metric\": \"R2\", \"value\": r2_score(y_te, y_pred), \"time_s\": dur})\n",
    "\n",
    "        # KAN (binary classification or regression)\n",
    "        t0 = time.time()\n",
    "        if task_type == \"classification\" and len(np.unique(y_tr)) == 2:\n",
    "            kan_model, _ = train_manual_kan(\n",
    "                X_tr, y_tr, X_val, y_val, task_type=\"classification\",\n",
    "                num_classes=2, K1=8, K2=8, M=16, lr=3e-3, weight_decay=1e-4, max_epochs=300, patience=50, batch_size=64\n",
    "            )\n",
    "            yhat_kan = predict_manual_kan(kan_model, X_te, \"classification\")\n",
    "            acc = accuracy_score(y_te, np.argmax(yhat_kan, axis=1))\n",
    "            auc = safe_auc(y_te, yhat_kan)\n",
    "            dur = time.time() - t0\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"KAN\", \"metric\": \"Accuracy\", \"value\": acc, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"KAN\", \"metric\": \"AUC\", \"value\": auc, \"time_s\": dur})\n",
    "        elif task_type == \"regression\":\n",
    "            kan_model, _ = train_manual_kan(\n",
    "                X_tr, y_tr, X_val, y_val, task_type=\"regression\",\n",
    "                num_classes=1, K1=8, K2=8, M=16, lr=3e-3, weight_decay=1e-4, max_epochs=300, patience=50, batch_size=64\n",
    "            )\n",
    "            yhat_kan = predict_manual_kan(kan_model, X_te, \"regression\")\n",
    "            rm = rmse(y_te, yhat_kan)\n",
    "            mae = mean_absolute_error(y_te, yhat_kan)\n",
    "            r2 = r2_score(y_te, yhat_kan)\n",
    "            dur = time.time() - t0\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"KAN\", \"metric\": \"RMSE\", \"value\": rm, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"KAN\", \"metric\": \"MAE\", \"value\": mae, \"time_s\": dur})\n",
    "            rows.append({\"dataset\": dataset_name, \"fold\": fold, \"model\": \"KAN\", \"metric\": \"R2\", \"value\": r2, \"time_s\": dur})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b5f4a",
   "metadata": {
    "_cell_guid": "3167dbc4-1a64-4ac0-9e62-177694618cab",
    "_uuid": "3258b780-0d72-45bb-90cc-90af00ae0230",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026592,
     "end_time": "2025-09-15T20:17:58.209348",
     "exception": false,
     "start_time": "2025-09-15T20:17:58.182756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run 5-fold CV on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac65191b",
   "metadata": {
    "_cell_guid": "da5af03b-1756-42e4-92f3-4f8d435c7bba",
    "_uuid": "9221fb27-8993-41cc-95f2-6e494d3fc88c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-09-15T20:17:58.235790",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HELOC from https://raw.githubusercontent.com/benoitparis/explainable-challenge/refs/heads/master/heloc_dataset_v1.csv\n",
      "\n",
      "=== HELOC (classification) ===\n",
      "Epoch [1/1000] train=0.6087 val=0.5823 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5603 val=0.5728 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.5551 val=0.5695 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5524 val=0.5654 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5501 val=0.5632 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5465 val=0.5637 l0=0.0000\n",
      "Epoch [13/1000] train=0.5412 val=0.5612 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5335 val=0.5731 l0=0.0000\n",
      "Epoch [30/1000] train=0.5223 val=0.5747 l0=0.0000\n",
      "Epoch [40/1000] train=0.5144 val=0.5784 l0=0.0000\n",
      "Epoch [50/1000] train=0.5076 val=0.5900 l0=0.0000\n",
      "Epoch [60/1000] train=0.5042 val=0.5956 l0=0.0000\n",
      "Epoch [70/1000] train=0.4905 val=0.6169 l0=0.0000\n",
      "Epoch [80/1000] train=0.4846 val=0.6247 l0=0.0000\n",
      "Epoch [90/1000] train=0.4735 val=0.6511 l0=0.0000\n",
      "Epoch [100/1000] train=0.4674 val=0.6670 l0=0.0000\n",
      "Epoch [110/1000] train=0.4568 val=0.6724 l0=0.0000\n",
      "Epoch [120/1000] train=0.4512 val=0.6786 l0=0.0000\n",
      "Epoch [130/1000] train=0.4401 val=0.6819 l0=0.0000\n",
      "Epoch [140/1000] train=0.4318 val=0.7080 l0=0.0000\n",
      "Epoch [150/1000] train=0.4270 val=0.7361 l0=0.0000\n",
      "Epoch [160/1000] train=0.4184 val=0.7301 l0=0.0000\n",
      "Epoch [170/1000] train=0.4132 val=0.7392 l0=0.0000\n",
      "Epoch [180/1000] train=0.4063 val=0.7713 l0=0.0000\n",
      "Epoch [190/1000] train=0.3981 val=0.7799 l0=0.0000\n",
      "Epoch [200/1000] train=0.3948 val=0.7968 l0=0.0000\n",
      "Epoch [210/1000] train=0.3892 val=0.8335 l0=0.0000\n",
      "Early stopping at epoch 213 (best val=0.5612)\n",
      "Restored best model (val=0.5612)\n",
      "Epoch [1/1000] train=0.6242 val=0.5849 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5725 val=0.5768 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.5594 val=0.5716 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5631 val=0.5666 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5600 val=0.5650 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5528 val=0.5599 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5489 val=0.5617 l0=0.0000\n",
      "Epoch [30/1000] train=0.5396 val=0.5638 l0=0.0000\n",
      "Epoch [40/1000] train=0.5385 val=0.5720 l0=0.0000\n",
      "Epoch [50/1000] train=0.5400 val=0.5706 l0=0.0000\n",
      "Epoch [60/1000] train=0.5353 val=0.5656 l0=0.0000\n",
      "Epoch [70/1000] train=0.5323 val=0.5768 l0=0.0000\n",
      "Epoch [80/1000] train=0.5285 val=0.5708 l0=0.0000\n",
      "Epoch [90/1000] train=0.5273 val=0.5728 l0=0.0000\n",
      "Epoch [100/1000] train=0.5238 val=0.5829 l0=0.0000\n",
      "Epoch [110/1000] train=0.5174 val=0.5812 l0=0.0000\n",
      "Epoch [120/1000] train=0.5172 val=0.5880 l0=0.0000\n",
      "Epoch [130/1000] train=0.5112 val=0.5968 l0=0.0000\n",
      "Epoch [140/1000] train=0.5097 val=0.5890 l0=0.0000\n",
      "Epoch [150/1000] train=0.5070 val=0.6036 l0=0.0000\n",
      "Epoch [160/1000] train=0.5030 val=0.5911 l0=0.0000\n",
      "Epoch [170/1000] train=0.4944 val=0.6018 l0=0.0000\n",
      "Epoch [180/1000] train=0.4952 val=0.6006 l0=0.0000\n",
      "Epoch [190/1000] train=0.4916 val=0.6157 l0=0.0000\n",
      "Epoch [200/1000] train=0.4834 val=0.6305 l0=0.0000\n",
      "Epoch [210/1000] train=0.4799 val=0.6220 l0=0.0000\n",
      "Early stopping at epoch 210 (best val=0.5599)\n",
      "Restored best model (val=0.5599)\n",
      "Epoch [1/1000] train=0.6192 val=0.5640 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5629 val=0.5524 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.5585 val=0.5515 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5552 val=0.5507 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.5566 val=0.5473 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5514 val=0.5469 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5491 val=0.5476 l0=0.0000\n",
      "Epoch [15/1000] train=0.5453 val=0.5455 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5375 val=0.5578 l0=0.0000\n",
      "Epoch [30/1000] train=0.5280 val=0.5538 l0=0.0000\n",
      "Epoch [40/1000] train=0.5159 val=0.5615 l0=0.0000\n",
      "Epoch [50/1000] train=0.5097 val=0.5762 l0=0.0000\n",
      "Epoch [60/1000] train=0.5002 val=0.5806 l0=0.0000\n",
      "Epoch [70/1000] train=0.4911 val=0.5839 l0=0.0000\n",
      "Epoch [80/1000] train=0.4819 val=0.6141 l0=0.0000\n",
      "Epoch [90/1000] train=0.4763 val=0.6190 l0=0.0000\n",
      "Epoch [100/1000] train=0.4661 val=0.6345 l0=0.0000\n",
      "Epoch [110/1000] train=0.4560 val=0.6535 l0=0.0000\n",
      "Epoch [120/1000] train=0.4484 val=0.6678 l0=0.0000\n",
      "Epoch [130/1000] train=0.4399 val=0.6935 l0=0.0000\n",
      "Epoch [140/1000] train=0.4328 val=0.7183 l0=0.0000\n",
      "Epoch [150/1000] train=0.4243 val=0.7375 l0=0.0000\n",
      "Epoch [160/1000] train=0.4163 val=0.7239 l0=0.0000\n",
      "Epoch [170/1000] train=0.4097 val=0.7555 l0=0.0000\n",
      "Epoch [180/1000] train=0.4051 val=0.7635 l0=0.0000\n",
      "Epoch [190/1000] train=0.3959 val=0.8078 l0=0.0000\n",
      "Epoch [200/1000] train=0.3884 val=0.8063 l0=0.0000\n",
      "Epoch [210/1000] train=0.3840 val=0.8372 l0=0.0000\n",
      "Early stopping at epoch 215 (best val=0.5455)\n",
      "Restored best model (val=0.5455)\n",
      "Epoch [1/1000] train=0.6211 val=0.5748 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5722 val=0.5590 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.5677 val=0.5560 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5663 val=0.5517 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.5590 val=0.5489 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5584 val=0.5465 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5578 val=0.5479 l0=0.0000\n",
      "Epoch [15/1000] train=0.5504 val=0.5453 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5488 val=0.5447 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.5505 val=0.5438 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.5459 val=0.5448 l0=0.0000\n",
      "Epoch [33/1000] train=0.5446 val=0.5438 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.5418 val=0.5436 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.5404 val=0.5485 l0=0.0000\n",
      "Epoch [60/1000] train=0.5360 val=0.5517 l0=0.0000\n",
      "Epoch [70/1000] train=0.5347 val=0.5514 l0=0.0000\n",
      "Epoch [80/1000] train=0.5292 val=0.5525 l0=0.0000\n",
      "Epoch [90/1000] train=0.5224 val=0.5584 l0=0.0000\n",
      "Epoch [100/1000] train=0.5212 val=0.5650 l0=0.0000\n",
      "Epoch [110/1000] train=0.5155 val=0.5669 l0=0.0000\n",
      "Epoch [120/1000] train=0.5124 val=0.5737 l0=0.0000\n",
      "Epoch [130/1000] train=0.5126 val=0.5727 l0=0.0000\n",
      "Epoch [140/1000] train=0.5042 val=0.5839 l0=0.0000\n",
      "Epoch [150/1000] train=0.5040 val=0.5962 l0=0.0000\n",
      "Epoch [160/1000] train=0.5003 val=0.5856 l0=0.0000\n",
      "Epoch [170/1000] train=0.4965 val=0.5920 l0=0.0000\n",
      "Epoch [180/1000] train=0.4921 val=0.6128 l0=0.0000\n",
      "Epoch [190/1000] train=0.4851 val=0.6033 l0=0.0000\n",
      "Epoch [200/1000] train=0.4882 val=0.6193 l0=0.0000\n",
      "Epoch [210/1000] train=0.4799 val=0.6348 l0=0.0000\n",
      "Epoch [220/1000] train=0.4771 val=0.6322 l0=0.0000\n",
      "Epoch [230/1000] train=0.4692 val=0.6382 l0=0.0000\n",
      "Epoch [240/1000] train=0.4686 val=0.6362 l0=0.0000\n",
      "Early stopping at epoch 240 (best val=0.5436)\n",
      "Restored best model (val=0.5436)\n",
      "Epoch [1/1000] train=0.6102 val=0.5842 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5611 val=0.5670 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5534 val=0.5657 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5482 val=0.5643 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5482 val=0.5643 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.5429 val=0.5630 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.5442 val=0.5616 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.5386 val=0.5587 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5378 val=0.5612 l0=0.0000\n",
      "Epoch [21/1000] train=0.5372 val=0.5580 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.5305 val=0.5657 l0=0.0000\n",
      "Epoch [40/1000] train=0.5188 val=0.5667 l0=0.0000\n",
      "Epoch [50/1000] train=0.5080 val=0.5707 l0=0.0000\n",
      "Epoch [60/1000] train=0.4996 val=0.5827 l0=0.0000\n",
      "Epoch [70/1000] train=0.4905 val=0.5994 l0=0.0000\n",
      "Epoch [80/1000] train=0.4812 val=0.6165 l0=0.0000\n",
      "Epoch [90/1000] train=0.4719 val=0.6340 l0=0.0000\n",
      "Epoch [100/1000] train=0.4621 val=0.6444 l0=0.0000\n",
      "Epoch [110/1000] train=0.4571 val=0.6664 l0=0.0000\n",
      "Epoch [120/1000] train=0.4510 val=0.6815 l0=0.0000\n",
      "Epoch [130/1000] train=0.4375 val=0.6971 l0=0.0000\n",
      "Epoch [140/1000] train=0.4316 val=0.7272 l0=0.0000\n",
      "Epoch [150/1000] train=0.4259 val=0.7335 l0=0.0000\n",
      "Epoch [160/1000] train=0.4143 val=0.7413 l0=0.0000\n",
      "Epoch [170/1000] train=0.4120 val=0.7790 l0=0.0000\n",
      "Epoch [180/1000] train=0.4072 val=0.7845 l0=0.0000\n",
      "Epoch [190/1000] train=0.4003 val=0.7996 l0=0.0000\n",
      "Epoch [200/1000] train=0.3986 val=0.8164 l0=0.0000\n",
      "Epoch [210/1000] train=0.3886 val=0.8311 l0=0.0000\n",
      "Epoch [220/1000] train=0.3852 val=0.8468 l0=0.0000\n",
      "Early stopping at epoch 221 (best val=0.5580)\n",
      "Restored best model (val=0.5580)\n",
      "Epoch [1/1000] train=0.6179 val=0.5775 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5770 val=0.5716 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.5579 val=0.5683 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.5584 val=0.5636 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5586 val=0.5618 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5632 val=0.5762 l0=0.0000\n",
      "Epoch [15/1000] train=0.5458 val=0.5614 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.5514 val=0.5602 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.5513 val=0.5597 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5497 val=0.5611 l0=0.0000\n",
      "Epoch [28/1000] train=0.5453 val=0.5580 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.5443 val=0.5602 l0=0.0000\n",
      "Epoch [35/1000] train=0.5412 val=0.5576 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.5407 val=0.5557 l0=0.0000 (*)\n",
      "Epoch [38/1000] train=0.5389 val=0.5546 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.5366 val=0.5618 l0=0.0000\n",
      "Epoch [50/1000] train=0.5369 val=0.5612 l0=0.0000\n",
      "Epoch [60/1000] train=0.5337 val=0.5578 l0=0.0000\n",
      "Epoch [70/1000] train=0.5314 val=0.5571 l0=0.0000\n",
      "Epoch [80/1000] train=0.5245 val=0.5663 l0=0.0000\n",
      "Epoch [90/1000] train=0.5214 val=0.5688 l0=0.0000\n",
      "Epoch [100/1000] train=0.5201 val=0.5645 l0=0.0000\n",
      "Epoch [110/1000] train=0.5107 val=0.5735 l0=0.0000\n",
      "Epoch [120/1000] train=0.5102 val=0.5750 l0=0.0000\n",
      "Epoch [130/1000] train=0.5076 val=0.5899 l0=0.0000\n",
      "Epoch [140/1000] train=0.5056 val=0.5998 l0=0.0000\n",
      "Epoch [150/1000] train=0.5029 val=0.6032 l0=0.0000\n",
      "Epoch [160/1000] train=0.4945 val=0.6085 l0=0.0000\n",
      "Epoch [170/1000] train=0.4885 val=0.6002 l0=0.0000\n",
      "Epoch [180/1000] train=0.4821 val=0.6143 l0=0.0000\n",
      "Epoch [190/1000] train=0.4772 val=0.6219 l0=0.0000\n",
      "Epoch [200/1000] train=0.4791 val=0.6186 l0=0.0000\n",
      "Epoch [210/1000] train=0.4729 val=0.6341 l0=0.0000\n",
      "Epoch [220/1000] train=0.4719 val=0.6429 l0=0.0000\n",
      "Epoch [230/1000] train=0.4645 val=0.6601 l0=0.0000\n",
      "Early stopping at epoch 238 (best val=0.5546)\n",
      "Restored best model (val=0.5546)\n",
      "Epoch [1/1000] train=0.6063 val=0.5885 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5631 val=0.5628 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5505 val=0.5626 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.5496 val=0.5610 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5521 val=0.5604 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5490 val=0.5598 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5498 val=0.5581 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5476 val=0.5610 l0=0.0000\n",
      "Epoch [12/1000] train=0.5445 val=0.5579 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5395 val=0.5674 l0=0.0000\n",
      "Epoch [30/1000] train=0.5253 val=0.5773 l0=0.0000\n",
      "Epoch [40/1000] train=0.5147 val=0.5918 l0=0.0000\n",
      "Epoch [50/1000] train=0.5073 val=0.5962 l0=0.0000\n",
      "Epoch [60/1000] train=0.5012 val=0.5951 l0=0.0000\n",
      "Epoch [70/1000] train=0.4888 val=0.6148 l0=0.0000\n",
      "Epoch [80/1000] train=0.4793 val=0.6331 l0=0.0000\n",
      "Epoch [90/1000] train=0.4663 val=0.6522 l0=0.0000\n",
      "Epoch [100/1000] train=0.4607 val=0.6747 l0=0.0000\n",
      "Epoch [110/1000] train=0.4461 val=0.6829 l0=0.0000\n",
      "Epoch [120/1000] train=0.4386 val=0.7258 l0=0.0000\n",
      "Epoch [130/1000] train=0.4338 val=0.7338 l0=0.0000\n",
      "Epoch [140/1000] train=0.4254 val=0.7316 l0=0.0000\n",
      "Epoch [150/1000] train=0.4187 val=0.7466 l0=0.0000\n",
      "Epoch [160/1000] train=0.4132 val=0.7637 l0=0.0000\n",
      "Epoch [170/1000] train=0.4155 val=0.7705 l0=0.0000\n",
      "Epoch [180/1000] train=0.4034 val=0.8011 l0=0.0000\n",
      "Epoch [190/1000] train=0.3991 val=0.7893 l0=0.0000\n",
      "Epoch [200/1000] train=0.3965 val=0.8154 l0=0.0000\n",
      "Epoch [210/1000] train=0.3912 val=0.7953 l0=0.0000\n",
      "Early stopping at epoch 212 (best val=0.5579)\n",
      "Restored best model (val=0.5579)\n",
      "Epoch [1/1000] train=0.6233 val=0.5777 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5700 val=0.5682 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.5609 val=0.5655 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.5606 val=0.5572 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5564 val=0.5555 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5551 val=0.5640 l0=0.0000\n",
      "Epoch [11/1000] train=0.5588 val=0.5555 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.5482 val=0.5552 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.5491 val=0.5537 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5483 val=0.5544 l0=0.0000\n",
      "Epoch [24/1000] train=0.5456 val=0.5537 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.5465 val=0.5535 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.5495 val=0.5659 l0=0.0000\n",
      "Epoch [40/1000] train=0.5415 val=0.5613 l0=0.0000\n",
      "Epoch [50/1000] train=0.5377 val=0.5578 l0=0.0000\n",
      "Epoch [60/1000] train=0.5348 val=0.5602 l0=0.0000\n",
      "Epoch [70/1000] train=0.5303 val=0.5637 l0=0.0000\n",
      "Epoch [80/1000] train=0.5301 val=0.5592 l0=0.0000\n",
      "Epoch [90/1000] train=0.5248 val=0.5645 l0=0.0000\n",
      "Epoch [100/1000] train=0.5195 val=0.5667 l0=0.0000\n",
      "Epoch [110/1000] train=0.5196 val=0.5632 l0=0.0000\n",
      "Epoch [120/1000] train=0.5172 val=0.5659 l0=0.0000\n",
      "Epoch [130/1000] train=0.5145 val=0.5625 l0=0.0000\n",
      "Epoch [140/1000] train=0.5086 val=0.5773 l0=0.0000\n",
      "Epoch [150/1000] train=0.5028 val=0.5855 l0=0.0000\n",
      "Epoch [160/1000] train=0.4991 val=0.5760 l0=0.0000\n",
      "Epoch [170/1000] train=0.4971 val=0.5754 l0=0.0000\n",
      "Epoch [180/1000] train=0.4950 val=0.5903 l0=0.0000\n",
      "Epoch [190/1000] train=0.4856 val=0.5902 l0=0.0000\n",
      "Epoch [200/1000] train=0.4858 val=0.5933 l0=0.0000\n",
      "Epoch [210/1000] train=0.4790 val=0.6156 l0=0.0000\n",
      "Epoch [220/1000] train=0.4730 val=0.6171 l0=0.0000\n",
      "Early stopping at epoch 228 (best val=0.5535)\n",
      "Restored best model (val=0.5535)\n",
      "Epoch [1/1000] train=0.6109 val=0.5704 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5619 val=0.5691 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.5599 val=0.5615 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5543 val=0.5609 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.5523 val=0.5609 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5523 val=0.5587 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5489 val=0.5586 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5487 val=0.5612 l0=0.0000\n",
      "Epoch [11/1000] train=0.5470 val=0.5581 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5410 val=0.5674 l0=0.0000\n",
      "Epoch [30/1000] train=0.5291 val=0.5744 l0=0.0000\n",
      "Epoch [40/1000] train=0.5206 val=0.5828 l0=0.0000\n",
      "Epoch [50/1000] train=0.5107 val=0.5898 l0=0.0000\n",
      "Epoch [60/1000] train=0.4992 val=0.5945 l0=0.0000\n",
      "Epoch [70/1000] train=0.4894 val=0.6110 l0=0.0000\n",
      "Epoch [80/1000] train=0.4786 val=0.6292 l0=0.0000\n",
      "Epoch [90/1000] train=0.4740 val=0.6398 l0=0.0000\n",
      "Epoch [100/1000] train=0.4624 val=0.6458 l0=0.0000\n",
      "Epoch [110/1000] train=0.4545 val=0.6773 l0=0.0000\n",
      "Epoch [120/1000] train=0.4430 val=0.6984 l0=0.0000\n",
      "Epoch [130/1000] train=0.4324 val=0.7077 l0=0.0000\n",
      "Epoch [140/1000] train=0.4274 val=0.7161 l0=0.0000\n",
      "Epoch [150/1000] train=0.4164 val=0.7588 l0=0.0000\n",
      "Epoch [160/1000] train=0.4103 val=0.7598 l0=0.0000\n",
      "Epoch [170/1000] train=0.4067 val=0.7741 l0=0.0000\n",
      "Epoch [180/1000] train=0.3939 val=0.8029 l0=0.0000\n",
      "Epoch [190/1000] train=0.3898 val=0.8231 l0=0.0000\n",
      "Epoch [200/1000] train=0.3852 val=0.8501 l0=0.0000\n",
      "Epoch [210/1000] train=0.3754 val=0.8736 l0=0.0000\n",
      "Early stopping at epoch 211 (best val=0.5581)\n",
      "Restored best model (val=0.5581)\n",
      "Epoch [1/1000] train=0.6215 val=0.5789 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.5682 val=0.5700 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5639 val=0.5679 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5576 val=0.5673 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5587 val=0.5625 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5547 val=0.5606 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5528 val=0.5795 l0=0.0000\n",
      "Epoch [20/1000] train=0.5480 val=0.5616 l0=0.0000\n",
      "Epoch [30/1000] train=0.5461 val=0.5623 l0=0.0000\n",
      "Epoch [40/1000] train=0.5400 val=0.5627 l0=0.0000\n",
      "Epoch [50/1000] train=0.5414 val=0.5704 l0=0.0000\n",
      "Epoch [60/1000] train=0.5330 val=0.5725 l0=0.0000\n",
      "Epoch [70/1000] train=0.5309 val=0.5723 l0=0.0000\n",
      "Epoch [80/1000] train=0.5267 val=0.5706 l0=0.0000\n",
      "Epoch [90/1000] train=0.5255 val=0.5819 l0=0.0000\n",
      "Epoch [100/1000] train=0.5170 val=0.5824 l0=0.0000\n",
      "Epoch [110/1000] train=0.5149 val=0.5883 l0=0.0000\n",
      "Epoch [120/1000] train=0.5078 val=0.6063 l0=0.0000\n",
      "Epoch [130/1000] train=0.5034 val=0.5921 l0=0.0000\n",
      "Epoch [140/1000] train=0.5004 val=0.6176 l0=0.0000\n",
      "Epoch [150/1000] train=0.4934 val=0.6137 l0=0.0000\n",
      "Epoch [160/1000] train=0.4921 val=0.6170 l0=0.0000\n",
      "Epoch [170/1000] train=0.4881 val=0.6294 l0=0.0000\n",
      "Epoch [180/1000] train=0.4851 val=0.6263 l0=0.0000\n",
      "Epoch [190/1000] train=0.4819 val=0.6265 l0=0.0000\n",
      "Epoch [200/1000] train=0.4803 val=0.6609 l0=0.0000\n",
      "Early stopping at epoch 209 (best val=0.5606)\n",
      "Restored best model (val=0.5606)\n",
      "\n",
      "=== Adult (classification) ===\n",
      "Epoch [1/1000] train=0.3564 val=0.3116 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3227 val=0.2977 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3160 val=0.2964 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3119 val=0.2962 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3108 val=0.2938 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.3043 val=0.2933 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3026 val=0.2945 l0=0.0000\n",
      "Epoch [11/1000] train=0.3013 val=0.2927 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.2985 val=0.2923 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.2963 val=0.2921 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2937 val=0.2945 l0=0.0000\n",
      "Epoch [30/1000] train=0.2865 val=0.2993 l0=0.0000\n",
      "Epoch [40/1000] train=0.2777 val=0.3143 l0=0.0000\n",
      "Epoch [50/1000] train=0.2694 val=0.3296 l0=0.0000\n",
      "Epoch [60/1000] train=0.2612 val=0.3406 l0=0.0000\n",
      "Epoch [70/1000] train=0.2550 val=0.3496 l0=0.0000\n",
      "Epoch [80/1000] train=0.2482 val=0.3724 l0=0.0000\n",
      "Epoch [90/1000] train=0.2432 val=0.3915 l0=0.0000\n",
      "Epoch [100/1000] train=0.2396 val=0.4016 l0=0.0000\n",
      "Epoch [110/1000] train=0.2355 val=0.4319 l0=0.0000\n",
      "Epoch [120/1000] train=0.2338 val=0.4355 l0=0.0000\n",
      "Epoch [130/1000] train=0.2302 val=0.4579 l0=0.0000\n",
      "Epoch [140/1000] train=0.2279 val=0.4500 l0=0.0000\n",
      "Epoch [150/1000] train=0.2256 val=0.4733 l0=0.0000\n",
      "Epoch [160/1000] train=0.2226 val=0.4862 l0=0.0000\n",
      "Epoch [170/1000] train=0.2221 val=0.4982 l0=0.0000\n",
      "Epoch [180/1000] train=0.2201 val=0.5147 l0=0.0000\n",
      "Epoch [190/1000] train=0.2184 val=0.5195 l0=0.0000\n",
      "Epoch [200/1000] train=0.2168 val=0.5285 l0=0.0000\n",
      "Epoch [210/1000] train=0.2161 val=0.5522 l0=0.0000\n",
      "Early stopping at epoch 217 (best val=0.2921)\n",
      "Restored best model (val=0.2921)\n",
      "Epoch [1/1000] train=0.4490 val=0.3472 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3577 val=0.3394 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3467 val=0.3077 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3364 val=0.3028 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3429 val=0.3012 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.3332 val=0.2989 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3292 val=0.2982 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.3246 val=0.2970 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.3239 val=0.2962 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.3191 val=0.2924 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.3183 val=0.3047 l0=0.0000\n",
      "Epoch [30/1000] train=0.3153 val=0.3682 l0=0.0000\n",
      "Epoch [32/1000] train=0.3114 val=0.2901 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.3102 val=0.2894 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.3097 val=0.2945 l0=0.0000\n",
      "Epoch [49/1000] train=0.3065 val=0.2893 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.3070 val=0.2990 l0=0.0000\n",
      "Epoch [59/1000] train=0.3050 val=0.2889 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.3038 val=0.2915 l0=0.0000\n",
      "Epoch [61/1000] train=0.3055 val=0.2875 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.3026 val=0.2895 l0=0.0000\n",
      "Epoch [80/1000] train=0.3019 val=0.2908 l0=0.0000\n",
      "Epoch [90/1000] train=0.3008 val=0.2901 l0=0.0000\n",
      "Epoch [100/1000] train=0.2990 val=0.2930 l0=0.0000\n",
      "Epoch [110/1000] train=0.2975 val=0.2896 l0=0.0000\n",
      "Epoch [120/1000] train=0.2959 val=0.2922 l0=0.0000\n",
      "Epoch [130/1000] train=0.2946 val=0.2936 l0=0.0000\n",
      "Epoch [140/1000] train=0.2923 val=0.2972 l0=0.0000\n",
      "Epoch [150/1000] train=0.2898 val=0.2971 l0=0.0000\n",
      "Epoch [160/1000] train=0.2859 val=0.2973 l0=0.0000\n",
      "Epoch [170/1000] train=0.2833 val=0.3087 l0=0.0000\n",
      "Epoch [180/1000] train=0.2798 val=0.3112 l0=0.0000\n",
      "Epoch [190/1000] train=0.2758 val=0.3205 l0=0.0000\n",
      "Epoch [200/1000] train=0.2716 val=0.3270 l0=0.0000\n",
      "Epoch [210/1000] train=0.2686 val=0.3470 l0=0.0000\n",
      "Epoch [220/1000] train=0.2645 val=0.3631 l0=0.0000\n",
      "Epoch [230/1000] train=0.2618 val=0.3509 l0=0.0000\n",
      "Epoch [240/1000] train=0.2599 val=0.3690 l0=0.0000\n",
      "Epoch [250/1000] train=0.2569 val=0.3760 l0=0.0000\n",
      "Epoch [260/1000] train=0.2543 val=0.3791 l0=0.0000\n",
      "Early stopping at epoch 261 (best val=0.2875)\n",
      "Restored best model (val=0.2875)\n",
      "Epoch [1/1000] train=0.3572 val=0.3251 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3209 val=0.3232 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3150 val=0.3152 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3101 val=0.3132 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3069 val=0.3131 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.3041 val=0.3123 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.3037 val=0.3089 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3012 val=0.3115 l0=0.0000\n",
      "Epoch [11/1000] train=0.3003 val=0.3076 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.2963 val=0.3075 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2936 val=0.3095 l0=0.0000\n",
      "Epoch [30/1000] train=0.2855 val=0.3131 l0=0.0000\n",
      "Epoch [40/1000] train=0.2777 val=0.3231 l0=0.0000\n",
      "Epoch [50/1000] train=0.2700 val=0.3247 l0=0.0000\n",
      "Epoch [60/1000] train=0.2629 val=0.3381 l0=0.0000\n",
      "Epoch [70/1000] train=0.2556 val=0.3663 l0=0.0000\n",
      "Epoch [80/1000] train=0.2493 val=0.3728 l0=0.0000\n",
      "Epoch [90/1000] train=0.2436 val=0.3873 l0=0.0000\n",
      "Epoch [100/1000] train=0.2381 val=0.4004 l0=0.0000\n",
      "Epoch [110/1000] train=0.2336 val=0.4257 l0=0.0000\n",
      "Epoch [120/1000] train=0.2309 val=0.4413 l0=0.0000\n",
      "Epoch [130/1000] train=0.2284 val=0.4580 l0=0.0000\n",
      "Epoch [140/1000] train=0.2263 val=0.4706 l0=0.0000\n",
      "Epoch [150/1000] train=0.2240 val=0.4923 l0=0.0000\n",
      "Epoch [160/1000] train=0.2217 val=0.4896 l0=0.0000\n",
      "Epoch [170/1000] train=0.2201 val=0.5071 l0=0.0000\n",
      "Epoch [180/1000] train=0.2181 val=0.5171 l0=0.0000\n",
      "Epoch [190/1000] train=0.2174 val=0.5104 l0=0.0000\n",
      "Epoch [200/1000] train=0.2156 val=0.5140 l0=0.0000\n",
      "Epoch [210/1000] train=0.2148 val=0.5379 l0=0.0000\n",
      "Early stopping at epoch 216 (best val=0.3075)\n",
      "Restored best model (val=0.3075)\n",
      "Epoch [1/1000] train=0.4517 val=0.3498 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3476 val=0.3435 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3447 val=0.3218 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.3335 val=0.3167 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3285 val=0.3461 l0=0.0000\n",
      "Epoch [14/1000] train=0.3195 val=0.3160 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.3190 val=0.3150 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.3189 val=0.3129 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.3169 val=0.3200 l0=0.0000\n",
      "Epoch [22/1000] train=0.3169 val=0.3124 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.3129 val=0.3081 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.3132 val=0.3091 l0=0.0000\n",
      "Epoch [34/1000] train=0.3100 val=0.3075 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.3097 val=0.3051 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.3088 val=0.3084 l0=0.0000\n",
      "Epoch [41/1000] train=0.3089 val=0.3046 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.3061 val=0.3154 l0=0.0000\n",
      "Epoch [51/1000] train=0.3074 val=0.3027 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.3051 val=0.3044 l0=0.0000\n",
      "Epoch [65/1000] train=0.3045 val=0.3019 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.3036 val=0.3042 l0=0.0000\n",
      "Epoch [71/1000] train=0.3036 val=0.3018 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.3022 val=0.3016 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.2998 val=0.3043 l0=0.0000\n",
      "Epoch [100/1000] train=0.2983 val=0.3032 l0=0.0000\n",
      "Epoch [110/1000] train=0.2963 val=0.3030 l0=0.0000\n",
      "Epoch [120/1000] train=0.2953 val=0.3061 l0=0.0000\n",
      "Epoch [130/1000] train=0.2932 val=0.3060 l0=0.0000\n",
      "Epoch [140/1000] train=0.2909 val=0.3145 l0=0.0000\n",
      "Epoch [150/1000] train=0.2888 val=0.3158 l0=0.0000\n",
      "Epoch [160/1000] train=0.2853 val=0.3154 l0=0.0000\n",
      "Epoch [170/1000] train=0.2819 val=0.3214 l0=0.0000\n",
      "Epoch [180/1000] train=0.2790 val=0.3262 l0=0.0000\n",
      "Epoch [190/1000] train=0.2758 val=0.3356 l0=0.0000\n",
      "Epoch [200/1000] train=0.2728 val=0.3456 l0=0.0000\n",
      "Epoch [210/1000] train=0.2675 val=0.3558 l0=0.0000\n",
      "Epoch [220/1000] train=0.2652 val=0.3603 l0=0.0000\n",
      "Epoch [230/1000] train=0.2618 val=0.3624 l0=0.0000\n",
      "Epoch [240/1000] train=0.2589 val=0.3830 l0=0.0000\n",
      "Epoch [250/1000] train=0.2568 val=0.3840 l0=0.0000\n",
      "Epoch [260/1000] train=0.2554 val=0.3856 l0=0.0000\n",
      "Epoch [270/1000] train=0.2532 val=0.3748 l0=0.0000\n",
      "Epoch [280/1000] train=0.2513 val=0.3904 l0=0.0000\n",
      "Early stopping at epoch 280 (best val=0.3016)\n",
      "Restored best model (val=0.3016)\n",
      "Epoch [1/1000] train=0.3542 val=0.3343 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3183 val=0.3240 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3127 val=0.3189 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3066 val=0.3188 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3044 val=0.3173 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.3017 val=0.3161 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.2980 val=0.3164 l0=0.0000\n",
      "Epoch [15/1000] train=0.2955 val=0.3157 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.2935 val=0.3150 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2906 val=0.3149 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.2841 val=0.3256 l0=0.0000\n",
      "Epoch [40/1000] train=0.2769 val=0.3358 l0=0.0000\n",
      "Epoch [50/1000] train=0.2677 val=0.3534 l0=0.0000\n",
      "Epoch [60/1000] train=0.2612 val=0.3540 l0=0.0000\n",
      "Epoch [70/1000] train=0.2544 val=0.3727 l0=0.0000\n",
      "Epoch [80/1000] train=0.2492 val=0.3854 l0=0.0000\n",
      "Epoch [90/1000] train=0.2445 val=0.4056 l0=0.0000\n",
      "Epoch [100/1000] train=0.2402 val=0.4229 l0=0.0000\n",
      "Epoch [110/1000] train=0.2363 val=0.4308 l0=0.0000\n",
      "Epoch [120/1000] train=0.2332 val=0.4458 l0=0.0000\n",
      "Epoch [130/1000] train=0.2304 val=0.4544 l0=0.0000\n",
      "Epoch [140/1000] train=0.2277 val=0.4640 l0=0.0000\n",
      "Epoch [150/1000] train=0.2246 val=0.4811 l0=0.0000\n",
      "Epoch [160/1000] train=0.2228 val=0.4805 l0=0.0000\n",
      "Epoch [170/1000] train=0.2207 val=0.5070 l0=0.0000\n",
      "Epoch [180/1000] train=0.2189 val=0.5152 l0=0.0000\n",
      "Epoch [190/1000] train=0.2168 val=0.5080 l0=0.0000\n",
      "Epoch [200/1000] train=0.2148 val=0.5283 l0=0.0000\n",
      "Epoch [210/1000] train=0.2137 val=0.5514 l0=0.0000\n",
      "Epoch [220/1000] train=0.2137 val=0.5408 l0=0.0000\n",
      "Early stopping at epoch 220 (best val=0.3149)\n",
      "Restored best model (val=0.3149)\n",
      "Epoch [1/1000] train=0.4319 val=0.3623 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3623 val=0.3347 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3450 val=0.3328 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3388 val=0.3254 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3338 val=0.3248 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3249 val=0.3281 l0=0.0000\n",
      "Epoch [11/1000] train=0.3244 val=0.3220 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.3237 val=0.3198 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.3234 val=0.3189 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.3197 val=0.3173 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.3150 val=0.3173 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.3145 val=0.3157 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.3124 val=0.3344 l0=0.0000\n",
      "Epoch [32/1000] train=0.3110 val=0.3137 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.3087 val=0.3133 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.3079 val=0.3129 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.3066 val=0.3122 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.3077 val=0.3290 l0=0.0000\n",
      "Epoch [48/1000] train=0.3051 val=0.3120 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.3047 val=0.3112 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.3016 val=0.3206 l0=0.0000\n",
      "Epoch [70/1000] train=0.3002 val=0.3121 l0=0.0000\n",
      "Epoch [80/1000] train=0.3001 val=0.3118 l0=0.0000\n",
      "Epoch [90/1000] train=0.2987 val=0.3127 l0=0.0000\n",
      "Epoch [100/1000] train=0.2972 val=0.3126 l0=0.0000\n",
      "Epoch [110/1000] train=0.2954 val=0.3135 l0=0.0000\n",
      "Epoch [120/1000] train=0.2926 val=0.3168 l0=0.0000\n",
      "Epoch [130/1000] train=0.2901 val=0.3159 l0=0.0000\n",
      "Epoch [140/1000] train=0.2867 val=0.3173 l0=0.0000\n",
      "Epoch [150/1000] train=0.2833 val=0.3275 l0=0.0000\n",
      "Epoch [160/1000] train=0.2806 val=0.3306 l0=0.0000\n",
      "Epoch [170/1000] train=0.2773 val=0.3327 l0=0.0000\n",
      "Epoch [180/1000] train=0.2726 val=0.3338 l0=0.0000\n",
      "Epoch [190/1000] train=0.2674 val=0.3558 l0=0.0000\n",
      "Epoch [200/1000] train=0.2639 val=0.3524 l0=0.0000\n",
      "Epoch [210/1000] train=0.2607 val=0.3638 l0=0.0000\n",
      "Epoch [220/1000] train=0.2585 val=0.3794 l0=0.0000\n",
      "Epoch [230/1000] train=0.2536 val=0.3922 l0=0.0000\n",
      "Epoch [240/1000] train=0.2511 val=0.3917 l0=0.0000\n",
      "Epoch [250/1000] train=0.2492 val=0.4029 l0=0.0000\n",
      "Early stopping at epoch 250 (best val=0.3112)\n",
      "Restored best model (val=0.3112)\n",
      "Epoch [1/1000] train=0.3571 val=0.3240 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3111 val=0.3155 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3090 val=0.3147 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3066 val=0.3105 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.3060 val=0.3100 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3002 val=0.3110 l0=0.0000\n",
      "Epoch [12/1000] train=0.2983 val=0.3096 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.2978 val=0.3093 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2924 val=0.3154 l0=0.0000\n",
      "Epoch [30/1000] train=0.2848 val=0.3209 l0=0.0000\n",
      "Epoch [40/1000] train=0.2769 val=0.3306 l0=0.0000\n",
      "Epoch [50/1000] train=0.2677 val=0.3457 l0=0.0000\n",
      "Epoch [60/1000] train=0.2600 val=0.3546 l0=0.0000\n",
      "Epoch [70/1000] train=0.2535 val=0.3751 l0=0.0000\n",
      "Epoch [80/1000] train=0.2480 val=0.3826 l0=0.0000\n",
      "Epoch [90/1000] train=0.2441 val=0.4011 l0=0.0000\n",
      "Epoch [100/1000] train=0.2404 val=0.4085 l0=0.0000\n",
      "Epoch [110/1000] train=0.2375 val=0.4343 l0=0.0000\n",
      "Epoch [120/1000] train=0.2340 val=0.4400 l0=0.0000\n",
      "Epoch [130/1000] train=0.2303 val=0.4481 l0=0.0000\n",
      "Epoch [140/1000] train=0.2275 val=0.4739 l0=0.0000\n",
      "Epoch [150/1000] train=0.2252 val=0.4768 l0=0.0000\n",
      "Epoch [160/1000] train=0.2239 val=0.4768 l0=0.0000\n",
      "Epoch [170/1000] train=0.2216 val=0.5005 l0=0.0000\n",
      "Epoch [180/1000] train=0.2198 val=0.5229 l0=0.0000\n",
      "Epoch [190/1000] train=0.2187 val=0.5221 l0=0.0000\n",
      "Epoch [200/1000] train=0.2173 val=0.5378 l0=0.0000\n",
      "Epoch [210/1000] train=0.2167 val=0.5429 l0=0.0000\n",
      "Early stopping at epoch 213 (best val=0.3093)\n",
      "Restored best model (val=0.3093)\n",
      "Epoch [1/1000] train=0.4307 val=0.3479 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3691 val=0.3468 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3514 val=0.3284 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3443 val=0.3241 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3349 val=0.3184 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.3331 val=0.3162 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3321 val=0.3157 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.3297 val=0.3146 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.3194 val=0.3111 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.3166 val=0.3109 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.3180 val=0.3108 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.3143 val=0.3101 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.3119 val=0.3091 l0=0.0000 (*)\n",
      "Epoch [34/1000] train=0.3099 val=0.3077 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.3108 val=0.3076 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.3103 val=0.3075 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.3081 val=0.3062 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.3064 val=0.3057 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.3072 val=0.3061 l0=0.0000\n",
      "Epoch [51/1000] train=0.3057 val=0.3050 l0=0.0000 (*)\n",
      "Epoch [52/1000] train=0.3065 val=0.3046 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.3046 val=0.3094 l0=0.0000\n",
      "Epoch [70/1000] train=0.3044 val=0.3045 l0=0.0000 (*)\n",
      "Epoch [75/1000] train=0.3019 val=0.3039 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.3025 val=0.3057 l0=0.0000\n",
      "Epoch [81/1000] train=0.3028 val=0.3030 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.3015 val=0.3039 l0=0.0000\n",
      "Epoch [92/1000] train=0.3009 val=0.3026 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.2986 val=0.3032 l0=0.0000\n",
      "Epoch [110/1000] train=0.2972 val=0.3061 l0=0.0000\n",
      "Epoch [120/1000] train=0.2961 val=0.3064 l0=0.0000\n",
      "Epoch [130/1000] train=0.2929 val=0.3085 l0=0.0000\n",
      "Epoch [140/1000] train=0.2912 val=0.3095 l0=0.0000\n",
      "Epoch [150/1000] train=0.2898 val=0.3108 l0=0.0000\n",
      "Epoch [160/1000] train=0.2871 val=0.3183 l0=0.0000\n",
      "Epoch [170/1000] train=0.2835 val=0.3203 l0=0.0000\n",
      "Epoch [180/1000] train=0.2807 val=0.3357 l0=0.0000\n",
      "Epoch [190/1000] train=0.2773 val=0.3336 l0=0.0000\n",
      "Epoch [200/1000] train=0.2734 val=0.3370 l0=0.0000\n",
      "Epoch [210/1000] train=0.2706 val=0.3401 l0=0.0000\n",
      "Epoch [220/1000] train=0.2680 val=0.3444 l0=0.0000\n",
      "Epoch [230/1000] train=0.2643 val=0.3666 l0=0.0000\n",
      "Epoch [240/1000] train=0.2624 val=0.3599 l0=0.0000\n",
      "Epoch [250/1000] train=0.2601 val=0.3691 l0=0.0000\n",
      "Epoch [260/1000] train=0.2576 val=0.3766 l0=0.0000\n",
      "Epoch [270/1000] train=0.2552 val=0.3948 l0=0.0000\n",
      "Epoch [280/1000] train=0.2541 val=0.3939 l0=0.0000\n",
      "Epoch [290/1000] train=0.2528 val=0.4017 l0=0.0000\n",
      "Early stopping at epoch 292 (best val=0.3026)\n",
      "Restored best model (val=0.3026)\n",
      "Epoch [1/1000] train=0.3542 val=0.3263 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3204 val=0.3247 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3147 val=0.3162 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3101 val=0.3147 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3080 val=0.3138 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3060 val=0.3102 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.3030 val=0.3096 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3007 val=0.3134 l0=0.0000\n",
      "Epoch [13/1000] train=0.2986 val=0.3077 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2939 val=0.3102 l0=0.0000\n",
      "Epoch [27/1000] train=0.2888 val=0.3067 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.2869 val=0.3108 l0=0.0000\n",
      "Epoch [40/1000] train=0.2796 val=0.3216 l0=0.0000\n",
      "Epoch [50/1000] train=0.2710 val=0.3316 l0=0.0000\n",
      "Epoch [60/1000] train=0.2638 val=0.3436 l0=0.0000\n",
      "Epoch [70/1000] train=0.2570 val=0.3603 l0=0.0000\n",
      "Epoch [80/1000] train=0.2521 val=0.3739 l0=0.0000\n",
      "Epoch [90/1000] train=0.2462 val=0.3976 l0=0.0000\n",
      "Epoch [100/1000] train=0.2414 val=0.4094 l0=0.0000\n",
      "Epoch [110/1000] train=0.2366 val=0.4325 l0=0.0000\n",
      "Epoch [120/1000] train=0.2321 val=0.4596 l0=0.0000\n",
      "Epoch [130/1000] train=0.2292 val=0.4671 l0=0.0000\n",
      "Epoch [140/1000] train=0.2267 val=0.4848 l0=0.0000\n",
      "Epoch [150/1000] train=0.2240 val=0.5080 l0=0.0000\n",
      "Epoch [160/1000] train=0.2214 val=0.5139 l0=0.0000\n",
      "Epoch [170/1000] train=0.2190 val=0.5269 l0=0.0000\n",
      "Epoch [180/1000] train=0.2174 val=0.5553 l0=0.0000\n",
      "Epoch [190/1000] train=0.2163 val=0.5632 l0=0.0000\n",
      "Epoch [200/1000] train=0.2160 val=0.5658 l0=0.0000\n",
      "Epoch [210/1000] train=0.2129 val=0.5867 l0=0.0000\n",
      "Epoch [220/1000] train=0.2115 val=0.5859 l0=0.0000\n",
      "Early stopping at epoch 227 (best val=0.3067)\n",
      "Restored best model (val=0.3067)\n",
      "Epoch [1/1000] train=0.4390 val=0.3768 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3665 val=0.3360 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3478 val=0.3259 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.3426 val=0.3222 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3280 val=0.3242 l0=0.0000\n",
      "Epoch [17/1000] train=0.3206 val=0.3198 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.3148 val=0.3535 l0=0.0000\n",
      "Epoch [21/1000] train=0.3172 val=0.3156 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.3157 val=0.3148 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.3140 val=0.3135 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.3137 val=0.3118 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.3112 val=0.3114 l0=0.0000 (*)\n",
      "Epoch [31/1000] train=0.3113 val=0.3102 l0=0.0000 (*)\n",
      "Epoch [33/1000] train=0.3100 val=0.3093 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.3066 val=0.3118 l0=0.0000\n",
      "Epoch [41/1000] train=0.3065 val=0.3072 l0=0.0000 (*)\n",
      "Epoch [48/1000] train=0.3049 val=0.3066 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.3065 val=0.3085 l0=0.0000\n",
      "Epoch [52/1000] train=0.3048 val=0.3060 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.3054 val=0.3168 l0=0.0000\n",
      "Epoch [70/1000] train=0.3019 val=0.3053 l0=0.0000 (*)\n",
      "Epoch [72/1000] train=0.3027 val=0.3047 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.3012 val=0.3054 l0=0.0000\n",
      "Epoch [90/1000] train=0.3000 val=0.3058 l0=0.0000\n",
      "Epoch [100/1000] train=0.2987 val=0.3057 l0=0.0000\n",
      "Epoch [110/1000] train=0.2974 val=0.3084 l0=0.0000\n",
      "Epoch [120/1000] train=0.2953 val=0.3064 l0=0.0000\n",
      "Epoch [130/1000] train=0.2941 val=0.3087 l0=0.0000\n",
      "Epoch [140/1000] train=0.2922 val=0.3103 l0=0.0000\n",
      "Epoch [150/1000] train=0.2903 val=0.3129 l0=0.0000\n",
      "Epoch [160/1000] train=0.2868 val=0.3114 l0=0.0000\n",
      "Epoch [170/1000] train=0.2828 val=0.3173 l0=0.0000\n",
      "Epoch [180/1000] train=0.2806 val=0.3218 l0=0.0000\n",
      "Epoch [190/1000] train=0.2770 val=0.3235 l0=0.0000\n",
      "Epoch [200/1000] train=0.2740 val=0.3252 l0=0.0000\n",
      "Epoch [210/1000] train=0.2703 val=0.3367 l0=0.0000\n",
      "Epoch [220/1000] train=0.2672 val=0.3497 l0=0.0000\n",
      "Epoch [230/1000] train=0.2634 val=0.3457 l0=0.0000\n",
      "Epoch [240/1000] train=0.2612 val=0.3523 l0=0.0000\n",
      "Epoch [250/1000] train=0.2589 val=0.3512 l0=0.0000\n",
      "Epoch [260/1000] train=0.2564 val=0.3749 l0=0.0000\n",
      "Epoch [270/1000] train=0.2548 val=0.3782 l0=0.0000\n",
      "Early stopping at epoch 272 (best val=0.3047)\n",
      "Restored best model (val=0.3047)\n",
      "\n",
      "=== Breast Cancer (classification) ===\n",
      "Epoch [1/1000] train=0.6558 val=0.6630 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5348 val=0.4946 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.4445 val=0.3261 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.2733 val=0.2341 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.2302 val=0.1782 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.2362 val=0.1246 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1803 val=0.0992 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1395 val=0.0912 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1281 val=0.0751 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1188 val=0.0656 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.1107 val=0.0557 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.0998 val=0.0500 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.0969 val=0.0457 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.0879 val=0.0455 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.0857 val=0.0389 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.0812 val=0.0374 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.0793 val=0.0359 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.0746 val=0.0328 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.0726 val=0.0314 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.0717 val=0.0296 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.0687 val=0.0292 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.0664 val=0.0262 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.0658 val=0.0256 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.0631 val=0.0252 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.0613 val=0.0239 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.0639 val=0.0225 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.0575 val=0.0224 l0=0.0000 (*)\n",
      "Epoch [31/1000] train=0.0565 val=0.0188 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.0567 val=0.0182 l0=0.0000 (*)\n",
      "Epoch [35/1000] train=0.2411 val=0.0175 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.0557 val=0.0172 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.0524 val=0.0167 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0513 val=0.0166 l0=0.0000 (*)\n",
      "Epoch [41/1000] train=0.0450 val=0.0144 l0=0.0000 (*)\n",
      "Epoch [42/1000] train=0.0644 val=0.0141 l0=0.0000 (*)\n",
      "Epoch [43/1000] train=0.0447 val=0.0138 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.0416 val=0.0135 l0=0.0000 (*)\n",
      "Epoch [47/1000] train=0.0477 val=0.0128 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.0414 val=0.0121 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0384 val=0.0107 l0=0.0000 (*)\n",
      "Epoch [55/1000] train=0.0354 val=0.0100 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0372 val=0.0106 l0=0.0000\n",
      "Epoch [62/1000] train=0.0326 val=0.0087 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.0265 val=0.0082 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.0295 val=0.0230 l0=0.0000\n",
      "Epoch [90/1000] train=0.0333 val=0.0194 l0=0.0000\n",
      "Epoch [100/1000] train=0.0157 val=0.0402 l0=0.0000\n",
      "Epoch [110/1000] train=0.0072 val=0.0393 l0=0.0000\n",
      "Epoch [120/1000] train=0.0060 val=0.0441 l0=0.0000\n",
      "Epoch [130/1000] train=0.0192 val=0.0829 l0=0.0000\n",
      "Epoch [140/1000] train=0.0166 val=0.0503 l0=0.0000\n",
      "Epoch [150/1000] train=0.0037 val=0.0458 l0=0.0000\n",
      "Epoch [160/1000] train=0.0010 val=0.0723 l0=0.0000\n",
      "Epoch [170/1000] train=0.0008 val=0.0761 l0=0.0000\n",
      "Epoch [180/1000] train=0.0007 val=0.0749 l0=0.0000\n",
      "Epoch [190/1000] train=0.0006 val=0.0706 l0=0.0000\n",
      "Epoch [200/1000] train=0.0005 val=0.0786 l0=0.0000\n",
      "Epoch [210/1000] train=0.0004 val=0.0727 l0=0.0000\n",
      "Epoch [220/1000] train=0.0004 val=0.0738 l0=0.0000\n",
      "Epoch [230/1000] train=0.0003 val=0.0757 l0=0.0000\n",
      "Epoch [240/1000] train=0.0003 val=0.0765 l0=0.0000\n",
      "Epoch [250/1000] train=0.0005 val=0.0821 l0=0.0000\n",
      "Epoch [260/1000] train=0.0063 val=0.0398 l0=0.0000\n",
      "Epoch [270/1000] train=0.0034 val=0.0934 l0=0.0000\n",
      "Early stopping at epoch 270 (best val=0.0082)\n",
      "Restored best model (val=0.0082)\n",
      "Epoch [1/1000] train=0.7932 val=0.6526 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5425 val=0.6351 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.4855 val=0.5750 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5041 val=0.4992 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.3802 val=0.3930 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.3151 val=0.3168 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.3103 val=0.2627 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.2585 val=0.2587 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.2467 val=0.1958 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2095 val=0.2167 l0=0.0000\n",
      "Epoch [22/1000] train=0.1958 val=0.1518 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1737 val=0.1387 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.1532 val=0.1225 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1454 val=0.1057 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.1665 val=0.0814 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.1095 val=0.0767 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1087 val=0.1179 l0=0.0000\n",
      "Epoch [34/1000] train=0.1228 val=0.0579 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.2048 val=0.0457 l0=0.0000 (*)\n",
      "Epoch [38/1000] train=0.0881 val=0.0441 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.0894 val=0.0434 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0938 val=0.0939 l0=0.0000\n",
      "Epoch [41/1000] train=0.0837 val=0.0411 l0=0.0000 (*)\n",
      "Epoch [43/1000] train=0.0932 val=0.0382 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.0711 val=0.0292 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1674 val=0.1014 l0=0.0000\n",
      "Epoch [57/1000] train=0.0748 val=0.0225 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1363 val=0.0246 l0=0.0000\n",
      "Epoch [62/1000] train=0.0664 val=0.0195 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.0814 val=0.0306 l0=0.0000\n",
      "Epoch [79/1000] train=0.0973 val=0.0146 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.0469 val=0.0631 l0=0.0000\n",
      "Epoch [84/1000] train=0.0692 val=0.0121 l0=0.0000 (*)\n",
      "Epoch [88/1000] train=0.0842 val=0.0116 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.0395 val=0.0163 l0=0.0000\n",
      "Epoch [100/1000] train=0.0624 val=0.0243 l0=0.0000\n",
      "Epoch [110/1000] train=0.0793 val=0.0231 l0=0.0000\n",
      "Epoch [120/1000] train=0.0552 val=0.0187 l0=0.0000\n",
      "Epoch [130/1000] train=0.0891 val=0.0406 l0=0.0000\n",
      "Epoch [140/1000] train=0.0391 val=0.0451 l0=0.0000\n",
      "Epoch [150/1000] train=0.0339 val=0.5440 l0=0.0000\n",
      "Epoch [160/1000] train=0.0380 val=0.0093 l0=0.0000 (*)\n",
      "Epoch [170/1000] train=0.0354 val=0.0403 l0=0.0000\n",
      "Epoch [180/1000] train=0.0650 val=0.3965 l0=0.0000\n",
      "Epoch [190/1000] train=0.0513 val=0.0586 l0=0.0000\n",
      "Epoch [200/1000] train=0.0332 val=0.0186 l0=0.0000\n",
      "Epoch [210/1000] train=0.0589 val=0.4146 l0=0.0000\n",
      "Epoch [220/1000] train=0.0337 val=0.1124 l0=0.0000\n",
      "Epoch [230/1000] train=0.0136 val=0.1331 l0=0.0000\n",
      "Epoch [240/1000] train=0.0659 val=0.1043 l0=0.0000\n",
      "Epoch [250/1000] train=0.0165 val=0.4403 l0=0.0000\n",
      "Epoch [260/1000] train=0.0254 val=0.0833 l0=0.0000\n",
      "Epoch [270/1000] train=0.0091 val=0.0397 l0=0.0000\n",
      "Epoch [280/1000] train=0.0426 val=0.0565 l0=0.0000\n",
      "Epoch [290/1000] train=0.1351 val=0.1743 l0=0.0000\n",
      "Epoch [300/1000] train=0.0101 val=0.0630 l0=0.0000\n",
      "Epoch [310/1000] train=0.1935 val=0.3031 l0=0.0000\n",
      "Epoch [320/1000] train=0.0198 val=0.2646 l0=0.0000\n",
      "Epoch [330/1000] train=0.0313 val=0.3579 l0=0.0000\n",
      "Epoch [340/1000] train=0.0317 val=0.2155 l0=0.0000\n",
      "Epoch [350/1000] train=0.0213 val=0.0676 l0=0.0000\n",
      "Epoch [360/1000] train=0.0090 val=0.3939 l0=0.0000\n",
      "Early stopping at epoch 360 (best val=0.0093)\n",
      "Restored best model (val=0.0093)\n",
      "Epoch [1/1000] train=0.6617 val=0.6585 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5003 val=0.4811 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3563 val=0.3239 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.2645 val=0.2412 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3398 val=0.1923 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1787 val=0.1588 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1449 val=0.1084 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1211 val=0.0930 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1087 val=0.0871 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.0974 val=0.0736 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.0892 val=0.0731 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.0841 val=0.0659 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.0928 val=0.0639 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.0709 val=0.0636 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.0675 val=0.0619 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.0600 val=0.0640 l0=0.0000\n",
      "Epoch [23/1000] train=0.0502 val=0.0584 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.0487 val=0.0582 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.0481 val=0.0577 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.0458 val=0.0523 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.0480 val=0.0408 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.3200 val=0.0379 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0356 val=0.0598 l0=0.0000\n",
      "Epoch [50/1000] train=0.0297 val=0.0593 l0=0.0000\n",
      "Epoch [60/1000] train=0.0263 val=0.0580 l0=0.0000\n",
      "Epoch [70/1000] train=0.0193 val=0.1173 l0=0.0000\n",
      "Epoch [80/1000] train=0.0289 val=0.0744 l0=0.0000\n",
      "Epoch [90/1000] train=0.0101 val=0.0833 l0=0.0000\n",
      "Epoch [100/1000] train=0.0105 val=0.0682 l0=0.0000\n",
      "Epoch [110/1000] train=0.0031 val=0.0663 l0=0.0000\n",
      "Epoch [120/1000] train=0.0016 val=0.0833 l0=0.0000\n",
      "Epoch [130/1000] train=0.0013 val=0.0958 l0=0.0000\n",
      "Epoch [140/1000] train=0.0009 val=0.0938 l0=0.0000\n",
      "Epoch [150/1000] train=0.0008 val=0.0744 l0=0.0000\n",
      "Epoch [160/1000] train=0.0006 val=0.0943 l0=0.0000\n",
      "Epoch [170/1000] train=0.0005 val=0.0979 l0=0.0000\n",
      "Epoch [180/1000] train=0.0004 val=0.0980 l0=0.0000\n",
      "Epoch [190/1000] train=0.0004 val=0.1026 l0=0.0000\n",
      "Epoch [200/1000] train=0.0003 val=0.1092 l0=0.0000\n",
      "Epoch [210/1000] train=0.0003 val=0.1149 l0=0.0000\n",
      "Epoch [220/1000] train=0.0003 val=0.1136 l0=0.0000\n",
      "Epoch [230/1000] train=0.0002 val=0.1139 l0=0.0000\n",
      "Early stopping at epoch 236 (best val=0.0379)\n",
      "Restored best model (val=0.0379)\n",
      "Epoch [1/1000] train=0.7914 val=0.6550 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5514 val=0.6473 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.4910 val=0.5728 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5388 val=0.5302 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.4459 val=0.5242 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.4470 val=0.4173 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.4000 val=0.4127 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.3752 val=0.3487 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.3984 val=0.3052 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.3017 val=0.2968 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.2519 val=0.2596 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2785 val=0.2183 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.2226 val=0.2071 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1868 val=0.1661 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.1913 val=0.1326 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1367 val=0.1220 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.1022 val=0.0961 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.0930 val=0.0838 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1109 val=0.4625 l0=0.0000\n",
      "Epoch [43/1000] train=0.0812 val=0.0662 l0=0.0000 (*)\n",
      "Epoch [48/1000] train=0.1084 val=0.0546 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0622 val=0.0685 l0=0.0000\n",
      "Epoch [51/1000] train=0.0697 val=0.0544 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0848 val=0.0532 l0=0.0000 (*)\n",
      "Epoch [66/1000] train=0.0687 val=0.0530 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.0714 val=0.0557 l0=0.0000\n",
      "Epoch [73/1000] train=0.0566 val=0.0486 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.0701 val=0.0597 l0=0.0000\n",
      "Epoch [90/1000] train=0.3259 val=0.0591 l0=0.0000\n",
      "Epoch [100/1000] train=0.0947 val=0.0559 l0=0.0000\n",
      "Epoch [110/1000] train=0.0568 val=0.0991 l0=0.0000\n",
      "Epoch [120/1000] train=0.0817 val=0.0681 l0=0.0000\n",
      "Epoch [130/1000] train=0.0537 val=0.0754 l0=0.0000\n",
      "Epoch [140/1000] train=0.0747 val=0.0639 l0=0.0000\n",
      "Epoch [150/1000] train=0.0267 val=0.1078 l0=0.0000\n",
      "Epoch [160/1000] train=0.0678 val=0.1954 l0=0.0000\n",
      "Epoch [170/1000] train=0.0644 val=0.2687 l0=0.0000\n",
      "Epoch [180/1000] train=0.0478 val=0.1307 l0=0.0000\n",
      "Epoch [190/1000] train=0.0966 val=0.0684 l0=0.0000\n",
      "Epoch [200/1000] train=0.0198 val=0.3749 l0=0.0000\n",
      "Epoch [210/1000] train=0.0799 val=0.0727 l0=0.0000\n",
      "Epoch [220/1000] train=0.0349 val=0.0952 l0=0.0000\n",
      "Epoch [230/1000] train=0.0262 val=0.1344 l0=0.0000\n",
      "Epoch [240/1000] train=0.0417 val=0.0665 l0=0.0000\n",
      "Epoch [250/1000] train=0.0143 val=0.1122 l0=0.0000\n",
      "Epoch [260/1000] train=0.0133 val=0.1123 l0=0.0000\n",
      "Epoch [270/1000] train=0.0104 val=0.6120 l0=0.0000\n",
      "Early stopping at epoch 273 (best val=0.0486)\n",
      "Restored best model (val=0.0486)\n",
      "Epoch [1/1000] train=0.6881 val=0.6463 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.4980 val=0.4521 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3532 val=0.2951 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.2678 val=0.2221 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.2520 val=0.1760 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1829 val=0.1411 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1474 val=0.1133 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1537 val=0.0889 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1143 val=0.0814 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.2293 val=0.0764 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.0983 val=0.0669 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.2686 val=0.0603 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.0822 val=0.0556 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.0709 val=0.0544 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.0708 val=0.0445 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.0561 val=0.0470 l0=0.0000\n",
      "Epoch [21/1000] train=0.0537 val=0.0416 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.0541 val=0.0386 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.0328 val=0.0546 l0=0.0000\n",
      "Epoch [40/1000] train=0.0190 val=0.0496 l0=0.0000\n",
      "Epoch [50/1000] train=0.0091 val=0.0692 l0=0.0000\n",
      "Epoch [60/1000] train=0.0051 val=0.0607 l0=0.0000\n",
      "Epoch [70/1000] train=0.0036 val=0.0700 l0=0.0000\n",
      "Epoch [80/1000] train=0.0025 val=0.0553 l0=0.0000\n",
      "Epoch [90/1000] train=0.0019 val=0.0563 l0=0.0000\n",
      "Epoch [100/1000] train=0.0015 val=0.0580 l0=0.0000\n",
      "Epoch [110/1000] train=0.0012 val=0.0625 l0=0.0000\n",
      "Epoch [120/1000] train=0.0010 val=0.0615 l0=0.0000\n",
      "Epoch [130/1000] train=0.0008 val=0.0579 l0=0.0000\n",
      "Epoch [140/1000] train=0.0007 val=0.0745 l0=0.0000\n",
      "Epoch [150/1000] train=0.0006 val=0.0525 l0=0.0000\n",
      "Epoch [160/1000] train=0.0005 val=0.0518 l0=0.0000\n",
      "Epoch [170/1000] train=0.0005 val=0.0485 l0=0.0000\n",
      "Epoch [180/1000] train=0.0004 val=0.0521 l0=0.0000\n",
      "Epoch [190/1000] train=0.0003 val=0.0537 l0=0.0000\n",
      "Epoch [200/1000] train=0.0003 val=0.0527 l0=0.0000\n",
      "Epoch [210/1000] train=0.0003 val=0.0529 l0=0.0000\n",
      "Epoch [220/1000] train=0.0002 val=0.0580 l0=0.0000\n",
      "Early stopping at epoch 222 (best val=0.0386)\n",
      "Restored best model (val=0.0386)\n",
      "Epoch [1/1000] train=0.7914 val=0.6567 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.4939 val=0.5680 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5335 val=0.5591 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.4672 val=0.4828 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.4165 val=0.4362 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.4026 val=0.3704 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.3760 val=0.3271 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.2870 val=0.3041 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.2670 val=0.2643 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.2366 val=0.2341 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.2264 val=0.2041 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.2123 val=0.1835 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1791 val=0.1671 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.1562 val=0.1547 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.1559 val=0.1127 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.1222 val=0.1115 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.1110 val=0.0854 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1091 val=0.1023 l0=0.0000\n",
      "Epoch [31/1000] train=0.1107 val=0.0744 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.0813 val=0.0680 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.0835 val=0.0659 l0=0.0000 (*)\n",
      "Epoch [38/1000] train=0.0819 val=0.0602 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.0715 val=0.0595 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0672 val=0.0631 l0=0.0000\n",
      "Epoch [47/1000] train=0.0636 val=0.0522 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1453 val=0.1022 l0=0.0000\n",
      "Epoch [52/1000] train=0.0640 val=0.0519 l0=0.0000 (*)\n",
      "Epoch [54/1000] train=0.0473 val=0.0502 l0=0.0000 (*)\n",
      "Epoch [55/1000] train=0.0473 val=0.0487 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0807 val=0.0562 l0=0.0000\n",
      "Epoch [70/1000] train=0.0542 val=0.0592 l0=0.0000\n",
      "Epoch [71/1000] train=0.0423 val=0.0457 l0=0.0000 (*)\n",
      "Epoch [72/1000] train=0.0293 val=0.0447 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.0379 val=0.0597 l0=0.0000\n",
      "Epoch [90/1000] train=0.0329 val=0.0522 l0=0.0000\n",
      "Epoch [100/1000] train=0.1501 val=0.0609 l0=0.0000\n",
      "Epoch [110/1000] train=0.0125 val=0.0481 l0=0.0000\n",
      "Epoch [120/1000] train=0.0304 val=0.0725 l0=0.0000\n",
      "Epoch [130/1000] train=0.0173 val=0.0768 l0=0.0000\n",
      "Epoch [140/1000] train=0.0394 val=0.0611 l0=0.0000\n",
      "Epoch [150/1000] train=0.3787 val=0.1046 l0=0.0000\n",
      "Epoch [160/1000] train=0.0117 val=0.0540 l0=0.0000\n",
      "Epoch [170/1000] train=0.0144 val=0.0773 l0=0.0000\n",
      "Epoch [180/1000] train=0.0017 val=0.0903 l0=0.0000\n",
      "Epoch [190/1000] train=0.0041 val=0.0887 l0=0.0000\n",
      "Epoch [200/1000] train=0.0011 val=0.0948 l0=0.0000\n",
      "Epoch [210/1000] train=0.0009 val=0.1032 l0=0.0000\n",
      "Epoch [220/1000] train=0.0008 val=0.1019 l0=0.0000\n",
      "Epoch [230/1000] train=0.0008 val=0.1049 l0=0.0000\n",
      "Epoch [240/1000] train=0.0007 val=0.1067 l0=0.0000\n",
      "Epoch [250/1000] train=0.0006 val=0.1084 l0=0.0000\n",
      "Epoch [260/1000] train=0.0006 val=0.1073 l0=0.0000\n",
      "Epoch [270/1000] train=0.0006 val=0.1088 l0=0.0000\n",
      "Early stopping at epoch 272 (best val=0.0447)\n",
      "Restored best model (val=0.0447)\n",
      "Epoch [1/1000] train=0.6976 val=0.6624 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5157 val=0.4512 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3825 val=0.2939 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.2762 val=0.2187 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.2355 val=0.1836 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1982 val=0.1398 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1575 val=0.1073 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1377 val=0.0888 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1255 val=0.0770 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1146 val=0.0670 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.3441 val=0.0635 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.1021 val=0.0609 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.1041 val=0.0589 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.0900 val=0.0576 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.0858 val=0.0508 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.0857 val=0.0440 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.0806 val=0.0377 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.0803 val=0.0344 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.0742 val=0.0337 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.0746 val=0.0302 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.0746 val=0.0259 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.0643 val=0.0246 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.0623 val=0.0233 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.0606 val=0.0217 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.0586 val=0.0215 l0=0.0000 (*)\n",
      "Epoch [34/1000] train=0.2666 val=0.0198 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.0498 val=0.0174 l0=0.0000 (*)\n",
      "Epoch [38/1000] train=0.0467 val=0.0166 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0444 val=0.0160 l0=0.0000 (*)\n",
      "Epoch [42/1000] train=0.0406 val=0.0142 l0=0.0000 (*)\n",
      "Epoch [44/1000] train=0.0438 val=0.0123 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0405 val=0.0172 l0=0.0000\n",
      "Epoch [51/1000] train=0.0481 val=0.0108 l0=0.0000 (*)\n",
      "Epoch [53/1000] train=0.0447 val=0.0097 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0333 val=0.0092 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.0276 val=0.0150 l0=0.0000\n",
      "Epoch [80/1000] train=0.0379 val=0.0310 l0=0.0000\n",
      "Epoch [81/1000] train=0.0300 val=0.0087 l0=0.0000 (*)\n",
      "Epoch [82/1000] train=0.0347 val=0.0079 l0=0.0000 (*)\n",
      "Epoch [84/1000] train=0.0429 val=0.0065 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.0314 val=0.0373 l0=0.0000\n",
      "Epoch [95/1000] train=0.0200 val=0.0062 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.0248 val=0.0252 l0=0.0000\n",
      "Epoch [102/1000] train=0.0230 val=0.0043 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.0155 val=0.0033 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.0173 val=0.0265 l0=0.0000\n",
      "Epoch [126/1000] train=0.0231 val=0.0028 l0=0.0000 (*)\n",
      "Epoch [129/1000] train=0.0227 val=0.0027 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.0382 val=0.0041 l0=0.0000\n",
      "Epoch [140/1000] train=0.0103 val=0.0244 l0=0.0000\n",
      "Epoch [150/1000] train=0.0961 val=0.0537 l0=0.0000\n",
      "Epoch [153/1000] train=0.0100 val=0.0020 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.0089 val=0.0032 l0=0.0000\n",
      "Epoch [162/1000] train=0.0060 val=0.0019 l0=0.0000 (*)\n",
      "Epoch [164/1000] train=0.0217 val=0.0014 l0=0.0000 (*)\n",
      "Epoch [170/1000] train=0.0279 val=0.0024 l0=0.0000\n",
      "Epoch [180/1000] train=0.0016 val=0.0441 l0=0.0000\n",
      "Epoch [183/1000] train=0.0362 val=0.0013 l0=0.0000 (*)\n",
      "Epoch [189/1000] train=0.0022 val=0.0012 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.0029 val=0.0356 l0=0.0000\n",
      "Epoch [200/1000] train=0.0008 val=0.0037 l0=0.0000\n",
      "Epoch [210/1000] train=0.0010 val=0.0017 l0=0.0000\n",
      "Epoch [215/1000] train=0.0009 val=0.0007 l0=0.0000 (*)\n",
      "Epoch [219/1000] train=0.0016 val=0.0006 l0=0.0000 (*)\n",
      "Epoch [220/1000] train=0.0112 val=0.0044 l0=0.0000\n",
      "Epoch [230/1000] train=0.0005 val=0.0040 l0=0.0000\n",
      "Epoch [240/1000] train=0.0004 val=0.0033 l0=0.0000\n",
      "Epoch [250/1000] train=0.0005 val=0.0034 l0=0.0000\n",
      "Epoch [260/1000] train=0.0003 val=0.0079 l0=0.0000\n",
      "Epoch [270/1000] train=0.0003 val=0.0048 l0=0.0000\n",
      "Epoch [280/1000] train=0.0002 val=0.0068 l0=0.0000\n",
      "Epoch [290/1000] train=0.0003 val=0.0034 l0=0.0000\n",
      "Epoch [300/1000] train=0.0002 val=0.0017 l0=0.0000\n",
      "Epoch [310/1000] train=0.0002 val=0.0033 l0=0.0000\n",
      "Epoch [320/1000] train=0.0002 val=0.0030 l0=0.0000\n",
      "Epoch [330/1000] train=0.0002 val=0.0024 l0=0.0000\n",
      "Epoch [340/1000] train=0.0002 val=0.0016 l0=0.0000\n",
      "Epoch [350/1000] train=0.0002 val=0.0002 l0=0.0000 (*)\n",
      "Epoch [360/1000] train=0.0003 val=0.0002 l0=0.0000\n",
      "Epoch [370/1000] train=0.0001 val=0.0007 l0=0.0000\n",
      "Epoch [380/1000] train=0.0001 val=0.0007 l0=0.0000\n",
      "Epoch [390/1000] train=0.0001 val=0.0004 l0=0.0000\n",
      "Epoch [400/1000] train=0.0001 val=0.0006 l0=0.0000\n",
      "Epoch [410/1000] train=0.0001 val=0.0006 l0=0.0000\n",
      "Epoch [420/1000] train=0.0001 val=0.0003 l0=0.0000\n",
      "Epoch [430/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [440/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [450/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [460/1000] train=0.0001 val=0.0004 l0=0.0000\n",
      "Epoch [470/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [480/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [490/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [500/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [510/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [520/1000] train=0.0001 val=0.0005 l0=0.0000\n",
      "Epoch [530/1000] train=0.0000 val=0.0003 l0=0.0000\n",
      "Epoch [540/1000] train=0.0000 val=0.0005 l0=0.0000\n",
      "Epoch [550/1000] train=0.0000 val=0.0005 l0=0.0000\n",
      "Early stopping at epoch 550 (best val=0.0002)\n",
      "Restored best model (val=0.0002)\n",
      "Epoch [1/1000] train=0.7903 val=0.6547 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5398 val=0.6358 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5665 val=0.6016 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.4901 val=0.5200 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.4361 val=0.5109 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.4451 val=0.4365 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.3928 val=0.3557 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.3373 val=0.3248 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.2816 val=0.2867 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.2587 val=0.2569 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.2296 val=0.2334 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.2399 val=0.1978 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.2002 val=0.1951 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1912 val=0.1703 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.1842 val=0.1669 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.1726 val=0.1480 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.1648 val=0.1185 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1700 val=0.1110 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.1375 val=0.1044 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1317 val=0.0986 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.1252 val=0.0757 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1192 val=0.0948 l0=0.0000\n",
      "Epoch [32/1000] train=0.1084 val=0.0574 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.0852 val=0.0486 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0989 val=0.0480 l0=0.0000 (*)\n",
      "Epoch [42/1000] train=0.0892 val=0.0474 l0=0.0000 (*)\n",
      "Epoch [44/1000] train=0.0868 val=0.0423 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.0860 val=0.0402 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0699 val=0.0563 l0=0.0000\n",
      "Epoch [53/1000] train=0.0687 val=0.0382 l0=0.0000 (*)\n",
      "Epoch [54/1000] train=0.0782 val=0.0354 l0=0.0000 (*)\n",
      "Epoch [56/1000] train=0.0746 val=0.0334 l0=0.0000 (*)\n",
      "Epoch [59/1000] train=0.0676 val=0.0302 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0681 val=0.0478 l0=0.0000\n",
      "Epoch [70/1000] train=0.0971 val=0.0640 l0=0.0000\n",
      "Epoch [73/1000] train=0.0621 val=0.0262 l0=0.0000 (*)\n",
      "Epoch [79/1000] train=0.0602 val=0.0259 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.0550 val=0.0229 l0=0.0000 (*)\n",
      "Epoch [89/1000] train=0.0523 val=0.0209 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.0527 val=0.0209 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.0919 val=0.0501 l0=0.0000\n",
      "Epoch [107/1000] train=0.0932 val=0.0199 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.0559 val=0.0190 l0=0.0000 (*)\n",
      "Epoch [118/1000] train=0.0487 val=0.0169 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1092 val=0.0344 l0=0.0000\n",
      "Epoch [130/1000] train=0.0638 val=0.0349 l0=0.0000\n",
      "Epoch [133/1000] train=0.2161 val=0.0159 l0=0.0000 (*)\n",
      "Epoch [135/1000] train=0.0413 val=0.0151 l0=0.0000 (*)\n",
      "Epoch [136/1000] train=0.0445 val=0.0141 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.0500 val=0.0231 l0=0.0000\n",
      "Epoch [148/1000] train=0.0476 val=0.0109 l0=0.0000 (*)\n",
      "Epoch [150/1000] train=0.0517 val=0.0177 l0=0.0000\n",
      "Epoch [152/1000] train=0.0397 val=0.0105 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.0623 val=0.0166 l0=0.0000\n",
      "Epoch [165/1000] train=0.0579 val=0.0072 l0=0.0000 (*)\n",
      "Epoch [170/1000] train=0.0558 val=0.0423 l0=0.0000\n",
      "Epoch [180/1000] train=0.0443 val=0.0788 l0=0.0000\n",
      "Epoch [190/1000] train=0.0481 val=0.0089 l0=0.0000\n",
      "Epoch [200/1000] train=0.0484 val=0.0105 l0=0.0000\n",
      "Epoch [210/1000] train=0.0413 val=0.0430 l0=0.0000\n",
      "Epoch [220/1000] train=0.0270 val=0.0063 l0=0.0000 (*)\n",
      "Epoch [221/1000] train=0.0232 val=0.0036 l0=0.0000 (*)\n",
      "Epoch [230/1000] train=0.0231 val=0.0057 l0=0.0000\n",
      "Epoch [240/1000] train=0.0402 val=0.0239 l0=0.0000\n",
      "Epoch [250/1000] train=0.0270 val=0.0646 l0=0.0000\n",
      "Epoch [260/1000] train=0.0224 val=0.0036 l0=0.0000 (*)\n",
      "Epoch [269/1000] train=0.0258 val=0.0021 l0=0.0000 (*)\n",
      "Epoch [270/1000] train=0.0259 val=0.0360 l0=0.0000\n",
      "Epoch [280/1000] train=0.0446 val=0.0845 l0=0.0000\n",
      "Epoch [290/1000] train=0.0695 val=0.0211 l0=0.0000\n",
      "Epoch [300/1000] train=0.0334 val=0.1052 l0=0.0000\n",
      "Epoch [310/1000] train=0.0858 val=0.0249 l0=0.0000\n",
      "Epoch [320/1000] train=0.0235 val=0.0023 l0=0.0000\n",
      "Epoch [330/1000] train=0.0076 val=0.0033 l0=0.0000\n",
      "Epoch [340/1000] train=0.1025 val=0.0116 l0=0.0000\n",
      "Epoch [350/1000] train=0.0320 val=0.0238 l0=0.0000\n",
      "Epoch [360/1000] train=0.1459 val=0.0342 l0=0.0000\n",
      "Epoch [370/1000] train=0.0032 val=0.0249 l0=0.0000\n",
      "Epoch [380/1000] train=0.0239 val=0.0681 l0=0.0000\n",
      "Epoch [390/1000] train=0.0253 val=0.0449 l0=0.0000\n",
      "Epoch [400/1000] train=0.0157 val=0.0341 l0=0.0000\n",
      "Epoch [410/1000] train=0.0352 val=0.0771 l0=0.0000\n",
      "Epoch [420/1000] train=0.0192 val=0.1335 l0=0.0000\n",
      "Epoch [430/1000] train=0.0023 val=0.0742 l0=0.0000\n",
      "Epoch [440/1000] train=0.0004 val=0.0488 l0=0.0000\n",
      "Epoch [450/1000] train=0.0003 val=0.0498 l0=0.0000\n",
      "Epoch [460/1000] train=0.0003 val=0.0481 l0=0.0000\n",
      "Early stopping at epoch 469 (best val=0.0021)\n",
      "Restored best model (val=0.0021)\n",
      "Epoch [1/1000] train=0.6430 val=0.4912 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5378 val=0.3477 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3791 val=0.2697 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.2885 val=0.2141 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.2293 val=0.1971 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1896 val=0.1650 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1528 val=0.1398 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1307 val=0.1313 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1277 val=0.1239 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1069 val=0.1016 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.0714 val=0.1008 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.0750 val=0.0947 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2195 val=0.0945 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.0588 val=0.0928 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.0565 val=0.0877 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.0563 val=0.0846 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.0545 val=0.0829 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.0533 val=0.0694 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.0615 val=0.0613 l0=0.0000 (*)\n",
      "Epoch [35/1000] train=0.0495 val=0.0597 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.0479 val=0.0596 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.0490 val=0.0592 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0557 val=0.0496 l0=0.0000 (*)\n",
      "Epoch [46/1000] train=0.0447 val=0.0481 l0=0.0000 (*)\n",
      "Epoch [47/1000] train=0.0418 val=0.0437 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0389 val=0.0467 l0=0.0000\n",
      "Epoch [51/1000] train=0.0382 val=0.0429 l0=0.0000 (*)\n",
      "Epoch [52/1000] train=0.0445 val=0.0372 l0=0.0000 (*)\n",
      "Epoch [57/1000] train=0.0366 val=0.0305 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0337 val=0.0427 l0=0.0000\n",
      "Epoch [63/1000] train=0.0329 val=0.0295 l0=0.0000 (*)\n",
      "Epoch [67/1000] train=0.0501 val=0.0285 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.0315 val=0.0206 l0=0.0000 (*)\n",
      "Epoch [73/1000] train=0.0314 val=0.0183 l0=0.0000 (*)\n",
      "Epoch [75/1000] train=0.0283 val=0.0140 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.0273 val=0.0190 l0=0.0000\n",
      "Epoch [90/1000] train=0.0202 val=0.0086 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1015 val=0.2136 l0=0.0000\n",
      "Epoch [104/1000] train=0.0170 val=0.0070 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.0139 val=0.3006 l0=0.0000\n",
      "Epoch [120/1000] train=0.0127 val=0.0644 l0=0.0000\n",
      "Epoch [125/1000] train=0.0208 val=0.0036 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.0496 val=0.0106 l0=0.0000\n",
      "Epoch [140/1000] train=0.0049 val=0.0440 l0=0.0000\n",
      "Epoch [150/1000] train=0.0022 val=0.2206 l0=0.0000\n",
      "Epoch [160/1000] train=0.0013 val=0.0367 l0=0.0000\n",
      "Epoch [170/1000] train=0.0011 val=0.0415 l0=0.0000\n",
      "Epoch [180/1000] train=0.0008 val=0.0474 l0=0.0000\n",
      "Epoch [190/1000] train=0.0006 val=0.0453 l0=0.0000\n",
      "Epoch [200/1000] train=0.0007 val=0.0596 l0=0.0000\n",
      "Epoch [210/1000] train=0.0005 val=0.0492 l0=0.0000\n",
      "Epoch [220/1000] train=0.0004 val=0.0403 l0=0.0000\n",
      "Epoch [230/1000] train=0.0004 val=0.1511 l0=0.0000\n",
      "Epoch [240/1000] train=0.0003 val=0.0347 l0=0.0000\n",
      "Epoch [250/1000] train=0.0003 val=0.0291 l0=0.0000\n",
      "Epoch [260/1000] train=0.0003 val=0.0320 l0=0.0000\n",
      "Epoch [270/1000] train=0.0002 val=0.0283 l0=0.0000\n",
      "Epoch [280/1000] train=0.0002 val=0.0279 l0=0.0000\n",
      "Epoch [290/1000] train=0.0002 val=0.0275 l0=0.0000\n",
      "Epoch [300/1000] train=0.0002 val=0.0244 l0=0.0000\n",
      "Epoch [310/1000] train=0.0002 val=0.0244 l0=0.0000\n",
      "Epoch [320/1000] train=0.0002 val=0.0216 l0=0.0000\n",
      "Early stopping at epoch 325 (best val=0.0036)\n",
      "Restored best model (val=0.0036)\n",
      "Epoch [1/1000] train=0.8001 val=0.8161 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.7245 val=0.7020 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.6647 val=0.5964 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.6824 val=0.5436 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.6287 val=0.5430 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.6240 val=0.4942 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5666 val=0.4393 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5514 val=0.4152 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.4847 val=0.4099 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.4681 val=0.4264 l0=0.0000\n",
      "Epoch [11/1000] train=0.4608 val=0.3702 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.4036 val=0.3108 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.3649 val=0.3092 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.4238 val=0.2697 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.3348 val=0.2506 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.2856 val=0.2254 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.2577 val=0.2022 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.2516 val=0.1882 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.2058 val=0.1590 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.1896 val=0.1392 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.2156 val=0.1214 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.2200 val=0.1111 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.1818 val=0.1105 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1869 val=0.1356 l0=0.0000\n",
      "Epoch [37/1000] train=0.1376 val=0.1089 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.1032 val=0.1079 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1036 val=0.1604 l0=0.0000\n",
      "Epoch [41/1000] train=0.1040 val=0.0934 l0=0.0000 (*)\n",
      "Epoch [42/1000] train=0.1360 val=0.0781 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0838 val=0.0832 l0=0.0000\n",
      "Epoch [52/1000] train=0.0789 val=0.0775 l0=0.0000 (*)\n",
      "Epoch [54/1000] train=0.0729 val=0.0731 l0=0.0000 (*)\n",
      "Epoch [59/1000] train=0.0745 val=0.0645 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0627 val=0.0650 l0=0.0000\n",
      "Epoch [67/1000] train=0.0625 val=0.0559 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.0874 val=0.1053 l0=0.0000\n",
      "Epoch [73/1000] train=0.0702 val=0.0472 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.2390 val=0.0538 l0=0.0000\n",
      "Epoch [90/1000] train=0.0494 val=0.0606 l0=0.0000\n",
      "Epoch [91/1000] train=0.0508 val=0.0416 l0=0.0000 (*)\n",
      "Epoch [96/1000] train=0.0585 val=0.0389 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1886 val=0.0373 l0=0.0000 (*)\n",
      "Epoch [106/1000] train=0.0427 val=0.0359 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.0416 val=0.0426 l0=0.0000\n",
      "Epoch [120/1000] train=0.0493 val=0.0625 l0=0.0000\n",
      "Epoch [126/1000] train=0.0451 val=0.0314 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1164 val=0.0435 l0=0.0000\n",
      "Epoch [140/1000] train=0.0363 val=0.0932 l0=0.0000\n",
      "Epoch [150/1000] train=0.0469 val=0.0441 l0=0.0000\n",
      "Epoch [152/1000] train=0.0341 val=0.0296 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.0440 val=0.0714 l0=0.0000\n",
      "Epoch [170/1000] train=0.1699 val=0.0495 l0=0.0000\n",
      "Epoch [180/1000] train=0.0339 val=0.0568 l0=0.0000\n",
      "Epoch [184/1000] train=0.0989 val=0.0286 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.0289 val=0.0659 l0=0.0000\n",
      "Epoch [200/1000] train=0.0726 val=0.0881 l0=0.0000\n",
      "Epoch [210/1000] train=0.0251 val=0.0328 l0=0.0000\n",
      "Epoch [220/1000] train=0.0362 val=0.0242 l0=0.0000 (*)\n",
      "Epoch [221/1000] train=0.0585 val=0.0145 l0=0.0000 (*)\n",
      "Epoch [230/1000] train=0.0403 val=0.0376 l0=0.0000\n",
      "Epoch [240/1000] train=0.0167 val=0.0227 l0=0.0000\n",
      "Epoch [243/1000] train=0.0181 val=0.0112 l0=0.0000 (*)\n",
      "Epoch [250/1000] train=0.0666 val=0.0362 l0=0.0000\n",
      "Epoch [260/1000] train=0.0243 val=0.0234 l0=0.0000\n",
      "Epoch [270/1000] train=0.0196 val=0.0319 l0=0.0000\n",
      "Epoch [280/1000] train=0.0057 val=0.0092 l0=0.0000 (*)\n",
      "Epoch [290/1000] train=0.0038 val=0.0433 l0=0.0000\n",
      "Epoch [300/1000] train=0.2226 val=0.0266 l0=0.0000\n",
      "Epoch [310/1000] train=0.0033 val=0.0365 l0=0.0000\n",
      "Epoch [320/1000] train=0.0043 val=0.0507 l0=0.0000\n",
      "Epoch [330/1000] train=0.0012 val=0.0449 l0=0.0000\n",
      "Epoch [340/1000] train=0.0043 val=0.0520 l0=0.0000\n",
      "Epoch [350/1000] train=0.0006 val=0.0480 l0=0.0000\n",
      "Epoch [360/1000] train=0.0006 val=0.0369 l0=0.0000\n",
      "Epoch [370/1000] train=0.0005 val=0.0474 l0=0.0000\n",
      "Epoch [380/1000] train=0.0005 val=0.0569 l0=0.0000\n",
      "Epoch [390/1000] train=0.0004 val=0.0555 l0=0.0000\n",
      "Epoch [400/1000] train=0.0003 val=0.0568 l0=0.0000\n",
      "Epoch [410/1000] train=0.0003 val=0.0566 l0=0.0000\n",
      "Epoch [420/1000] train=0.0003 val=0.0582 l0=0.0000\n",
      "Epoch [430/1000] train=0.0003 val=0.0582 l0=0.0000\n",
      "Epoch [440/1000] train=0.0003 val=0.0575 l0=0.0000\n",
      "Epoch [450/1000] train=0.0003 val=0.0587 l0=0.0000\n",
      "Epoch [460/1000] train=0.0012 val=0.0559 l0=0.0000\n",
      "Epoch [470/1000] train=0.0360 val=0.0578 l0=0.0000\n",
      "Epoch [480/1000] train=0.0024 val=0.0579 l0=0.0000\n",
      "Early stopping at epoch 480 (best val=0.0092)\n",
      "Restored best model (val=0.0092)\n",
      "\n",
      "=== California Housing (regression) ===\n",
      "Epoch [1/1000] train=0.2708 val=0.1778 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.1823 val=0.1675 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.1731 val=0.1501 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1675 val=0.1468 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1638 val=0.1428 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1584 val=0.1463 l0=0.0000\n",
      "Epoch [13/1000] train=0.1558 val=0.1366 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.1519 val=0.1364 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1476 val=0.1348 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.1496 val=0.1325 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.1444 val=0.1277 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1414 val=0.1236 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1371 val=0.1221 l0=0.0000 (*)\n",
      "Epoch [33/1000] train=0.1341 val=0.1216 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.1330 val=0.1171 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1336 val=0.1215 l0=0.0000\n",
      "Epoch [41/1000] train=0.1306 val=0.1166 l0=0.0000 (*)\n",
      "Epoch [42/1000] train=0.1302 val=0.1148 l0=0.0000 (*)\n",
      "Epoch [43/1000] train=0.1300 val=0.1147 l0=0.0000 (*)\n",
      "Epoch [47/1000] train=0.1291 val=0.1142 l0=0.0000 (*)\n",
      "Epoch [48/1000] train=0.1282 val=0.1136 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1285 val=0.1218 l0=0.0000\n",
      "Epoch [60/1000] train=0.1248 val=0.1124 l0=0.0000 (*)\n",
      "Epoch [68/1000] train=0.1228 val=0.1116 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1221 val=0.1158 l0=0.0000\n",
      "Epoch [72/1000] train=0.1229 val=0.1115 l0=0.0000 (*)\n",
      "Epoch [73/1000] train=0.1208 val=0.1104 l0=0.0000 (*)\n",
      "Epoch [79/1000] train=0.1195 val=0.1103 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1190 val=0.1099 l0=0.0000 (*)\n",
      "Epoch [81/1000] train=0.1206 val=0.1096 l0=0.0000 (*)\n",
      "Epoch [82/1000] train=0.1218 val=0.1093 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1184 val=0.1095 l0=0.0000\n",
      "Epoch [92/1000] train=0.1179 val=0.1073 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1177 val=0.1099 l0=0.0000\n",
      "Epoch [110/1000] train=0.1163 val=0.1083 l0=0.0000\n",
      "Epoch [116/1000] train=0.1162 val=0.1060 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1154 val=0.1152 l0=0.0000\n",
      "Epoch [130/1000] train=0.1146 val=0.1102 l0=0.0000\n",
      "Epoch [140/1000] train=0.1131 val=0.1097 l0=0.0000\n",
      "Epoch [150/1000] train=0.1123 val=0.1076 l0=0.0000\n",
      "Epoch [160/1000] train=0.1115 val=0.1075 l0=0.0000\n",
      "Epoch [170/1000] train=0.1112 val=0.1078 l0=0.0000\n",
      "Epoch [178/1000] train=0.1101 val=0.1059 l0=0.0000 (*)\n",
      "Epoch [180/1000] train=0.1096 val=0.1058 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.1075 val=0.1067 l0=0.0000\n",
      "Epoch [192/1000] train=0.1075 val=0.1053 l0=0.0000 (*)\n",
      "Epoch [200/1000] train=0.1071 val=0.1084 l0=0.0000\n",
      "Epoch [210/1000] train=0.1054 val=0.1077 l0=0.0000\n",
      "Epoch [220/1000] train=0.1053 val=0.1128 l0=0.0000\n",
      "Epoch [230/1000] train=0.1036 val=0.1071 l0=0.0000\n",
      "Epoch [240/1000] train=0.1021 val=0.1075 l0=0.0000\n",
      "Epoch [248/1000] train=0.1021 val=0.1050 l0=0.0000 (*)\n",
      "Epoch [250/1000] train=0.1023 val=0.1063 l0=0.0000\n",
      "Epoch [260/1000] train=0.1003 val=0.1069 l0=0.0000\n",
      "Epoch [270/1000] train=0.1000 val=0.1112 l0=0.0000\n",
      "Epoch [280/1000] train=0.0988 val=0.1075 l0=0.0000\n",
      "Epoch [290/1000] train=0.0982 val=0.1066 l0=0.0000\n",
      "Epoch [300/1000] train=0.0974 val=0.1070 l0=0.0000\n",
      "Epoch [310/1000] train=0.0963 val=0.1077 l0=0.0000\n",
      "Epoch [320/1000] train=0.0961 val=0.1102 l0=0.0000\n",
      "Epoch [330/1000] train=0.0951 val=0.1106 l0=0.0000\n",
      "Epoch [340/1000] train=0.0944 val=0.1098 l0=0.0000\n",
      "Epoch [350/1000] train=0.0942 val=0.1120 l0=0.0000\n",
      "Epoch [360/1000] train=0.0932 val=0.1092 l0=0.0000\n",
      "Epoch [370/1000] train=0.0929 val=0.1106 l0=0.0000\n",
      "Epoch [380/1000] train=0.0924 val=0.1091 l0=0.0000\n",
      "Epoch [390/1000] train=0.0915 val=0.1094 l0=0.0000\n",
      "Epoch [400/1000] train=0.0913 val=0.1106 l0=0.0000\n",
      "Epoch [410/1000] train=0.0900 val=0.1117 l0=0.0000\n",
      "Epoch [420/1000] train=0.0899 val=0.1105 l0=0.0000\n",
      "Epoch [430/1000] train=0.0887 val=0.1099 l0=0.0000\n",
      "Epoch [440/1000] train=0.0878 val=0.1116 l0=0.0000\n",
      "Early stopping at epoch 448 (best val=0.1050)\n",
      "Restored best model (val=0.1050)\n",
      "Epoch [1/1000] train=0.3260 val=0.2101 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.2038 val=0.1801 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1945 val=0.1732 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1798 val=0.1597 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1814 val=0.1594 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1718 val=0.1559 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1720 val=0.1475 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1703 val=0.1751 l0=0.0000\n",
      "Epoch [12/1000] train=0.1680 val=0.1473 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.1590 val=0.1446 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.1603 val=0.1376 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1606 val=0.1369 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1543 val=0.1360 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1532 val=0.1775 l0=0.0000\n",
      "Epoch [31/1000] train=0.1590 val=0.1345 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.1495 val=0.1299 l0=0.0000 (*)\n",
      "Epoch [34/1000] train=0.1527 val=0.1290 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1593 val=0.1288 l0=0.0000 (*)\n",
      "Epoch [43/1000] train=0.1510 val=0.1284 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.1488 val=0.1277 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1492 val=0.1315 l0=0.0000\n",
      "Epoch [52/1000] train=0.1525 val=0.1259 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1469 val=0.1248 l0=0.0000 (*)\n",
      "Epoch [68/1000] train=0.1478 val=0.1226 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1420 val=0.1279 l0=0.0000\n",
      "Epoch [78/1000] train=0.1482 val=0.1213 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1408 val=0.1195 l0=0.0000 (*)\n",
      "Epoch [84/1000] train=0.1409 val=0.1194 l0=0.0000 (*)\n",
      "Epoch [88/1000] train=0.1451 val=0.1189 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1393 val=0.1210 l0=0.0000\n",
      "Epoch [91/1000] train=0.1402 val=0.1185 l0=0.0000 (*)\n",
      "Epoch [94/1000] train=0.1366 val=0.1185 l0=0.0000 (*)\n",
      "Epoch [99/1000] train=0.1373 val=0.1170 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1375 val=0.1515 l0=0.0000\n",
      "Epoch [103/1000] train=0.1390 val=0.1157 l0=0.0000 (*)\n",
      "Epoch [109/1000] train=0.1371 val=0.1148 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1384 val=0.1162 l0=0.0000\n",
      "Epoch [111/1000] train=0.1358 val=0.1142 l0=0.0000 (*)\n",
      "Epoch [116/1000] train=0.1353 val=0.1142 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1360 val=0.1358 l0=0.0000\n",
      "Epoch [125/1000] train=0.1318 val=0.1127 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1331 val=0.1178 l0=0.0000\n",
      "Epoch [135/1000] train=0.1323 val=0.1110 l0=0.0000 (*)\n",
      "Epoch [139/1000] train=0.1275 val=0.1106 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1279 val=0.1232 l0=0.0000\n",
      "Epoch [144/1000] train=0.1291 val=0.1104 l0=0.0000 (*)\n",
      "Epoch [148/1000] train=0.1305 val=0.1104 l0=0.0000 (*)\n",
      "Epoch [150/1000] train=0.1295 val=0.1224 l0=0.0000\n",
      "Epoch [154/1000] train=0.1292 val=0.1099 l0=0.0000 (*)\n",
      "Epoch [158/1000] train=0.1264 val=0.1085 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.1276 val=0.1128 l0=0.0000\n",
      "Epoch [161/1000] train=0.1251 val=0.1077 l0=0.0000 (*)\n",
      "Epoch [165/1000] train=0.1250 val=0.1065 l0=0.0000 (*)\n",
      "Epoch [170/1000] train=0.1238 val=0.1132 l0=0.0000\n",
      "Epoch [180/1000] train=0.1242 val=0.1072 l0=0.0000\n",
      "Epoch [190/1000] train=0.1239 val=0.1063 l0=0.0000 (*)\n",
      "Epoch [200/1000] train=0.1220 val=0.1179 l0=0.0000\n",
      "Epoch [201/1000] train=0.1231 val=0.1063 l0=0.0000 (*)\n",
      "Epoch [210/1000] train=0.1244 val=0.1107 l0=0.0000\n",
      "Epoch [220/1000] train=0.1210 val=0.1225 l0=0.0000\n",
      "Epoch [230/1000] train=0.1200 val=0.1077 l0=0.0000\n",
      "Epoch [235/1000] train=0.1197 val=0.1056 l0=0.0000 (*)\n",
      "Epoch [240/1000] train=0.1197 val=0.1295 l0=0.0000\n",
      "Epoch [249/1000] train=0.1198 val=0.1039 l0=0.0000 (*)\n",
      "Epoch [250/1000] train=0.1190 val=0.1042 l0=0.0000\n",
      "Epoch [260/1000] train=0.1197 val=0.1191 l0=0.0000\n",
      "Epoch [270/1000] train=0.1165 val=0.1060 l0=0.0000\n",
      "Epoch [280/1000] train=0.1173 val=0.1070 l0=0.0000\n",
      "Epoch [290/1000] train=0.1164 val=0.1080 l0=0.0000\n",
      "Epoch [300/1000] train=0.1157 val=0.1125 l0=0.0000\n",
      "Epoch [310/1000] train=0.1150 val=0.1086 l0=0.0000\n",
      "Epoch [320/1000] train=0.1159 val=0.1066 l0=0.0000\n",
      "Epoch [330/1000] train=0.1157 val=0.1077 l0=0.0000\n",
      "Epoch [340/1000] train=0.1143 val=0.1145 l0=0.0000\n",
      "Epoch [346/1000] train=0.1134 val=0.1036 l0=0.0000 (*)\n",
      "Epoch [350/1000] train=0.1135 val=0.1079 l0=0.0000\n",
      "Epoch [360/1000] train=0.1129 val=0.1082 l0=0.0000\n",
      "Epoch [366/1000] train=0.1127 val=0.1030 l0=0.0000 (*)\n",
      "Epoch [370/1000] train=0.1133 val=0.1067 l0=0.0000\n",
      "Epoch [380/1000] train=0.1117 val=0.1059 l0=0.0000\n",
      "Epoch [390/1000] train=0.1112 val=0.1094 l0=0.0000\n",
      "Epoch [400/1000] train=0.1107 val=0.1054 l0=0.0000\n",
      "Epoch [408/1000] train=0.1103 val=0.1030 l0=0.0000 (*)\n",
      "Epoch [410/1000] train=0.1106 val=0.1071 l0=0.0000\n",
      "Epoch [420/1000] train=0.1088 val=0.1093 l0=0.0000\n",
      "Epoch [430/1000] train=0.1104 val=0.1066 l0=0.0000\n",
      "Epoch [436/1000] train=0.1078 val=0.1018 l0=0.0000 (*)\n",
      "Epoch [440/1000] train=0.1081 val=0.1076 l0=0.0000\n",
      "Epoch [450/1000] train=0.1069 val=0.1072 l0=0.0000\n",
      "Epoch [460/1000] train=0.1060 val=0.1043 l0=0.0000\n",
      "Epoch [470/1000] train=0.1072 val=0.1077 l0=0.0000\n",
      "Epoch [480/1000] train=0.1069 val=0.1053 l0=0.0000\n",
      "Epoch [490/1000] train=0.1048 val=0.1030 l0=0.0000\n",
      "Epoch [500/1000] train=0.1040 val=0.1061 l0=0.0000\n",
      "Epoch [510/1000] train=0.1035 val=0.1037 l0=0.0000\n",
      "Epoch [520/1000] train=0.1031 val=0.1068 l0=0.0000\n",
      "Epoch [522/1000] train=0.1032 val=0.1016 l0=0.0000 (*)\n",
      "Epoch [530/1000] train=0.1034 val=0.1045 l0=0.0000\n",
      "Epoch [540/1000] train=0.1025 val=0.1048 l0=0.0000\n",
      "Epoch [550/1000] train=0.1028 val=0.1055 l0=0.0000\n",
      "Epoch [560/1000] train=0.1019 val=0.1042 l0=0.0000\n",
      "Epoch [570/1000] train=0.1006 val=0.1090 l0=0.0000\n",
      "Epoch [580/1000] train=0.1013 val=0.1056 l0=0.0000\n",
      "Epoch [590/1000] train=0.1008 val=0.1074 l0=0.0000\n",
      "Epoch [600/1000] train=0.1008 val=0.1040 l0=0.0000\n",
      "Epoch [610/1000] train=0.1000 val=0.1064 l0=0.0000\n",
      "Epoch [620/1000] train=0.1005 val=0.1058 l0=0.0000\n",
      "Epoch [630/1000] train=0.0985 val=0.1067 l0=0.0000\n",
      "Epoch [640/1000] train=0.0978 val=0.1060 l0=0.0000\n",
      "Epoch [650/1000] train=0.0978 val=0.1052 l0=0.0000\n",
      "Epoch [660/1000] train=0.0971 val=0.1074 l0=0.0000\n",
      "Epoch [670/1000] train=0.0984 val=0.1080 l0=0.0000\n",
      "Epoch [680/1000] train=0.0973 val=0.1076 l0=0.0000\n",
      "Epoch [690/1000] train=0.0973 val=0.1136 l0=0.0000\n",
      "Epoch [700/1000] train=0.0957 val=0.1081 l0=0.0000\n",
      "Epoch [710/1000] train=0.0964 val=0.1057 l0=0.0000\n",
      "Epoch [720/1000] train=0.0955 val=0.1057 l0=0.0000\n",
      "Early stopping at epoch 722 (best val=0.1016)\n",
      "Restored best model (val=0.1016)\n",
      "Epoch [1/1000] train=0.2738 val=0.2005 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.1902 val=0.1543 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1729 val=0.1451 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1669 val=0.1442 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1612 val=0.1431 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1637 val=0.1378 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1634 val=0.1395 l0=0.0000\n",
      "Epoch [12/1000] train=0.1571 val=0.1337 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.1538 val=0.1291 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1480 val=0.1240 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.1470 val=0.1206 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1434 val=0.1189 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1425 val=0.1159 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1372 val=0.1159 l0=0.0000\n",
      "Epoch [31/1000] train=0.1359 val=0.1156 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.1350 val=0.1140 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.1322 val=0.1135 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1299 val=0.1123 l0=0.0000 (*)\n",
      "Epoch [42/1000] train=0.1320 val=0.1091 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1286 val=0.1181 l0=0.0000\n",
      "Epoch [55/1000] train=0.1263 val=0.1082 l0=0.0000 (*)\n",
      "Epoch [59/1000] train=0.1255 val=0.1076 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1257 val=0.1195 l0=0.0000\n",
      "Epoch [64/1000] train=0.1259 val=0.1059 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1259 val=0.1070 l0=0.0000\n",
      "Epoch [75/1000] train=0.1228 val=0.1051 l0=0.0000 (*)\n",
      "Epoch [77/1000] train=0.1224 val=0.1049 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1221 val=0.1104 l0=0.0000\n",
      "Epoch [82/1000] train=0.1212 val=0.1033 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1201 val=0.1047 l0=0.0000\n",
      "Epoch [92/1000] train=0.1199 val=0.1029 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1197 val=0.1055 l0=0.0000\n",
      "Epoch [102/1000] train=0.1184 val=0.1025 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1171 val=0.1035 l0=0.0000\n",
      "Epoch [120/1000] train=0.1156 val=0.1061 l0=0.0000\n",
      "Epoch [130/1000] train=0.1149 val=0.1076 l0=0.0000\n",
      "Epoch [136/1000] train=0.1144 val=0.1021 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1133 val=0.1049 l0=0.0000\n",
      "Epoch [141/1000] train=0.1134 val=0.1018 l0=0.0000 (*)\n",
      "Epoch [149/1000] train=0.1119 val=0.1014 l0=0.0000 (*)\n",
      "Epoch [150/1000] train=0.1118 val=0.1002 l0=0.0000 (*)\n",
      "Epoch [151/1000] train=0.1120 val=0.1000 l0=0.0000 (*)\n",
      "Epoch [155/1000] train=0.1113 val=0.0990 l0=0.0000 (*)\n",
      "Epoch [157/1000] train=0.1116 val=0.0983 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.1108 val=0.1070 l0=0.0000\n",
      "Epoch [170/1000] train=0.1094 val=0.1036 l0=0.0000\n",
      "Epoch [180/1000] train=0.1082 val=0.1016 l0=0.0000\n",
      "Epoch [190/1000] train=0.1071 val=0.1009 l0=0.0000\n",
      "Epoch [200/1000] train=0.1063 val=0.1055 l0=0.0000\n",
      "Epoch [210/1000] train=0.1048 val=0.1042 l0=0.0000\n",
      "Epoch [220/1000] train=0.1036 val=0.1003 l0=0.0000\n",
      "Epoch [230/1000] train=0.1027 val=0.1022 l0=0.0000\n",
      "Epoch [240/1000] train=0.1022 val=0.1012 l0=0.0000\n",
      "Epoch [250/1000] train=0.0999 val=0.1050 l0=0.0000\n",
      "Epoch [260/1000] train=0.0994 val=0.1045 l0=0.0000\n",
      "Epoch [270/1000] train=0.0992 val=0.1041 l0=0.0000\n",
      "Epoch [280/1000] train=0.0972 val=0.1022 l0=0.0000\n",
      "Epoch [290/1000] train=0.0971 val=0.1066 l0=0.0000\n",
      "Epoch [300/1000] train=0.0966 val=0.1031 l0=0.0000\n",
      "Epoch [310/1000] train=0.0965 val=0.1007 l0=0.0000\n",
      "Epoch [320/1000] train=0.0951 val=0.1024 l0=0.0000\n",
      "Epoch [330/1000] train=0.0953 val=0.1041 l0=0.0000\n",
      "Epoch [340/1000] train=0.0937 val=0.1040 l0=0.0000\n",
      "Epoch [350/1000] train=0.0939 val=0.1061 l0=0.0000\n",
      "Early stopping at epoch 357 (best val=0.0983)\n",
      "Restored best model (val=0.0983)\n",
      "Epoch [1/1000] train=0.3239 val=0.2907 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.2137 val=0.2282 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1958 val=0.1703 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1813 val=0.1637 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1820 val=0.1570 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1772 val=0.1476 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1777 val=0.1622 l0=0.0000\n",
      "Epoch [12/1000] train=0.1712 val=0.1390 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.1652 val=0.1388 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1606 val=0.1344 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.1590 val=0.1328 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.1621 val=0.1302 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1578 val=0.1260 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.1499 val=0.1255 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.1512 val=0.1236 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.1544 val=0.1234 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1492 val=0.1272 l0=0.0000\n",
      "Epoch [45/1000] train=0.1452 val=0.1231 l0=0.0000 (*)\n",
      "Epoch [47/1000] train=0.1449 val=0.1223 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1448 val=0.1199 l0=0.0000 (*)\n",
      "Epoch [51/1000] train=0.1445 val=0.1195 l0=0.0000 (*)\n",
      "Epoch [52/1000] train=0.1467 val=0.1182 l0=0.0000 (*)\n",
      "Epoch [58/1000] train=0.1425 val=0.1174 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1406 val=0.1187 l0=0.0000\n",
      "Epoch [62/1000] train=0.1418 val=0.1167 l0=0.0000 (*)\n",
      "Epoch [63/1000] train=0.1468 val=0.1156 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1459 val=0.1340 l0=0.0000\n",
      "Epoch [73/1000] train=0.1403 val=0.1155 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1394 val=0.1157 l0=0.0000\n",
      "Epoch [82/1000] train=0.1414 val=0.1130 l0=0.0000 (*)\n",
      "Epoch [86/1000] train=0.1375 val=0.1129 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1383 val=0.1179 l0=0.0000\n",
      "Epoch [97/1000] train=0.1358 val=0.1125 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1346 val=0.1229 l0=0.0000\n",
      "Epoch [104/1000] train=0.1351 val=0.1117 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1380 val=0.1132 l0=0.0000\n",
      "Epoch [119/1000] train=0.1369 val=0.1107 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1325 val=0.1106 l0=0.0000 (*)\n",
      "Epoch [122/1000] train=0.1327 val=0.1102 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1349 val=0.1111 l0=0.0000\n",
      "Epoch [134/1000] train=0.1314 val=0.1100 l0=0.0000 (*)\n",
      "Epoch [137/1000] train=0.1344 val=0.1099 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1333 val=0.1150 l0=0.0000\n",
      "Epoch [145/1000] train=0.1295 val=0.1099 l0=0.0000 (*)\n",
      "Epoch [150/1000] train=0.1295 val=0.1090 l0=0.0000 (*)\n",
      "Epoch [158/1000] train=0.1272 val=0.1076 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.1284 val=0.1118 l0=0.0000\n",
      "Epoch [170/1000] train=0.1276 val=0.1080 l0=0.0000\n",
      "Epoch [177/1000] train=0.1280 val=0.1059 l0=0.0000 (*)\n",
      "Epoch [180/1000] train=0.1275 val=0.1362 l0=0.0000\n",
      "Epoch [186/1000] train=0.1274 val=0.1056 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.1236 val=0.1071 l0=0.0000\n",
      "Epoch [200/1000] train=0.1226 val=0.1155 l0=0.0000\n",
      "Epoch [202/1000] train=0.1219 val=0.1048 l0=0.0000 (*)\n",
      "Epoch [210/1000] train=0.1225 val=0.1143 l0=0.0000\n",
      "Epoch [217/1000] train=0.1212 val=0.1048 l0=0.0000 (*)\n",
      "Epoch [220/1000] train=0.1217 val=0.1078 l0=0.0000\n",
      "Epoch [221/1000] train=0.1220 val=0.1046 l0=0.0000 (*)\n",
      "Epoch [225/1000] train=0.1226 val=0.1042 l0=0.0000 (*)\n",
      "Epoch [227/1000] train=0.1212 val=0.1019 l0=0.0000 (*)\n",
      "Epoch [230/1000] train=0.1201 val=0.1076 l0=0.0000\n",
      "Epoch [240/1000] train=0.1198 val=0.1017 l0=0.0000 (*)\n",
      "Epoch [250/1000] train=0.1179 val=0.1081 l0=0.0000\n",
      "Epoch [260/1000] train=0.1181 val=0.1137 l0=0.0000\n",
      "Epoch [263/1000] train=0.1177 val=0.1009 l0=0.0000 (*)\n",
      "Epoch [270/1000] train=0.1186 val=0.1058 l0=0.0000\n",
      "Epoch [280/1000] train=0.1164 val=0.1065 l0=0.0000\n",
      "Epoch [290/1000] train=0.1174 val=0.1008 l0=0.0000 (*)\n",
      "Epoch [300/1000] train=0.1146 val=0.1032 l0=0.0000\n",
      "Epoch [310/1000] train=0.1134 val=0.1055 l0=0.0000\n",
      "Epoch [320/1000] train=0.1138 val=0.1056 l0=0.0000\n",
      "Epoch [327/1000] train=0.1131 val=0.1005 l0=0.0000 (*)\n",
      "Epoch [330/1000] train=0.1136 val=0.1034 l0=0.0000\n",
      "Epoch [340/1000] train=0.1117 val=0.1053 l0=0.0000\n",
      "Epoch [350/1000] train=0.1122 val=0.1054 l0=0.0000\n",
      "Epoch [360/1000] train=0.1125 val=0.1105 l0=0.0000\n",
      "Epoch [367/1000] train=0.1114 val=0.1004 l0=0.0000 (*)\n",
      "Epoch [370/1000] train=0.1109 val=0.1026 l0=0.0000\n",
      "Epoch [380/1000] train=0.1098 val=0.1028 l0=0.0000\n",
      "Epoch [390/1000] train=0.1099 val=0.1027 l0=0.0000\n",
      "Epoch [391/1000] train=0.1092 val=0.1001 l0=0.0000 (*)\n",
      "Epoch [400/1000] train=0.1090 val=0.1036 l0=0.0000\n",
      "Epoch [404/1000] train=0.1082 val=0.0998 l0=0.0000 (*)\n",
      "Epoch [410/1000] train=0.1086 val=0.1000 l0=0.0000\n",
      "Epoch [420/1000] train=0.1089 val=0.1057 l0=0.0000\n",
      "Epoch [430/1000] train=0.1075 val=0.1027 l0=0.0000\n",
      "Epoch [440/1000] train=0.1066 val=0.1004 l0=0.0000\n",
      "Epoch [446/1000] train=0.1059 val=0.0997 l0=0.0000 (*)\n",
      "Epoch [450/1000] train=0.1053 val=0.1012 l0=0.0000\n",
      "Epoch [460/1000] train=0.1055 val=0.1052 l0=0.0000\n",
      "Epoch [470/1000] train=0.1050 val=0.1008 l0=0.0000\n",
      "Epoch [480/1000] train=0.1042 val=0.1004 l0=0.0000\n",
      "Epoch [481/1000] train=0.1043 val=0.0992 l0=0.0000 (*)\n",
      "Epoch [490/1000] train=0.1029 val=0.1017 l0=0.0000\n",
      "Epoch [500/1000] train=0.1030 val=0.1076 l0=0.0000\n",
      "Epoch [510/1000] train=0.1022 val=0.0997 l0=0.0000\n",
      "Epoch [520/1000] train=0.1018 val=0.0996 l0=0.0000\n",
      "Epoch [524/1000] train=0.1007 val=0.0978 l0=0.0000 (*)\n",
      "Epoch [530/1000] train=0.1011 val=0.1003 l0=0.0000\n",
      "Epoch [540/1000] train=0.0999 val=0.1010 l0=0.0000\n",
      "Epoch [549/1000] train=0.1001 val=0.0977 l0=0.0000 (*)\n",
      "Epoch [550/1000] train=0.0998 val=0.0998 l0=0.0000\n",
      "Epoch [560/1000] train=0.0994 val=0.1006 l0=0.0000\n",
      "Epoch [570/1000] train=0.0978 val=0.0994 l0=0.0000\n",
      "Epoch [579/1000] train=0.0974 val=0.0975 l0=0.0000 (*)\n",
      "Epoch [580/1000] train=0.0974 val=0.0997 l0=0.0000\n",
      "Epoch [586/1000] train=0.0977 val=0.0967 l0=0.0000 (*)\n",
      "Epoch [590/1000] train=0.0972 val=0.0990 l0=0.0000\n",
      "Epoch [600/1000] train=0.0968 val=0.0991 l0=0.0000\n",
      "Epoch [610/1000] train=0.0966 val=0.0983 l0=0.0000\n",
      "Epoch [620/1000] train=0.0968 val=0.1047 l0=0.0000\n",
      "Epoch [621/1000] train=0.0955 val=0.0961 l0=0.0000 (*)\n",
      "Epoch [630/1000] train=0.0951 val=0.1046 l0=0.0000\n",
      "Epoch [633/1000] train=0.0959 val=0.0959 l0=0.0000 (*)\n",
      "Epoch [640/1000] train=0.0950 val=0.0970 l0=0.0000\n",
      "Epoch [650/1000] train=0.0957 val=0.1001 l0=0.0000\n",
      "Epoch [654/1000] train=0.0957 val=0.0959 l0=0.0000 (*)\n",
      "Epoch [660/1000] train=0.0956 val=0.0974 l0=0.0000\n",
      "Epoch [666/1000] train=0.0942 val=0.0953 l0=0.0000 (*)\n",
      "Epoch [670/1000] train=0.0935 val=0.0970 l0=0.0000\n",
      "Epoch [680/1000] train=0.0928 val=0.1032 l0=0.0000\n",
      "Epoch [690/1000] train=0.0930 val=0.1031 l0=0.0000\n",
      "Epoch [700/1000] train=0.0918 val=0.0991 l0=0.0000\n",
      "Epoch [710/1000] train=0.0922 val=0.0977 l0=0.0000\n",
      "Epoch [717/1000] train=0.0912 val=0.0950 l0=0.0000 (*)\n",
      "Epoch [720/1000] train=0.0919 val=0.0967 l0=0.0000\n",
      "Epoch [730/1000] train=0.0902 val=0.0993 l0=0.0000\n",
      "Epoch [740/1000] train=0.0890 val=0.0955 l0=0.0000\n",
      "Epoch [750/1000] train=0.0895 val=0.1003 l0=0.0000\n",
      "Epoch [760/1000] train=0.0891 val=0.0978 l0=0.0000\n",
      "Epoch [770/1000] train=0.0886 val=0.0967 l0=0.0000\n",
      "Epoch [780/1000] train=0.0879 val=0.1003 l0=0.0000\n",
      "Epoch [790/1000] train=0.0870 val=0.0986 l0=0.0000\n",
      "Epoch [800/1000] train=0.0882 val=0.1017 l0=0.0000\n",
      "Epoch [810/1000] train=0.0878 val=0.0974 l0=0.0000\n",
      "Epoch [820/1000] train=0.0868 val=0.0970 l0=0.0000\n",
      "Epoch [830/1000] train=0.0866 val=0.0981 l0=0.0000\n",
      "Epoch [840/1000] train=0.0859 val=0.0993 l0=0.0000\n",
      "Epoch [850/1000] train=0.0855 val=0.0994 l0=0.0000\n",
      "Epoch [860/1000] train=0.0861 val=0.1001 l0=0.0000\n",
      "Epoch [870/1000] train=0.0858 val=0.0960 l0=0.0000\n",
      "Epoch [880/1000] train=0.0846 val=0.1003 l0=0.0000\n",
      "Epoch [890/1000] train=0.0858 val=0.0973 l0=0.0000\n",
      "Epoch [900/1000] train=0.0848 val=0.0995 l0=0.0000\n",
      "Epoch [910/1000] train=0.0845 val=0.0984 l0=0.0000\n",
      "Early stopping at epoch 917 (best val=0.0950)\n",
      "Restored best model (val=0.0950)\n",
      "Epoch [1/1000] train=0.2709 val=0.1773 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.1882 val=0.1705 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1796 val=0.1573 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1709 val=0.1543 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1682 val=0.1485 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1641 val=0.1480 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1621 val=0.1481 l0=0.0000\n",
      "Epoch [12/1000] train=0.1576 val=0.1450 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.1563 val=0.1427 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.1530 val=0.1399 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.1492 val=0.1361 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1463 val=0.1343 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.1462 val=0.1341 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.1418 val=0.1325 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.1401 val=0.1306 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1396 val=0.1289 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1350 val=0.1317 l0=0.0000\n",
      "Epoch [31/1000] train=0.1365 val=0.1262 l0=0.0000 (*)\n",
      "Epoch [32/1000] train=0.1345 val=0.1250 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1307 val=0.1269 l0=0.0000\n",
      "Epoch [43/1000] train=0.1304 val=0.1236 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.1283 val=0.1229 l0=0.0000 (*)\n",
      "Epoch [46/1000] train=0.1287 val=0.1216 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1256 val=0.1250 l0=0.0000\n",
      "Epoch [53/1000] train=0.1257 val=0.1206 l0=0.0000 (*)\n",
      "Epoch [56/1000] train=0.1258 val=0.1187 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1235 val=0.1229 l0=0.0000\n",
      "Epoch [70/1000] train=0.1223 val=0.1230 l0=0.0000\n",
      "Epoch [79/1000] train=0.1213 val=0.1173 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1217 val=0.1170 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1183 val=0.1207 l0=0.0000\n",
      "Epoch [91/1000] train=0.1183 val=0.1161 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1178 val=0.1164 l0=0.0000\n",
      "Epoch [104/1000] train=0.1164 val=0.1157 l0=0.0000 (*)\n",
      "Epoch [105/1000] train=0.1161 val=0.1148 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1168 val=0.1166 l0=0.0000\n",
      "Epoch [112/1000] train=0.1153 val=0.1126 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1146 val=0.1160 l0=0.0000\n",
      "Epoch [130/1000] train=0.1127 val=0.1167 l0=0.0000\n",
      "Epoch [134/1000] train=0.1122 val=0.1126 l0=0.0000 (*)\n",
      "Epoch [137/1000] train=0.1124 val=0.1123 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1117 val=0.1163 l0=0.0000\n",
      "Epoch [147/1000] train=0.1116 val=0.1118 l0=0.0000 (*)\n",
      "Epoch [150/1000] train=0.1113 val=0.1117 l0=0.0000 (*)\n",
      "Epoch [156/1000] train=0.1111 val=0.1117 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.1097 val=0.1126 l0=0.0000\n",
      "Epoch [170/1000] train=0.1091 val=0.1139 l0=0.0000\n",
      "Epoch [171/1000] train=0.1090 val=0.1117 l0=0.0000 (*)\n",
      "Epoch [172/1000] train=0.1084 val=0.1103 l0=0.0000 (*)\n",
      "Epoch [180/1000] train=0.1071 val=0.1168 l0=0.0000\n",
      "Epoch [183/1000] train=0.1077 val=0.1097 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.1068 val=0.1135 l0=0.0000\n",
      "Epoch [200/1000] train=0.1056 val=0.1194 l0=0.0000\n",
      "Epoch [210/1000] train=0.1044 val=0.1117 l0=0.0000\n",
      "Epoch [220/1000] train=0.1044 val=0.1109 l0=0.0000\n",
      "Epoch [230/1000] train=0.1030 val=0.1099 l0=0.0000\n",
      "Epoch [231/1000] train=0.1029 val=0.1096 l0=0.0000 (*)\n",
      "Epoch [232/1000] train=0.1035 val=0.1092 l0=0.0000 (*)\n",
      "Epoch [237/1000] train=0.1029 val=0.1089 l0=0.0000 (*)\n",
      "Epoch [240/1000] train=0.1024 val=0.1136 l0=0.0000\n",
      "Epoch [250/1000] train=0.1020 val=0.1153 l0=0.0000\n",
      "Epoch [260/1000] train=0.1004 val=0.1126 l0=0.0000\n",
      "Epoch [270/1000] train=0.1004 val=0.1122 l0=0.0000\n",
      "Epoch [280/1000] train=0.0995 val=0.1140 l0=0.0000\n",
      "Epoch [290/1000] train=0.0991 val=0.1107 l0=0.0000\n",
      "Epoch [300/1000] train=0.0984 val=0.1128 l0=0.0000\n",
      "Epoch [310/1000] train=0.0969 val=0.1130 l0=0.0000\n",
      "Epoch [320/1000] train=0.0969 val=0.1175 l0=0.0000\n",
      "Epoch [330/1000] train=0.0957 val=0.1144 l0=0.0000\n",
      "Epoch [340/1000] train=0.0948 val=0.1142 l0=0.0000\n",
      "Epoch [350/1000] train=0.0949 val=0.1141 l0=0.0000\n",
      "Epoch [360/1000] train=0.0944 val=0.1120 l0=0.0000\n",
      "Epoch [370/1000] train=0.0928 val=0.1146 l0=0.0000\n",
      "Epoch [380/1000] train=0.0924 val=0.1159 l0=0.0000\n",
      "Epoch [390/1000] train=0.0915 val=0.1148 l0=0.0000\n",
      "Epoch [400/1000] train=0.0909 val=0.1124 l0=0.0000\n",
      "Epoch [410/1000] train=0.0908 val=0.1203 l0=0.0000\n",
      "Epoch [420/1000] train=0.0899 val=0.1162 l0=0.0000\n",
      "Epoch [430/1000] train=0.0889 val=0.1153 l0=0.0000\n",
      "Early stopping at epoch 437 (best val=0.1089)\n",
      "Restored best model (val=0.1089)\n",
      "Epoch [1/1000] train=0.3399 val=0.2067 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.2139 val=0.1815 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1932 val=0.1778 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.1939 val=0.1758 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1868 val=0.1658 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1784 val=0.1606 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1748 val=0.1577 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1759 val=0.1587 l0=0.0000\n",
      "Epoch [13/1000] train=0.1720 val=0.1503 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.1615 val=0.1463 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1603 val=0.1514 l0=0.0000\n",
      "Epoch [21/1000] train=0.1640 val=0.1424 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1590 val=0.1400 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1596 val=0.1397 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.1569 val=0.1377 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1579 val=0.1374 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.1502 val=0.1370 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.1493 val=0.1364 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1504 val=0.1387 l0=0.0000\n",
      "Epoch [41/1000] train=0.1494 val=0.1322 l0=0.0000 (*)\n",
      "Epoch [46/1000] train=0.1433 val=0.1319 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.1442 val=0.1316 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1469 val=0.1345 l0=0.0000\n",
      "Epoch [52/1000] train=0.1470 val=0.1297 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1430 val=0.1336 l0=0.0000\n",
      "Epoch [63/1000] train=0.1441 val=0.1285 l0=0.0000 (*)\n",
      "Epoch [68/1000] train=0.1403 val=0.1283 l0=0.0000 (*)\n",
      "Epoch [69/1000] train=0.1437 val=0.1269 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1436 val=0.1820 l0=0.0000\n",
      "Epoch [80/1000] train=0.1448 val=0.1627 l0=0.0000\n",
      "Epoch [90/1000] train=0.1376 val=0.1316 l0=0.0000\n",
      "Epoch [91/1000] train=0.1365 val=0.1262 l0=0.0000 (*)\n",
      "Epoch [93/1000] train=0.1425 val=0.1253 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1385 val=0.1341 l0=0.0000\n",
      "Epoch [110/1000] train=0.1364 val=0.1273 l0=0.0000\n",
      "Epoch [111/1000] train=0.1309 val=0.1242 l0=0.0000 (*)\n",
      "Epoch [115/1000] train=0.1357 val=0.1215 l0=0.0000 (*)\n",
      "Epoch [119/1000] train=0.1341 val=0.1212 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1334 val=0.1288 l0=0.0000\n",
      "Epoch [123/1000] train=0.1306 val=0.1211 l0=0.0000 (*)\n",
      "Epoch [125/1000] train=0.1307 val=0.1206 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1300 val=0.1209 l0=0.0000\n",
      "Epoch [136/1000] train=0.1281 val=0.1177 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1276 val=0.1182 l0=0.0000\n",
      "Epoch [150/1000] train=0.1267 val=0.1378 l0=0.0000\n",
      "Epoch [160/1000] train=0.1294 val=0.1247 l0=0.0000\n",
      "Epoch [164/1000] train=0.1281 val=0.1174 l0=0.0000 (*)\n",
      "Epoch [170/1000] train=0.1264 val=0.1192 l0=0.0000\n",
      "Epoch [176/1000] train=0.1238 val=0.1165 l0=0.0000 (*)\n",
      "Epoch [180/1000] train=0.1252 val=0.1167 l0=0.0000\n",
      "Epoch [184/1000] train=0.1247 val=0.1163 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.1251 val=0.1151 l0=0.0000 (*)\n",
      "Epoch [196/1000] train=0.1220 val=0.1143 l0=0.0000 (*)\n",
      "Epoch [200/1000] train=0.1232 val=0.1182 l0=0.0000\n",
      "Epoch [210/1000] train=0.1232 val=0.1205 l0=0.0000\n",
      "Epoch [220/1000] train=0.1213 val=0.1207 l0=0.0000\n",
      "Epoch [230/1000] train=0.1204 val=0.1137 l0=0.0000 (*)\n",
      "Epoch [240/1000] train=0.1201 val=0.1261 l0=0.0000\n",
      "Epoch [250/1000] train=0.1196 val=0.1153 l0=0.0000\n",
      "Epoch [255/1000] train=0.1193 val=0.1132 l0=0.0000 (*)\n",
      "Epoch [258/1000] train=0.1194 val=0.1124 l0=0.0000 (*)\n",
      "Epoch [260/1000] train=0.1209 val=0.1174 l0=0.0000\n",
      "Epoch [270/1000] train=0.1188 val=0.1144 l0=0.0000\n",
      "Epoch [280/1000] train=0.1182 val=0.1156 l0=0.0000\n",
      "Epoch [290/1000] train=0.1191 val=0.1272 l0=0.0000\n",
      "Epoch [300/1000] train=0.1165 val=0.1141 l0=0.0000\n",
      "Epoch [302/1000] train=0.1170 val=0.1122 l0=0.0000 (*)\n",
      "Epoch [308/1000] train=0.1160 val=0.1114 l0=0.0000 (*)\n",
      "Epoch [310/1000] train=0.1167 val=0.1152 l0=0.0000\n",
      "Epoch [320/1000] train=0.1168 val=0.1150 l0=0.0000\n",
      "Epoch [330/1000] train=0.1159 val=0.1130 l0=0.0000\n",
      "Epoch [340/1000] train=0.1156 val=0.1103 l0=0.0000 (*)\n",
      "Epoch [350/1000] train=0.1144 val=0.1099 l0=0.0000 (*)\n",
      "Epoch [360/1000] train=0.1145 val=0.1121 l0=0.0000\n",
      "Epoch [370/1000] train=0.1149 val=0.1100 l0=0.0000\n",
      "Epoch [377/1000] train=0.1134 val=0.1099 l0=0.0000 (*)\n",
      "Epoch [380/1000] train=0.1126 val=0.1118 l0=0.0000\n",
      "Epoch [381/1000] train=0.1134 val=0.1088 l0=0.0000 (*)\n",
      "Epoch [390/1000] train=0.1129 val=0.1173 l0=0.0000\n",
      "Epoch [400/1000] train=0.1128 val=0.1135 l0=0.0000\n",
      "Epoch [410/1000] train=0.1111 val=0.1182 l0=0.0000\n",
      "Epoch [420/1000] train=0.1113 val=0.1165 l0=0.0000\n",
      "Epoch [430/1000] train=0.1117 val=0.1156 l0=0.0000\n",
      "Epoch [440/1000] train=0.1099 val=0.1107 l0=0.0000\n",
      "Epoch [450/1000] train=0.1095 val=0.1154 l0=0.0000\n",
      "Epoch [460/1000] train=0.1085 val=0.1122 l0=0.0000\n",
      "Epoch [470/1000] train=0.1090 val=0.1124 l0=0.0000\n",
      "Epoch [480/1000] train=0.1078 val=0.1121 l0=0.0000\n",
      "Epoch [490/1000] train=0.1079 val=0.1152 l0=0.0000\n",
      "Epoch [500/1000] train=0.1086 val=0.1115 l0=0.0000\n",
      "Epoch [510/1000] train=0.1073 val=0.1167 l0=0.0000\n",
      "Epoch [520/1000] train=0.1059 val=0.1096 l0=0.0000\n",
      "Epoch [530/1000] train=0.1069 val=0.1105 l0=0.0000\n",
      "Epoch [540/1000] train=0.1061 val=0.1101 l0=0.0000\n",
      "Epoch [550/1000] train=0.1049 val=0.1113 l0=0.0000\n",
      "Epoch [560/1000] train=0.1050 val=0.1119 l0=0.0000\n",
      "Epoch [570/1000] train=0.1040 val=0.1113 l0=0.0000\n",
      "Epoch [574/1000] train=0.1042 val=0.1086 l0=0.0000 (*)\n",
      "Epoch [580/1000] train=0.1041 val=0.1119 l0=0.0000\n",
      "Epoch [589/1000] train=0.1032 val=0.1085 l0=0.0000 (*)\n",
      "Epoch [590/1000] train=0.1040 val=0.1117 l0=0.0000\n",
      "Epoch [600/1000] train=0.1030 val=0.1157 l0=0.0000\n",
      "Epoch [610/1000] train=0.1028 val=0.1085 l0=0.0000 (*)\n",
      "Epoch [620/1000] train=0.1025 val=0.1101 l0=0.0000\n",
      "Epoch [630/1000] train=0.1018 val=0.1138 l0=0.0000\n",
      "Epoch [640/1000] train=0.1024 val=0.1128 l0=0.0000\n",
      "Epoch [650/1000] train=0.1016 val=0.1110 l0=0.0000\n",
      "Epoch [660/1000] train=0.1010 val=0.1097 l0=0.0000\n",
      "Epoch [666/1000] train=0.1017 val=0.1083 l0=0.0000 (*)\n",
      "Epoch [670/1000] train=0.1008 val=0.1165 l0=0.0000\n",
      "Epoch [680/1000] train=0.0997 val=0.1104 l0=0.0000\n",
      "Epoch [690/1000] train=0.1003 val=0.1112 l0=0.0000\n",
      "Epoch [700/1000] train=0.0998 val=0.1117 l0=0.0000\n",
      "Epoch [710/1000] train=0.0989 val=0.1108 l0=0.0000\n",
      "Epoch [720/1000] train=0.0976 val=0.1140 l0=0.0000\n",
      "Epoch [730/1000] train=0.0981 val=0.1126 l0=0.0000\n",
      "Epoch [740/1000] train=0.0976 val=0.1124 l0=0.0000\n",
      "Epoch [750/1000] train=0.0981 val=0.1107 l0=0.0000\n",
      "Epoch [760/1000] train=0.0962 val=0.1108 l0=0.0000\n",
      "Epoch [770/1000] train=0.0963 val=0.1111 l0=0.0000\n",
      "Epoch [780/1000] train=0.0956 val=0.1193 l0=0.0000\n",
      "Epoch [790/1000] train=0.0952 val=0.1124 l0=0.0000\n",
      "Epoch [800/1000] train=0.0948 val=0.1149 l0=0.0000\n",
      "Epoch [810/1000] train=0.0948 val=0.1149 l0=0.0000\n",
      "Epoch [820/1000] train=0.0939 val=0.1125 l0=0.0000\n",
      "Epoch [830/1000] train=0.0938 val=0.1121 l0=0.0000\n",
      "Epoch [840/1000] train=0.0942 val=0.1112 l0=0.0000\n",
      "Epoch [850/1000] train=0.0950 val=0.1125 l0=0.0000\n",
      "Epoch [860/1000] train=0.0920 val=0.1134 l0=0.0000\n",
      "Early stopping at epoch 866 (best val=0.1083)\n",
      "Restored best model (val=0.1083)\n",
      "Epoch [1/1000] train=0.2771 val=0.1812 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.1843 val=0.1682 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.1730 val=0.1490 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1751 val=0.1480 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1673 val=0.1457 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1648 val=0.1424 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1640 val=0.1452 l0=0.0000\n",
      "Epoch [14/1000] train=0.1586 val=0.1421 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.1571 val=0.1395 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.1563 val=0.1348 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1501 val=0.1529 l0=0.0000\n",
      "Epoch [21/1000] train=0.1491 val=0.1332 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.1458 val=0.1277 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1394 val=0.1265 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.1385 val=0.1252 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1398 val=0.1266 l0=0.0000\n",
      "Epoch [36/1000] train=0.1324 val=0.1219 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1316 val=0.1287 l0=0.0000\n",
      "Epoch [48/1000] train=0.1284 val=0.1217 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.1286 val=0.1211 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1276 val=0.1230 l0=0.0000\n",
      "Epoch [52/1000] train=0.1277 val=0.1197 l0=0.0000 (*)\n",
      "Epoch [55/1000] train=0.1270 val=0.1189 l0=0.0000 (*)\n",
      "Epoch [57/1000] train=0.1270 val=0.1181 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1246 val=0.1219 l0=0.0000\n",
      "Epoch [61/1000] train=0.1252 val=0.1180 l0=0.0000 (*)\n",
      "Epoch [64/1000] train=0.1251 val=0.1179 l0=0.0000 (*)\n",
      "Epoch [66/1000] train=0.1231 val=0.1170 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1242 val=0.1202 l0=0.0000\n",
      "Epoch [72/1000] train=0.1242 val=0.1167 l0=0.0000 (*)\n",
      "Epoch [73/1000] train=0.1237 val=0.1145 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1223 val=0.1199 l0=0.0000\n",
      "Epoch [90/1000] train=0.1200 val=0.1153 l0=0.0000\n",
      "Epoch [91/1000] train=0.1200 val=0.1144 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1190 val=0.1213 l0=0.0000\n",
      "Epoch [105/1000] train=0.1184 val=0.1139 l0=0.0000 (*)\n",
      "Epoch [107/1000] train=0.1176 val=0.1136 l0=0.0000 (*)\n",
      "Epoch [108/1000] train=0.1171 val=0.1133 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1180 val=0.1155 l0=0.0000\n",
      "Epoch [111/1000] train=0.1179 val=0.1127 l0=0.0000 (*)\n",
      "Epoch [115/1000] train=0.1181 val=0.1122 l0=0.0000 (*)\n",
      "Epoch [116/1000] train=0.1167 val=0.1118 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1165 val=0.1182 l0=0.0000\n",
      "Epoch [130/1000] train=0.1149 val=0.1149 l0=0.0000\n",
      "Epoch [140/1000] train=0.1124 val=0.1155 l0=0.0000\n",
      "Epoch [150/1000] train=0.1119 val=0.1140 l0=0.0000\n",
      "Epoch [160/1000] train=0.1113 val=0.1144 l0=0.0000\n",
      "Epoch [170/1000] train=0.1110 val=0.1128 l0=0.0000\n",
      "Epoch [180/1000] train=0.1091 val=0.1131 l0=0.0000\n",
      "Epoch [189/1000] train=0.1080 val=0.1101 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.1068 val=0.1149 l0=0.0000\n",
      "Epoch [200/1000] train=0.1059 val=0.1137 l0=0.0000\n",
      "Epoch [210/1000] train=0.1054 val=0.1133 l0=0.0000\n",
      "Epoch [220/1000] train=0.1039 val=0.1176 l0=0.0000\n",
      "Epoch [230/1000] train=0.1025 val=0.1138 l0=0.0000\n",
      "Epoch [240/1000] train=0.1024 val=0.1146 l0=0.0000\n",
      "Epoch [250/1000] train=0.1016 val=0.1189 l0=0.0000\n",
      "Epoch [260/1000] train=0.1011 val=0.1180 l0=0.0000\n",
      "Epoch [270/1000] train=0.0993 val=0.1140 l0=0.0000\n",
      "Epoch [280/1000] train=0.0984 val=0.1164 l0=0.0000\n",
      "Epoch [290/1000] train=0.0971 val=0.1173 l0=0.0000\n",
      "Epoch [300/1000] train=0.0955 val=0.1163 l0=0.0000\n",
      "Epoch [310/1000] train=0.0954 val=0.1183 l0=0.0000\n",
      "Epoch [320/1000] train=0.0951 val=0.1128 l0=0.0000\n",
      "Epoch [330/1000] train=0.0940 val=0.1138 l0=0.0000\n",
      "Epoch [340/1000] train=0.0935 val=0.1130 l0=0.0000\n",
      "Epoch [350/1000] train=0.0922 val=0.1146 l0=0.0000\n",
      "Epoch [360/1000] train=0.0918 val=0.1146 l0=0.0000\n",
      "Epoch [370/1000] train=0.0907 val=0.1133 l0=0.0000\n",
      "Epoch [380/1000] train=0.0910 val=0.1160 l0=0.0000\n",
      "Early stopping at epoch 389 (best val=0.1101)\n",
      "Restored best model (val=0.1101)\n",
      "Epoch [1/1000] train=0.3286 val=0.1977 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1959 val=0.1792 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.1966 val=0.1663 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1882 val=0.1600 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1819 val=0.1565 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1801 val=0.1542 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1772 val=0.1561 l0=0.0000\n",
      "Epoch [12/1000] train=0.1759 val=0.1527 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.1771 val=0.1490 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.1632 val=0.1460 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1663 val=0.1534 l0=0.0000\n",
      "Epoch [21/1000] train=0.1606 val=0.1431 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.1595 val=0.1421 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1636 val=0.1370 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1552 val=0.1544 l0=0.0000\n",
      "Epoch [40/1000] train=0.1518 val=0.1427 l0=0.0000\n",
      "Epoch [42/1000] train=0.1502 val=0.1356 l0=0.0000 (*)\n",
      "Epoch [48/1000] train=0.1528 val=0.1334 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.1524 val=0.1324 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1467 val=0.1334 l0=0.0000\n",
      "Epoch [52/1000] train=0.1532 val=0.1323 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1489 val=0.1295 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1441 val=0.1332 l0=0.0000\n",
      "Epoch [79/1000] train=0.1403 val=0.1290 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1393 val=0.1369 l0=0.0000\n",
      "Epoch [82/1000] train=0.1423 val=0.1285 l0=0.0000 (*)\n",
      "Epoch [87/1000] train=0.1425 val=0.1272 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1411 val=0.1668 l0=0.0000\n",
      "Epoch [100/1000] train=0.1377 val=0.1386 l0=0.0000\n",
      "Epoch [101/1000] train=0.1354 val=0.1265 l0=0.0000 (*)\n",
      "Epoch [104/1000] train=0.1345 val=0.1250 l0=0.0000 (*)\n",
      "Epoch [105/1000] train=0.1359 val=0.1219 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1314 val=0.1464 l0=0.0000\n",
      "Epoch [116/1000] train=0.1324 val=0.1215 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1332 val=0.1375 l0=0.0000\n",
      "Epoch [121/1000] train=0.1336 val=0.1212 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1310 val=0.1279 l0=0.0000\n",
      "Epoch [134/1000] train=0.1328 val=0.1209 l0=0.0000 (*)\n",
      "Epoch [136/1000] train=0.1283 val=0.1206 l0=0.0000 (*)\n",
      "Epoch [137/1000] train=0.1311 val=0.1186 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1328 val=0.1207 l0=0.0000\n",
      "Epoch [146/1000] train=0.1270 val=0.1179 l0=0.0000 (*)\n",
      "Epoch [150/1000] train=0.1303 val=0.1246 l0=0.0000\n",
      "Epoch [160/1000] train=0.1283 val=0.1177 l0=0.0000 (*)\n",
      "Epoch [170/1000] train=0.1274 val=0.1175 l0=0.0000 (*)\n",
      "Epoch [180/1000] train=0.1257 val=0.1271 l0=0.0000\n",
      "Epoch [190/1000] train=0.1258 val=0.1198 l0=0.0000\n",
      "Epoch [193/1000] train=0.1243 val=0.1162 l0=0.0000 (*)\n",
      "Epoch [200/1000] train=0.1243 val=0.1169 l0=0.0000\n",
      "Epoch [210/1000] train=0.1232 val=0.1220 l0=0.0000\n",
      "Epoch [212/1000] train=0.1243 val=0.1150 l0=0.0000 (*)\n",
      "Epoch [218/1000] train=0.1223 val=0.1145 l0=0.0000 (*)\n",
      "Epoch [220/1000] train=0.1237 val=0.1254 l0=0.0000\n",
      "Epoch [230/1000] train=0.1230 val=0.1178 l0=0.0000\n",
      "Epoch [235/1000] train=0.1224 val=0.1144 l0=0.0000 (*)\n",
      "Epoch [240/1000] train=0.1215 val=0.1135 l0=0.0000 (*)\n",
      "Epoch [250/1000] train=0.1198 val=0.1136 l0=0.0000\n",
      "Epoch [260/1000] train=0.1201 val=0.1172 l0=0.0000\n",
      "Epoch [262/1000] train=0.1197 val=0.1126 l0=0.0000 (*)\n",
      "Epoch [270/1000] train=0.1180 val=0.1501 l0=0.0000\n",
      "Epoch [280/1000] train=0.1202 val=0.1154 l0=0.0000\n",
      "Epoch [290/1000] train=0.1187 val=0.1135 l0=0.0000\n",
      "Epoch [299/1000] train=0.1175 val=0.1126 l0=0.0000 (*)\n",
      "Epoch [300/1000] train=0.1179 val=0.1166 l0=0.0000\n",
      "Epoch [305/1000] train=0.1167 val=0.1118 l0=0.0000 (*)\n",
      "Epoch [310/1000] train=0.1154 val=0.1151 l0=0.0000\n",
      "Epoch [320/1000] train=0.1183 val=0.1206 l0=0.0000\n",
      "Epoch [330/1000] train=0.1152 val=0.1179 l0=0.0000\n",
      "Epoch [340/1000] train=0.1152 val=0.1143 l0=0.0000\n",
      "Epoch [350/1000] train=0.1160 val=0.1144 l0=0.0000\n",
      "Epoch [360/1000] train=0.1135 val=0.1253 l0=0.0000\n",
      "Epoch [370/1000] train=0.1136 val=0.1133 l0=0.0000\n",
      "Epoch [380/1000] train=0.1132 val=0.1200 l0=0.0000\n",
      "Epoch [390/1000] train=0.1130 val=0.1173 l0=0.0000\n",
      "Epoch [400/1000] train=0.1107 val=0.1137 l0=0.0000\n",
      "Epoch [410/1000] train=0.1106 val=0.1147 l0=0.0000\n",
      "Epoch [414/1000] train=0.1108 val=0.1112 l0=0.0000 (*)\n",
      "Epoch [420/1000] train=0.1086 val=0.1153 l0=0.0000\n",
      "Epoch [430/1000] train=0.1087 val=0.1120 l0=0.0000\n",
      "Epoch [440/1000] train=0.1072 val=0.1171 l0=0.0000\n",
      "Epoch [450/1000] train=0.1071 val=0.1162 l0=0.0000\n",
      "Epoch [451/1000] train=0.1078 val=0.1107 l0=0.0000 (*)\n",
      "Epoch [453/1000] train=0.1072 val=0.1090 l0=0.0000 (*)\n",
      "Epoch [460/1000] train=0.1066 val=0.1104 l0=0.0000\n",
      "Epoch [470/1000] train=0.1065 val=0.1100 l0=0.0000\n",
      "Epoch [480/1000] train=0.1056 val=0.1131 l0=0.0000\n",
      "Epoch [490/1000] train=0.1038 val=0.1134 l0=0.0000\n",
      "Epoch [500/1000] train=0.1051 val=0.1163 l0=0.0000\n",
      "Epoch [510/1000] train=0.1038 val=0.1152 l0=0.0000\n",
      "Epoch [520/1000] train=0.1040 val=0.1162 l0=0.0000\n",
      "Epoch [530/1000] train=0.1019 val=0.1145 l0=0.0000\n",
      "Epoch [540/1000] train=0.1022 val=0.1107 l0=0.0000\n",
      "Epoch [550/1000] train=0.1009 val=0.1126 l0=0.0000\n",
      "Epoch [560/1000] train=0.0998 val=0.1104 l0=0.0000\n",
      "Epoch [570/1000] train=0.1001 val=0.1128 l0=0.0000\n",
      "Epoch [576/1000] train=0.1001 val=0.1087 l0=0.0000 (*)\n",
      "Epoch [580/1000] train=0.1008 val=0.1090 l0=0.0000\n",
      "Epoch [590/1000] train=0.0997 val=0.1111 l0=0.0000\n",
      "Epoch [600/1000] train=0.0996 val=0.1127 l0=0.0000\n",
      "Epoch [610/1000] train=0.0985 val=0.1176 l0=0.0000\n",
      "Epoch [620/1000] train=0.0976 val=0.1138 l0=0.0000\n",
      "Epoch [630/1000] train=0.0976 val=0.1095 l0=0.0000\n",
      "Epoch [640/1000] train=0.0971 val=0.1107 l0=0.0000\n",
      "Epoch [650/1000] train=0.0974 val=0.1121 l0=0.0000\n",
      "Epoch [660/1000] train=0.0959 val=0.1147 l0=0.0000\n",
      "Epoch [670/1000] train=0.0951 val=0.1169 l0=0.0000\n",
      "Epoch [680/1000] train=0.0940 val=0.1120 l0=0.0000\n",
      "Epoch [690/1000] train=0.0935 val=0.1110 l0=0.0000\n",
      "Epoch [700/1000] train=0.0937 val=0.1132 l0=0.0000\n",
      "Epoch [710/1000] train=0.0932 val=0.1136 l0=0.0000\n",
      "Epoch [720/1000] train=0.0932 val=0.1115 l0=0.0000\n",
      "Epoch [730/1000] train=0.0921 val=0.1123 l0=0.0000\n",
      "Epoch [740/1000] train=0.0915 val=0.1206 l0=0.0000\n",
      "Epoch [750/1000] train=0.0913 val=0.1129 l0=0.0000\n",
      "Epoch [760/1000] train=0.0918 val=0.1149 l0=0.0000\n",
      "Epoch [770/1000] train=0.0899 val=0.1146 l0=0.0000\n",
      "Early stopping at epoch 776 (best val=0.1087)\n",
      "Restored best model (val=0.1087)\n",
      "Epoch [1/1000] train=0.2620 val=0.1751 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.1789 val=0.1719 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1681 val=0.1718 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.1662 val=0.1635 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.1695 val=0.1607 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1534 val=0.1516 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.1539 val=0.1516 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.1526 val=0.1489 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.1489 val=0.1432 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.1461 val=0.1426 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1413 val=0.1540 l0=0.0000\n",
      "Epoch [23/1000] train=0.1395 val=0.1382 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.1317 val=0.1340 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1310 val=0.1445 l0=0.0000\n",
      "Epoch [32/1000] train=0.1309 val=0.1325 l0=0.0000 (*)\n",
      "Epoch [35/1000] train=0.1283 val=0.1318 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1265 val=0.1376 l0=0.0000\n",
      "Epoch [43/1000] train=0.1254 val=0.1296 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.1259 val=0.1295 l0=0.0000 (*)\n",
      "Epoch [46/1000] train=0.1253 val=0.1288 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.1240 val=0.1273 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1246 val=0.1269 l0=0.0000 (*)\n",
      "Epoch [58/1000] train=0.1223 val=0.1266 l0=0.0000 (*)\n",
      "Epoch [59/1000] train=0.1216 val=0.1245 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1229 val=0.1318 l0=0.0000\n",
      "Epoch [70/1000] train=0.1193 val=0.1334 l0=0.0000\n",
      "Epoch [78/1000] train=0.1190 val=0.1232 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1182 val=0.1254 l0=0.0000\n",
      "Epoch [90/1000] train=0.1171 val=0.1270 l0=0.0000\n",
      "Epoch [100/1000] train=0.1149 val=0.1235 l0=0.0000\n",
      "Epoch [104/1000] train=0.1142 val=0.1213 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1132 val=0.1232 l0=0.0000\n",
      "Epoch [120/1000] train=0.1129 val=0.1221 l0=0.0000\n",
      "Epoch [127/1000] train=0.1118 val=0.1202 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1110 val=0.1255 l0=0.0000\n",
      "Epoch [140/1000] train=0.1101 val=0.1219 l0=0.0000\n",
      "Epoch [150/1000] train=0.1080 val=0.1230 l0=0.0000\n",
      "Epoch [158/1000] train=0.1078 val=0.1193 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.1081 val=0.1198 l0=0.0000\n",
      "Epoch [170/1000] train=0.1072 val=0.1215 l0=0.0000\n",
      "Epoch [180/1000] train=0.1061 val=0.1207 l0=0.0000\n",
      "Epoch [190/1000] train=0.1046 val=0.1202 l0=0.0000\n",
      "Epoch [200/1000] train=0.1039 val=0.1212 l0=0.0000\n",
      "Epoch [203/1000] train=0.1039 val=0.1192 l0=0.0000 (*)\n",
      "Epoch [210/1000] train=0.1036 val=0.1227 l0=0.0000\n",
      "Epoch [220/1000] train=0.1020 val=0.1193 l0=0.0000\n",
      "Epoch [230/1000] train=0.1017 val=0.1214 l0=0.0000\n",
      "Epoch [233/1000] train=0.1009 val=0.1182 l0=0.0000 (*)\n",
      "Epoch [239/1000] train=0.1011 val=0.1177 l0=0.0000 (*)\n",
      "Epoch [240/1000] train=0.1002 val=0.1200 l0=0.0000\n",
      "Epoch [250/1000] train=0.1001 val=0.1218 l0=0.0000\n",
      "Epoch [260/1000] train=0.0995 val=0.1204 l0=0.0000\n",
      "Epoch [270/1000] train=0.0982 val=0.1183 l0=0.0000\n",
      "Epoch [278/1000] train=0.0982 val=0.1173 l0=0.0000 (*)\n",
      "Epoch [280/1000] train=0.0977 val=0.1246 l0=0.0000\n",
      "Epoch [290/1000] train=0.0973 val=0.1177 l0=0.0000\n",
      "Epoch [300/1000] train=0.0967 val=0.1190 l0=0.0000\n",
      "Epoch [310/1000] train=0.0960 val=0.1189 l0=0.0000\n",
      "Epoch [320/1000] train=0.0957 val=0.1190 l0=0.0000\n",
      "Epoch [330/1000] train=0.0945 val=0.1194 l0=0.0000\n",
      "Epoch [340/1000] train=0.0939 val=0.1193 l0=0.0000\n",
      "Epoch [350/1000] train=0.0942 val=0.1189 l0=0.0000\n",
      "Epoch [360/1000] train=0.0932 val=0.1205 l0=0.0000\n",
      "Epoch [370/1000] train=0.0923 val=0.1201 l0=0.0000\n",
      "Epoch [380/1000] train=0.0920 val=0.1218 l0=0.0000\n",
      "Epoch [390/1000] train=0.0909 val=0.1214 l0=0.0000\n",
      "Epoch [400/1000] train=0.0903 val=0.1221 l0=0.0000\n",
      "Epoch [410/1000] train=0.0900 val=0.1186 l0=0.0000\n",
      "Epoch [420/1000] train=0.0892 val=0.1195 l0=0.0000\n",
      "Epoch [430/1000] train=0.0882 val=0.1208 l0=0.0000\n",
      "Epoch [440/1000] train=0.0886 val=0.1205 l0=0.0000\n",
      "Epoch [450/1000] train=0.0879 val=0.1197 l0=0.0000\n",
      "Epoch [460/1000] train=0.0876 val=0.1227 l0=0.0000\n",
      "Epoch [470/1000] train=0.0868 val=0.1209 l0=0.0000\n",
      "Early stopping at epoch 478 (best val=0.1173)\n",
      "Restored best model (val=0.1173)\n",
      "Epoch [1/1000] train=0.3256 val=0.2268 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.2072 val=0.1922 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1878 val=0.1730 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1732 val=0.1610 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1699 val=0.1539 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1643 val=0.1643 l0=0.0000\n",
      "Epoch [13/1000] train=0.1623 val=0.1502 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.1667 val=0.1494 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.1610 val=0.1490 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1607 val=0.1759 l0=0.0000\n",
      "Epoch [22/1000] train=0.1582 val=0.1472 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.1562 val=0.1443 l0=0.0000 (*)\n",
      "Epoch [24/1000] train=0.1636 val=0.1442 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.1511 val=0.1411 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1473 val=0.1484 l0=0.0000\n",
      "Epoch [31/1000] train=0.1505 val=0.1375 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1499 val=0.1401 l0=0.0000\n",
      "Epoch [41/1000] train=0.1427 val=0.1352 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1463 val=0.1399 l0=0.0000\n",
      "Epoch [58/1000] train=0.1414 val=0.1345 l0=0.0000 (*)\n",
      "Epoch [59/1000] train=0.1419 val=0.1336 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1424 val=0.1342 l0=0.0000\n",
      "Epoch [69/1000] train=0.1375 val=0.1333 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1398 val=0.1353 l0=0.0000\n",
      "Epoch [71/1000] train=0.1386 val=0.1327 l0=0.0000 (*)\n",
      "Epoch [75/1000] train=0.1393 val=0.1326 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1359 val=0.1371 l0=0.0000\n",
      "Epoch [87/1000] train=0.1368 val=0.1310 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1377 val=0.1354 l0=0.0000\n",
      "Epoch [91/1000] train=0.1342 val=0.1305 l0=0.0000 (*)\n",
      "Epoch [94/1000] train=0.1318 val=0.1301 l0=0.0000 (*)\n",
      "Epoch [98/1000] train=0.1353 val=0.1294 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1322 val=0.1314 l0=0.0000\n",
      "Epoch [102/1000] train=0.1354 val=0.1289 l0=0.0000 (*)\n",
      "Epoch [104/1000] train=0.1324 val=0.1283 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1342 val=0.1282 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1276 val=0.1318 l0=0.0000\n",
      "Epoch [125/1000] train=0.1271 val=0.1269 l0=0.0000 (*)\n",
      "Epoch [127/1000] train=0.1340 val=0.1252 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1302 val=0.1311 l0=0.0000\n",
      "Epoch [139/1000] train=0.1277 val=0.1220 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1263 val=0.1237 l0=0.0000\n",
      "Epoch [150/1000] train=0.1236 val=0.1310 l0=0.0000\n",
      "Epoch [160/1000] train=0.1250 val=0.1247 l0=0.0000\n",
      "Epoch [170/1000] train=0.1224 val=0.1250 l0=0.0000\n",
      "Epoch [180/1000] train=0.1200 val=0.1366 l0=0.0000\n",
      "Epoch [188/1000] train=0.1212 val=0.1205 l0=0.0000 (*)\n",
      "Epoch [190/1000] train=0.1233 val=0.1252 l0=0.0000\n",
      "Epoch [200/1000] train=0.1185 val=0.1236 l0=0.0000\n",
      "Epoch [210/1000] train=0.1183 val=0.1277 l0=0.0000\n",
      "Epoch [216/1000] train=0.1175 val=0.1194 l0=0.0000 (*)\n",
      "Epoch [220/1000] train=0.1170 val=0.1265 l0=0.0000\n",
      "Epoch [230/1000] train=0.1160 val=0.1225 l0=0.0000\n",
      "Epoch [240/1000] train=0.1150 val=0.1209 l0=0.0000\n",
      "Epoch [250/1000] train=0.1150 val=0.1254 l0=0.0000\n",
      "Epoch [252/1000] train=0.1165 val=0.1191 l0=0.0000 (*)\n",
      "Epoch [260/1000] train=0.1147 val=0.1246 l0=0.0000\n",
      "Epoch [270/1000] train=0.1152 val=0.1325 l0=0.0000\n",
      "Epoch [280/1000] train=0.1141 val=0.1217 l0=0.0000\n",
      "Epoch [290/1000] train=0.1127 val=0.1229 l0=0.0000\n",
      "Epoch [300/1000] train=0.1113 val=0.1215 l0=0.0000\n",
      "Epoch [310/1000] train=0.1117 val=0.1279 l0=0.0000\n",
      "Epoch [320/1000] train=0.1122 val=0.1205 l0=0.0000\n",
      "Epoch [325/1000] train=0.1105 val=0.1190 l0=0.0000 (*)\n",
      "Epoch [330/1000] train=0.1113 val=0.1221 l0=0.0000\n",
      "Epoch [335/1000] train=0.1095 val=0.1189 l0=0.0000 (*)\n",
      "Epoch [340/1000] train=0.1111 val=0.1192 l0=0.0000\n",
      "Epoch [343/1000] train=0.1090 val=0.1184 l0=0.0000 (*)\n",
      "Epoch [348/1000] train=0.1093 val=0.1170 l0=0.0000 (*)\n",
      "Epoch [350/1000] train=0.1090 val=0.1197 l0=0.0000\n",
      "Epoch [360/1000] train=0.1084 val=0.1184 l0=0.0000\n",
      "Epoch [366/1000] train=0.1086 val=0.1169 l0=0.0000 (*)\n",
      "Epoch [370/1000] train=0.1067 val=0.1205 l0=0.0000\n",
      "Epoch [380/1000] train=0.1067 val=0.1216 l0=0.0000\n",
      "Epoch [390/1000] train=0.1079 val=0.1218 l0=0.0000\n",
      "Epoch [400/1000] train=0.1060 val=0.1198 l0=0.0000\n",
      "Epoch [410/1000] train=0.1048 val=0.1173 l0=0.0000\n",
      "Epoch [411/1000] train=0.1042 val=0.1169 l0=0.0000 (*)\n",
      "Epoch [420/1000] train=0.1053 val=0.1195 l0=0.0000\n",
      "Epoch [424/1000] train=0.1038 val=0.1165 l0=0.0000 (*)\n",
      "Epoch [430/1000] train=0.1034 val=0.1185 l0=0.0000\n",
      "Epoch [440/1000] train=0.1019 val=0.1170 l0=0.0000\n",
      "Epoch [442/1000] train=0.1025 val=0.1154 l0=0.0000 (*)\n",
      "Epoch [450/1000] train=0.1018 val=0.1174 l0=0.0000\n",
      "Epoch [455/1000] train=0.1010 val=0.1149 l0=0.0000 (*)\n",
      "Epoch [460/1000] train=0.1016 val=0.1182 l0=0.0000\n",
      "Epoch [470/1000] train=0.0999 val=0.1151 l0=0.0000\n",
      "Epoch [480/1000] train=0.0995 val=0.1195 l0=0.0000\n",
      "Epoch [482/1000] train=0.0999 val=0.1149 l0=0.0000 (*)\n",
      "Epoch [487/1000] train=0.0982 val=0.1148 l0=0.0000 (*)\n",
      "Epoch [490/1000] train=0.0993 val=0.1267 l0=0.0000\n",
      "Epoch [500/1000] train=0.0995 val=0.1243 l0=0.0000\n",
      "Epoch [510/1000] train=0.0973 val=0.1153 l0=0.0000\n",
      "Epoch [520/1000] train=0.0978 val=0.1167 l0=0.0000\n",
      "Epoch [522/1000] train=0.0974 val=0.1144 l0=0.0000 (*)\n",
      "Epoch [530/1000] train=0.0982 val=0.1184 l0=0.0000\n",
      "Epoch [540/1000] train=0.0952 val=0.1169 l0=0.0000\n",
      "Epoch [541/1000] train=0.0954 val=0.1141 l0=0.0000 (*)\n",
      "Epoch [550/1000] train=0.0952 val=0.1153 l0=0.0000\n",
      "Epoch [560/1000] train=0.0956 val=0.1150 l0=0.0000\n",
      "Epoch [568/1000] train=0.0945 val=0.1134 l0=0.0000 (*)\n",
      "Epoch [570/1000] train=0.0933 val=0.1155 l0=0.0000\n",
      "Epoch [580/1000] train=0.0935 val=0.1208 l0=0.0000\n",
      "Epoch [584/1000] train=0.0935 val=0.1132 l0=0.0000 (*)\n",
      "Epoch [590/1000] train=0.0930 val=0.1164 l0=0.0000\n",
      "Epoch [600/1000] train=0.0927 val=0.1124 l0=0.0000 (*)\n",
      "Epoch [610/1000] train=0.0915 val=0.1137 l0=0.0000\n",
      "Epoch [620/1000] train=0.0916 val=0.1135 l0=0.0000\n",
      "Epoch [630/1000] train=0.0904 val=0.1126 l0=0.0000\n",
      "Epoch [640/1000] train=0.0909 val=0.1134 l0=0.0000\n",
      "Epoch [650/1000] train=0.0904 val=0.1140 l0=0.0000\n",
      "Epoch [657/1000] train=0.0889 val=0.1120 l0=0.0000 (*)\n",
      "Epoch [660/1000] train=0.0897 val=0.1132 l0=0.0000\n",
      "Epoch [667/1000] train=0.0893 val=0.1104 l0=0.0000 (*)\n",
      "Epoch [670/1000] train=0.0889 val=0.1147 l0=0.0000\n",
      "Epoch [680/1000] train=0.0891 val=0.1156 l0=0.0000\n",
      "Epoch [690/1000] train=0.0884 val=0.1145 l0=0.0000\n",
      "Epoch [700/1000] train=0.0871 val=0.1117 l0=0.0000\n",
      "Epoch [710/1000] train=0.0870 val=0.1159 l0=0.0000\n",
      "Epoch [720/1000] train=0.0869 val=0.1117 l0=0.0000\n",
      "Epoch [728/1000] train=0.0869 val=0.1100 l0=0.0000 (*)\n",
      "Epoch [730/1000] train=0.0871 val=0.1181 l0=0.0000\n",
      "Epoch [740/1000] train=0.0861 val=0.1115 l0=0.0000\n",
      "Epoch [750/1000] train=0.0859 val=0.1126 l0=0.0000\n",
      "Epoch [757/1000] train=0.0856 val=0.1093 l0=0.0000 (*)\n",
      "Epoch [760/1000] train=0.0864 val=0.1121 l0=0.0000\n",
      "Epoch [770/1000] train=0.0857 val=0.1137 l0=0.0000\n",
      "Epoch [780/1000] train=0.0852 val=0.1141 l0=0.0000\n",
      "Epoch [790/1000] train=0.0850 val=0.1137 l0=0.0000\n",
      "Epoch [800/1000] train=0.0848 val=0.1118 l0=0.0000\n",
      "Epoch [810/1000] train=0.0843 val=0.1203 l0=0.0000\n",
      "Epoch [820/1000] train=0.0833 val=0.1122 l0=0.0000\n",
      "Epoch [830/1000] train=0.0850 val=0.1117 l0=0.0000\n",
      "Epoch [840/1000] train=0.0837 val=0.1096 l0=0.0000\n",
      "Epoch [845/1000] train=0.0832 val=0.1089 l0=0.0000 (*)\n",
      "Epoch [850/1000] train=0.0832 val=0.1165 l0=0.0000\n",
      "Epoch [860/1000] train=0.0828 val=0.1129 l0=0.0000\n",
      "Epoch [870/1000] train=0.0824 val=0.1151 l0=0.0000\n",
      "Epoch [878/1000] train=0.0818 val=0.1088 l0=0.0000 (*)\n",
      "Epoch [880/1000] train=0.0824 val=0.1132 l0=0.0000\n",
      "Epoch [890/1000] train=0.0827 val=0.1124 l0=0.0000\n",
      "Epoch [900/1000] train=0.0811 val=0.1148 l0=0.0000\n",
      "Epoch [910/1000] train=0.0814 val=0.1136 l0=0.0000\n",
      "Epoch [920/1000] train=0.0808 val=0.1124 l0=0.0000\n",
      "Epoch [930/1000] train=0.0809 val=0.1126 l0=0.0000\n",
      "Epoch [940/1000] train=0.0810 val=0.1137 l0=0.0000\n",
      "Epoch [950/1000] train=0.0793 val=0.1148 l0=0.0000\n",
      "Epoch [960/1000] train=0.0804 val=0.1134 l0=0.0000\n",
      "Epoch [970/1000] train=0.0805 val=0.1169 l0=0.0000\n",
      "Epoch [979/1000] train=0.0792 val=0.1085 l0=0.0000 (*)\n",
      "Epoch [980/1000] train=0.0796 val=0.1164 l0=0.0000\n",
      "Epoch [990/1000] train=0.0791 val=0.1117 l0=0.0000\n",
      "Epoch [1000/1000] train=0.0797 val=0.1125 l0=0.0000\n",
      "Restored best model (val=0.1085)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HELOC</td>\n",
       "      <td>1</td>\n",
       "      <td>NousNet (Fixed)</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.714627</td>\n",
       "      <td>61.891135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HELOC</td>\n",
       "      <td>1</td>\n",
       "      <td>NousNet (Fixed)</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.791234</td>\n",
       "      <td>61.891135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HELOC</td>\n",
       "      <td>1</td>\n",
       "      <td>NousNet (Softmax+Proto)</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.717495</td>\n",
       "      <td>393.931322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HELOC</td>\n",
       "      <td>1</td>\n",
       "      <td>NousNet (Softmax+Proto)</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.791502</td>\n",
       "      <td>393.931322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HELOC</td>\n",
       "      <td>1</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.725143</td>\n",
       "      <td>0.229449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  fold                    model    metric     value      time_s\n",
       "0   HELOC     1          NousNet (Fixed)  Accuracy  0.714627   61.891135\n",
       "1   HELOC     1          NousNet (Fixed)       AUC  0.791234   61.891135\n",
       "2   HELOC     1  NousNet (Softmax+Proto)  Accuracy  0.717495  393.931322\n",
       "3   HELOC     1  NousNet (Softmax+Proto)       AUC  0.791502  393.931322\n",
       "4   HELOC     1                  XGBoost  Accuracy  0.725143    0.229449"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.5551 val=0.5695 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5524 val=0.5654 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.5501 val=0.5632 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5465 val=0.5637 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/1000] train=0.5412 val=0.5612 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5335 val=0.5731 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5223 val=0.5747 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5144 val=0.5784 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5076 val=0.5900 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5042 val=0.5956 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.4905 val=0.6169 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.4846 val=0.6247 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.4735 val=0.6511 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.4674 val=0.6670 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.4568 val=0.6724 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.4512 val=0.6786 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.4401 val=0.6819 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.4318 val=0.7080 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.4270 val=0.7361 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4184 val=0.7301 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4132 val=0.7392 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4063 val=0.7713 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.3981 val=0.7799 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.3948 val=0.7968 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.3892 val=0.8335 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 213 (best val=0.5612)\n",
      "Restored best model (val=0.5612)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6242 val=0.5849 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5725 val=0.5768 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.5594 val=0.5716 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5631 val=0.5666 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.5600 val=0.5650 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5528 val=0.5599 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5489 val=0.5617 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5396 val=0.5638 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5385 val=0.5720 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5400 val=0.5706 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5353 val=0.5656 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.5323 val=0.5768 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.5285 val=0.5707 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.5273 val=0.5728 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.5238 val=0.5829 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.5174 val=0.5812 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.5172 val=0.5878 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.5112 val=0.5968 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.5097 val=0.5889 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.5071 val=0.6035 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.5030 val=0.5915 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4944 val=0.6018 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4951 val=0.6007 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.4916 val=0.6144 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.4836 val=0.6301 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.4791 val=0.6210 l0=0.0000\n",
      "Early stopping at epoch 210 (best val=0.5599)\n",
      "Restored best model (val=0.5599)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6192 val=0.5640 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5629 val=0.5524 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.5585 val=0.5515 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5552 val=0.5507 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.5566 val=0.5473 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000] train=0.5514 val=0.5469 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5491 val=0.5476 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.5453 val=0.5455 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5375 val=0.5578 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5280 val=0.5538 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5159 val=0.5615 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5097 val=0.5762 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5002 val=0.5806 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.4911 val=0.5839 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.4819 val=0.6141 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.4763 val=0.6190 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.4661 val=0.6345 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.4560 val=0.6535 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.4484 val=0.6678 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.4399 val=0.6935 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.4328 val=0.7183 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.4243 val=0.7375 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4163 val=0.7239 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4097 val=0.7555 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4051 val=0.7635 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.3959 val=0.8078 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.3884 val=0.8063 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.3840 val=0.8372 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 215 (best val=0.5455)\n",
      "Restored best model (val=0.5455)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6211 val=0.5748 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5722 val=0.5590 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.5677 val=0.5560 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5667 val=0.5521 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.5591 val=0.5508 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.5584 val=0.5466 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000] train=0.5544 val=0.5464 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5574 val=0.5475 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.5504 val=0.5455 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5486 val=0.5448 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/1000] train=0.5502 val=0.5442 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5454 val=0.5450 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/1000] train=0.5443 val=0.5441 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5417 val=0.5437 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5402 val=0.5488 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5359 val=0.5516 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.5345 val=0.5518 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.5294 val=0.5522 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.5225 val=0.5587 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.5213 val=0.5657 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.5152 val=0.5664 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.5124 val=0.5733 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.5130 val=0.5726 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.5046 val=0.5850 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.5046 val=0.5951 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.5007 val=0.5843 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4965 val=0.5944 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4916 val=0.6133 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.4853 val=0.6075 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.4879 val=0.6235 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.4785 val=0.6373 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000] train=0.4748 val=0.6374 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/1000] train=0.4690 val=0.6432 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/1000] train=0.4658 val=0.6434 l0=0.0000\n",
      "Early stopping at epoch 240 (best val=0.5437)\n",
      "Restored best model (val=0.5437)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6102 val=0.5842 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5611 val=0.5670 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5534 val=0.5657 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/1000] train=0.5482 val=0.5643 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5482 val=0.5643 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/1000] train=0.5429 val=0.5630 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.5442 val=0.5616 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/1000] train=0.5386 val=0.5587 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5378 val=0.5612 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/1000] train=0.5372 val=0.5580 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5305 val=0.5657 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5188 val=0.5667 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5080 val=0.5707 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.4996 val=0.5827 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.4905 val=0.5994 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.4812 val=0.6165 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.4719 val=0.6340 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.4621 val=0.6444 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.4571 val=0.6664 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.4510 val=0.6815 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.4375 val=0.6971 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.4316 val=0.7272 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.4259 val=0.7335 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4143 val=0.7413 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4120 val=0.7790 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4072 val=0.7845 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.4003 val=0.7996 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.3986 val=0.8164 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.3886 val=0.8311 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000] train=0.3852 val=0.8468 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 221 (best val=0.5580)\n",
      "Restored best model (val=0.5580)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6179 val=0.5775 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5770 val=0.5716 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.5579 val=0.5683 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.5584 val=0.5636 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.5586 val=0.5618 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5632 val=0.5762 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.5458 val=0.5614 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/1000] train=0.5514 val=0.5602 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/1000] train=0.5513 val=0.5597 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5497 val=0.5611 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/1000] train=0.5453 val=0.5580 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5442 val=0.5599 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/1000] train=0.5411 val=0.5576 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/1000] train=0.5407 val=0.5558 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/1000] train=0.5387 val=0.5549 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5367 val=0.5619 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5369 val=0.5613 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5337 val=0.5577 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.5313 val=0.5572 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.5245 val=0.5661 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.5214 val=0.5690 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.5201 val=0.5643 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.5106 val=0.5735 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.5102 val=0.5748 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.5077 val=0.5887 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.5058 val=0.5992 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.5033 val=0.6021 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4945 val=0.6081 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4881 val=0.6007 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4819 val=0.6159 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.4768 val=0.6211 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.4784 val=0.6221 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.4720 val=0.6339 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000] train=0.4703 val=0.6368 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/1000] train=0.4639 val=0.6579 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 238 (best val=0.5549)\n",
      "Restored best model (val=0.5549)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6063 val=0.5885 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5631 val=0.5628 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5505 val=0.5626 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.5496 val=0.5610 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.5521 val=0.5604 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000] train=0.5490 val=0.5598 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/1000] train=0.5498 val=0.5581 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5476 val=0.5610 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/1000] train=0.5445 val=0.5579 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5395 val=0.5674 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5253 val=0.5773 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5147 val=0.5918 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5073 val=0.5962 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5012 val=0.5951 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.4888 val=0.6148 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.4793 val=0.6331 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.4663 val=0.6522 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.4607 val=0.6747 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.4461 val=0.6829 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.4386 val=0.7258 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.4338 val=0.7338 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.4254 val=0.7316 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.4187 val=0.7466 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4132 val=0.7637 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4155 val=0.7705 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4034 val=0.8011 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.3991 val=0.7893 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.3965 val=0.8154 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.3912 val=0.7953 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 212 (best val=0.5579)\n",
      "Restored best model (val=0.5579)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6233 val=0.5777 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5700 val=0.5682 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.5609 val=0.5655 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.5606 val=0.5572 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.5564 val=0.5555 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5551 val=0.5640 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000] train=0.5588 val=0.5555 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.5482 val=0.5552 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/1000] train=0.5491 val=0.5537 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5483 val=0.5544 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/1000] train=0.5456 val=0.5537 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/1000] train=0.5465 val=0.5535 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5495 val=0.5659 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5415 val=0.5613 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5377 val=0.5581 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5348 val=0.5599 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.5304 val=0.5634 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.5301 val=0.5588 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.5247 val=0.5645 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.5195 val=0.5668 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.5194 val=0.5629 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.5170 val=0.5658 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.5146 val=0.5619 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.5089 val=0.5779 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.5029 val=0.5833 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4996 val=0.5763 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4975 val=0.5752 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4949 val=0.5873 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.4869 val=0.5853 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.4847 val=0.5881 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.4833 val=0.6118 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000] train=0.4745 val=0.6062 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 228 (best val=0.5535)\n",
      "Restored best model (val=0.5535)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6109 val=0.5704 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.5619 val=0.5691 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.5599 val=0.5615 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5543 val=0.5609 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.5523 val=0.5609 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000] train=0.5523 val=0.5587 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/1000] train=0.5489 val=0.5586 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5487 val=0.5612 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000] train=0.5470 val=0.5581 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5410 val=0.5674 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5291 val=0.5744 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5206 val=0.5828 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5107 val=0.5898 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.4992 val=0.5945 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.4894 val=0.6110 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.4786 val=0.6292 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.4740 val=0.6398 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.4624 val=0.6458 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.4545 val=0.6773 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.4430 val=0.6984 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.4324 val=0.7077 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.4274 val=0.7161 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.4164 val=0.7588 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4103 val=0.7598 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4067 val=0.7741 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.3939 val=0.8029 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.3898 val=0.8231 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.3852 val=0.8501 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.3754 val=0.8736 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 211 (best val=0.5581)\n",
      "Restored best model (val=0.5581)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.6215 val=0.5789 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.5682 val=0.5700 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.5639 val=0.5679 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.5576 val=0.5673 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000] train=0.5587 val=0.5625 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/1000] train=0.5547 val=0.5606 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.5528 val=0.5795 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.5480 val=0.5616 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.5461 val=0.5623 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.5401 val=0.5625 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.5415 val=0.5702 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.5331 val=0.5719 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.5308 val=0.5723 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.5266 val=0.5709 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.5257 val=0.5826 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.5170 val=0.5836 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.5147 val=0.5879 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.5077 val=0.6060 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.5033 val=0.5926 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.5003 val=0.6174 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.4935 val=0.6124 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.4921 val=0.6164 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.4881 val=0.6283 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.4849 val=0.6250 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.4810 val=0.6258 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.4783 val=0.6665 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 209 (best val=0.5606)\n",
      "Restored best model (val=0.5606)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Adult (classification) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.3564 val=0.3116 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.3227 val=0.2977 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.3160 val=0.2964 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.3119 val=0.2962 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.3108 val=0.2938 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000] train=0.3043 val=0.2933 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.3026 val=0.2945 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000] train=0.3013 val=0.2927 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/1000] train=0.2985 val=0.2923 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/1000] train=0.2963 val=0.2921 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.2937 val=0.2945 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.2865 val=0.2993 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.2777 val=0.3143 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.2694 val=0.3296 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.2612 val=0.3406 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.2550 val=0.3496 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.2482 val=0.3724 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.2432 val=0.3915 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.2396 val=0.4016 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.2355 val=0.4319 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.2338 val=0.4355 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.2302 val=0.4578 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.2279 val=0.4500 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.2256 val=0.4733 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.2226 val=0.4862 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.2221 val=0.4982 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.2201 val=0.5146 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.2184 val=0.5195 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.2168 val=0.5283 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.2161 val=0.5519 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 217 (best val=0.2921)\n",
      "Restored best model (val=0.2921)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.4503 val=0.3785 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.3506 val=0.3238 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.3369 val=0.3067 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.3396 val=0.3046 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.3334 val=0.3034 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.3298 val=0.2956 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/1000] train=0.3246 val=0.2951 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.3234 val=0.2946 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/1000] train=0.3201 val=0.2931 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.3194 val=0.3016 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.3160 val=0.3751 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/1000] train=0.3119 val=0.2902 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/1000] train=0.3103 val=0.2896 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.3100 val=0.2952 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/1000] train=0.3093 val=0.2896 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/1000] train=0.3066 val=0.2895 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.3071 val=0.2994 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/1000] train=0.3060 val=0.2894 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/1000] train=0.3051 val=0.2889 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.3039 val=0.2914 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/1000] train=0.3054 val=0.2873 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.3026 val=0.2895 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.3018 val=0.2904 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.3005 val=0.2893 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.2985 val=0.2921 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.2971 val=0.2899 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.2952 val=0.2914 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.2935 val=0.2942 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.2915 val=0.2959 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.2889 val=0.2952 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.2855 val=0.2980 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.2829 val=0.3042 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.2788 val=0.3047 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.2763 val=0.3106 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.2715 val=0.3257 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.2706 val=0.3188 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000] train=0.2656 val=0.3461 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/1000] train=0.2620 val=0.3358 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/1000] train=0.2603 val=0.3489 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/1000] train=0.2569 val=0.3588 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/1000] train=0.2542 val=0.3581 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 261 (best val=0.2873)\n",
      "Restored best model (val=0.2873)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.3572 val=0.3251 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.3209 val=0.3232 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.3150 val=0.3152 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.3101 val=0.3132 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.3069 val=0.3131 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.3041 val=0.3123 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000] train=0.3037 val=0.3089 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.3012 val=0.3115 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000] train=0.3003 val=0.3076 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/1000] train=0.2963 val=0.3075 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.2936 val=0.3095 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.2855 val=0.3131 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.2777 val=0.3231 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.2700 val=0.3247 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.2629 val=0.3381 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.2556 val=0.3663 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.2493 val=0.3728 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.2436 val=0.3873 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.2381 val=0.4004 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.2336 val=0.4257 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.2309 val=0.4413 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.2284 val=0.4580 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.2263 val=0.4706 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.2240 val=0.4923 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.2217 val=0.4896 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.2201 val=0.5071 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.2181 val=0.5171 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.2174 val=0.5104 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.2156 val=0.5139 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.2149 val=0.5377 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 216 (best val=0.3075)\n",
      "Restored best model (val=0.3075)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.4516 val=0.3830 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.3620 val=0.3420 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.3520 val=0.3288 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.3389 val=0.3244 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.3318 val=0.3227 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.3292 val=0.3842 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/1000] train=0.3237 val=0.3190 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/1000] train=0.3188 val=0.3160 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/1000] train=0.3223 val=0.3142 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.3154 val=0.3201 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/1000] train=0.3160 val=0.3123 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/1000] train=0.3120 val=0.3082 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.3123 val=0.3082 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/1000] train=0.3094 val=0.3055 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.3082 val=0.3094 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/1000] train=0.3085 val=0.3041 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/1000] train=0.3064 val=0.3041 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.3061 val=0.3184 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/1000] train=0.3073 val=0.3035 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/1000] train=0.3051 val=0.3022 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.3047 val=0.3037 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/1000] train=0.3043 val=0.3020 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.3033 val=0.3049 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.3016 val=0.3015 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.2997 val=0.3052 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.2980 val=0.3046 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.2960 val=0.3050 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.2948 val=0.3056 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.2930 val=0.3069 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.2891 val=0.3165 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.2866 val=0.3189 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.2826 val=0.3204 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.2787 val=0.3276 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.2751 val=0.3308 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.2722 val=0.3412 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.2696 val=0.3445 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.2664 val=0.3548 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000] train=0.2630 val=0.3593 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/1000] train=0.2592 val=0.3616 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/1000] train=0.2580 val=0.3810 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/1000] train=0.2553 val=0.3785 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/1000] train=0.2522 val=0.3701 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/1000] train=0.2510 val=0.3939 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/1000] train=0.2484 val=0.4024 l0=0.0000\n",
      "Early stopping at epoch 280 (best val=0.3015)\n",
      "Restored best model (val=0.3015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.3542 val=0.3343 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.3183 val=0.3240 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.3127 val=0.3189 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000] train=0.3066 val=0.3188 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000] train=0.3044 val=0.3173 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.3017 val=0.3161 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.2980 val=0.3164 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.2955 val=0.3157 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/1000] train=0.2935 val=0.3150 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.2906 val=0.3149 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.2841 val=0.3256 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.2769 val=0.3358 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.2677 val=0.3534 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.2612 val=0.3540 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.2544 val=0.3727 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.2492 val=0.3854 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.2445 val=0.4056 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.2402 val=0.4229 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.2363 val=0.4308 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000] train=0.2332 val=0.4458 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000] train=0.2304 val=0.4544 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000] train=0.2277 val=0.4640 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000] train=0.2246 val=0.4811 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000] train=0.2228 val=0.4805 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000] train=0.2207 val=0.5070 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] train=0.2189 val=0.5152 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000] train=0.2168 val=0.5080 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000] train=0.2148 val=0.5281 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000] train=0.2137 val=0.5513 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000] train=0.2137 val=0.5409 l0=0.0000\n",
      "Early stopping at epoch 220 (best val=0.3149)\n",
      "Restored best model (val=0.3149)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] train=0.4250 val=0.3515 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] train=0.3678 val=0.3446 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] train=0.3505 val=0.3309 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000] train=0.3429 val=0.3306 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000] train=0.3321 val=0.3246 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] train=0.3300 val=0.3235 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000] train=0.3253 val=0.3207 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000] train=0.3218 val=0.3185 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/1000] train=0.3165 val=0.3177 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/1000] train=0.3201 val=0.3168 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000] train=0.3148 val=0.3177 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/1000] train=0.3143 val=0.3151 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000] train=0.3121 val=0.3318 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/1000] train=0.3112 val=0.3136 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/1000] train=0.3088 val=0.3134 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/1000] train=0.3077 val=0.3127 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/1000] train=0.3068 val=0.3124 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000] train=0.3077 val=0.3280 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/1000] train=0.3050 val=0.3116 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000] train=0.3045 val=0.3109 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/1000] train=0.3037 val=0.3108 l0=0.0000 (*)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000] train=0.3014 val=0.3192 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000] train=0.3000 val=0.3120 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000] train=0.3000 val=0.3112 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000] train=0.2987 val=0.3123 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000] train=0.2972 val=0.3131 l0=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000] train=0.2954 val=0.3129 l0=0.0000\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "results = []\n",
    "\n",
    "# HELOC\n",
    "try:\n",
    "    Xh, yh, f_h, c_h = load_heloc()\n",
    "    print(\"\\n=== HELOC (classification) ===\")\n",
    "    res_h = run_cv_benchmark(Xh, yh, f_h, c_h, dataset_name=\"HELOC\", task_type=\"classification\", n_splits=5)\n",
    "    results.append(res_h)\n",
    "except Exception as e:\n",
    "    print(\"HELOC failed to load:\", e)\n",
    "\n",
    "# Adult\n",
    "try:\n",
    "    Xa, ya, f_a, c_a = load_adult()\n",
    "    print(\"\\n=== Adult (classification) ===\")\n",
    "    res_a = run_cv_benchmark(Xa, ya, f_a, c_a, dataset_name=\"Adult\", task_type=\"classification\", n_splits=5)\n",
    "    results.append(res_a)\n",
    "except Exception as e:\n",
    "    print(\"Adult failed to load:\", e)\n",
    "\n",
    "# Breast Cancer\n",
    "try:\n",
    "    Xb, yb, f_b, c_b = load_breast_cancer_ds()\n",
    "    print(\"\\n=== Breast Cancer (classification) ===\")\n",
    "    res_b = run_cv_benchmark(Xb, yb, f_b, c_b, dataset_name=\"BreastCancer\", task_type=\"classification\", n_splits=5)\n",
    "    results.append(res_b)\n",
    "except Exception as e:\n",
    "    print(\"Breast Cancer failed to load:\", e)\n",
    "\n",
    "# California Housing\n",
    "try:\n",
    "    Xc, yc, f_c = load_california()\n",
    "    print(\"\\n=== California Housing (regression) ===\")\n",
    "    res_c = run_cv_benchmark(Xc, yc, f_c, class_names=None, dataset_name=\"California\", task_type=\"regression\", n_splits=5)\n",
    "    results.append(res_c)\n",
    "except Exception as e:\n",
    "    print(\"California failed to load:\", e)\n",
    "\n",
    "all_results = pd.concat(results, axis=0, ignore_index=True) if results else pd.DataFrame()\n",
    "all_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c2aa9",
   "metadata": {
    "_cell_guid": "6418a754-4575-4797-bed4-561e9ebf536b",
    "_uuid": "34ad6353-359c-4c0c-bc5a-fed5444e40fc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Aggregate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0801aa30",
   "metadata": {
    "_cell_guid": "201303c0-94ee-4401-97ef-5527abadee16",
    "_uuid": "24d605c9-428d-46ee-985e-63ad49dd59d7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset                   model   metric   mean    std  mean_time_s\n",
      "       Adult                     EBM      AUC 0.9264 0.0022       8.9093\n",
      "       Adult                     EBM Accuracy 0.8721 0.0027       8.9093\n",
      "       Adult                     KAN      AUC 0.9138 0.0021     777.4691\n",
      "       Adult                     KAN Accuracy 0.8602 0.0038     777.4691\n",
      "       Adult                     MLP      AUC 0.9130 0.0026     129.4381\n",
      "       Adult                     MLP Accuracy 0.8584 0.0042     129.4381\n",
      "       Adult         NousNet (Fixed)      AUC 0.9131 0.0022     314.0258\n",
      "       Adult         NousNet (Fixed) Accuracy 0.8586 0.0033     314.0258\n",
      "       Adult NousNet (Softmax+Proto)      AUC 0.9140 0.0021    8231.9642\n",
      "       Adult NousNet (Softmax+Proto) Accuracy 0.8581 0.0040    8231.9642\n",
      "       Adult                 XGBoost      AUC 0.9290 0.0022       1.1645\n",
      "       Adult                 XGBoost Accuracy 0.8743 0.0030       1.1645\n",
      "BreastCancer                     EBM      AUC 0.9957 0.0056       0.7359\n",
      "BreastCancer                     EBM Accuracy 0.9701 0.0237       0.7359\n",
      "BreastCancer                     KAN      AUC 0.9950 0.0068       6.9398\n",
      "BreastCancer                     KAN Accuracy 0.9649 0.0138       6.9398\n",
      "BreastCancer                     MLP      AUC 0.9962 0.0066       2.1994\n",
      "BreastCancer                     MLP Accuracy 0.9789 0.0048       2.1994\n",
      "BreastCancer         NousNet (Fixed)      AUC 0.9950 0.0064       6.3405\n",
      "BreastCancer         NousNet (Fixed) Accuracy 0.9754 0.0114       6.3405\n",
      "BreastCancer NousNet (Softmax+Proto)      AUC 0.9920 0.0098      54.4870\n",
      "BreastCancer NousNet (Softmax+Proto) Accuracy 0.9701 0.0100      54.4870\n",
      "BreastCancer                 XGBoost      AUC 0.9934 0.0068       0.2304\n",
      "BreastCancer                 XGBoost Accuracy 0.9649 0.0088       0.2304\n",
      "  California                     EBM      MAE 0.3986 0.0061       1.7771\n",
      "  California                     EBM       R2 0.7624 0.0103       1.7771\n",
      "  California                     EBM     RMSE 0.5623 0.0117       1.7771\n",
      "  California                     KAN      MAE 0.3710 0.0088     355.7970\n",
      "  California                     KAN       R2 0.7762 0.0142     355.7970\n",
      "  California                     KAN     RMSE 0.5456 0.0175     355.7970\n",
      "  California                     MLP      MAE 0.3359 0.0071      98.6053\n",
      "  California                     MLP       R2 0.8040 0.0074      98.6053\n",
      "  California                     MLP     RMSE 0.5107 0.0118      98.6053\n",
      "  California         NousNet (Fixed)      MAE 0.3406 0.0074     256.9243\n",
      "  California         NousNet (Fixed)       R2 0.8017 0.0072     256.9243\n",
      "  California         NousNet (Fixed)     RMSE 0.5137 0.0095     256.9243\n",
      "  California       NousNet (Softmax)      MAE 0.3307 0.0075    1565.1374\n",
      "  California       NousNet (Softmax)       R2 0.8086 0.0102    1565.1374\n",
      "  California       NousNet (Softmax)     RMSE 0.5045 0.0105    1565.1374\n",
      "  California                 XGBoost      MAE 0.2860 0.0054       1.6930\n",
      "  California                 XGBoost       R2 0.8548 0.0082       1.6930\n",
      "  California                 XGBoost     RMSE 0.4394 0.0109       1.6930\n",
      "       HELOC                     EBM      AUC 0.7992 0.0058       1.4208\n",
      "       HELOC                     EBM Accuracy 0.7268 0.0082       1.4208\n",
      "       HELOC                     KAN      AUC 0.7955 0.0070      73.5118\n",
      "       HELOC                     KAN Accuracy 0.7222 0.0103      73.5118\n",
      "       HELOC                     MLP      AUC 0.7915 0.0044      28.0916\n",
      "       HELOC                     MLP Accuracy 0.7222 0.0074      28.0916\n",
      "       HELOC         NousNet (Fixed)      AUC 0.7909 0.0040      65.2987\n",
      "       HELOC         NousNet (Fixed) Accuracy 0.7197 0.0097      65.2987\n",
      "       HELOC NousNet (Softmax+Proto)      AUC 0.7922 0.0021     419.9995\n",
      "       HELOC NousNet (Softmax+Proto) Accuracy 0.7165 0.0021     419.9995\n",
      "       HELOC                 XGBoost      AUC 0.7957 0.0071       0.1930\n",
      "       HELOC                 XGBoost Accuracy 0.7232 0.0072       0.1930\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABjYAAASlCAYAAAALTeBgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd5hU5dUA8DO7NAFBmiBFQLChURSFWLEQu8ZeYyFiQ4wRa9SAJfk05tOgfioxdsUeFXuJsfdG7AoIoihFQZAFKbvv9wfZgYVdZIHd5S6/3/Pw6J65M/c9e2Zn79lz70wupZQCAAAAAAAgAwpqegEAAAAAAABLy2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAltK9994bzZs3jxkzZqzQx33yySeje/fu0aBBg8jlcvHDDz8s9X0vuOCCyOVyS7VtLpeLCy64IP/10KFDY+21147Zs2dXcsUAAKxIt9xyS+RyuRg7dmyl7/v8889HLpeL559/foWvixXjq6++igYNGsQrr7yyQh935MiRscsuu0TTpk0jl8vFQw89tNT3rczzZocddogddtgh//XHH38cderUiQ8//LDyiwZYQQw2gFqptDF4++23y719hx12iI033rhMrFOnTpHL5cr9t9tuu+W3K/1D8nfffbfENcydOzeuuuqq2HLLLWP11VePxo0bx5ZbbhlXXXVVzJ07t9z7FBcXx8033xw77LBDNG/ePOrXrx+dOnWKvn37VphLZVx77bWRy+WiV69ey/1Yq5ri4uIYPHhwnHLKKdG4ceN8fIcddvjZ58ySfP/993HwwQfHaqutFtdcc03cfvvt0ahRo6pKo4xjjjkm5syZE3//+9+rZX8AALXdynK8feedd8aQIUNW2ON98sknkcvlokGDBpU6CYf5LrrooujVq1dss802+VhpX7novwYNGiz14x599NHxwQcfxJ///Oe4/fbbY4sttqiK5S+mW7duseeee8agQYOqZX8A5alT0wsAWJl07949Tj/99MXibdu2rdTjFBUVxZ577hkvvPBC7LXXXnHMMcdEQUFBPPnkk3HqqafGAw88EI899liZP2DPmjUr9t9//3jyySdj++23j3PPPTeaN28eY8eOjXvvvTduvfXWGDduXLRv336Z8xs2bFh06tQp3nzzzRg1alR07dp1mR9rVfPII4/EZ599Fscff/xit7Vv3z4uueSSMrGlfc689dZb8eOPP8bFF18cffr0WSFrXVoNGjSIo48+Oq644oo45ZRTlvrKDwAAyreyHG/feeed8eGHH8bvf//7FfJ4d9xxR7Rp0yamTp0a999/f/Tr12+FPO6qYPLkyXHrrbfGrbfeWu7t1113XZkTpwoLC5fqcWfNmhWvvfZanHfeeTFgwIAVstbKOPHEE2OPPfaI0aNHR5cuXap9/wAGGwALadeuXfzmN79Z7scZOHBgvPDCC3H11VeXOcg86aST4pprrokBAwbEGWecEdddd13+tjPPPDOefPLJ+Nvf/rZYAzJ48OD429/+tlxrGjNmTLz66qvxwAMPxAknnBDDhg2LwYMHL9djVpWioqJqu2phad18882xzTbbRLt27Ra7rWnTpsv8vJk0aVJERKyxxhrLs7xldvDBB8dll10Wzz33XOy00041sgYAgNogS8fblZFSijvvvDMOP/zwGDNmTAwbNmylHWysjH3EHXfcEXXq1Im999673NsPPPDAaNmyZaUfd/LkyRFRc31Enz59olmzZnHrrbfGRRddVCNrAFZt3ooKYAX7+uuv48Ybb4yddtqp3DNnTj755Nhxxx3jhhtuiK+//jp/n7///e/xq1/9qtyzqgoLC+OMM84oc7XGp59+GuPGjVvqdQ0bNiyaNWsWe+65Zxx44IExbNiwcrf74Ycf4rTTTotOnTpF/fr1o3379nHUUUeVeeutn376KS644IJYb731okGDBrHWWmvF/vvvH6NHj46Iit+vdezYsZHL5eKWW27Jx4455pho3LhxjB49OvbYY49YffXV44gjjoiIiJdeeikOOuigWHvttaN+/frRoUOHOO2002LWrFmLrfvTTz+Ngw8+OFq1ahWrrbZarL/++nHeeedFRMRzzz0XuVwuHnzwwcXud+edd0Yul4vXXnutwu/dTz/9FE8++eQSr6iYN29epT97Y4cddoijjz46IiK23HLLyOVyccwxx+Rvv++++6JHjx6x2mqrRcuWLeM3v/lNjB8//mcfd/bs2XHaaadFq1atYvXVV4999tkn/1xbVI8ePaJ58+YxfPjwSq0dAICylvZ4+6OPPoqddtopVltttWjfvn386U9/ipKSksW2W/Tz0Up16tSpzDHjonbYYYd47LHH4ssvv8y/vVGnTp3yt48bNy4+/fTTpc7rlVdeibFjx8ahhx4ahx56aLz44ovlHluWlJTElVdeGb/4xS+iQYMG0apVq9htt90We0vdO+64I3r27BkNGzaMZs2axfbbbx9PP/10pfMuffvhF154Ifr37x9rrrlmvl/68ssvo3///rH++uvHaqutFi1atIiDDjqo3M8wWVL/M2PGjGjUqFGceuqpi93v66+/jsLCwsWu3F7UQw89FL169SpzVcbCUkoxffr0SCkt8XEWdsEFF0THjh0jYv4JcovW+L333ovdd989mjRpEo0bN46dd945Xn/99aV67Ouvvz66dOkSq622WvTs2TNeeumlcrerW7du7LDDDvoIoMa4YgOo1aZNm1buZ2FU9BkXc+fOLXf7Ro0axWqrrbZU+3ziiSeiuLg4jjrqqAq3Oeqoo+K5556LJ598Mvr16xdPPPFEzJs3L4488sil2kdExIYbbhi9e/de6g8JHDZsWOy///5Rr169OOyww+K6666Lt956K7bccsv8NjNmzIjtttsuPvnkk/jtb38bm2++eXz33Xfx8MMPx9dffx0tW7aM4uLi2GuvveLZZ5+NQw89NE499dT48ccf45lnnokPP/xwmS5DnjdvXuy6666x7bbbxv/+7/9Gw4YNI2L+H/ZnzpwZJ510UrRo0SLefPPNuPrqq+Prr7+O++67L3//999/P7bbbruoW7duHH/88dGpU6cYPXp0PPLII/HnP/85dthhh+jQoUMMGzYs9ttvv8W+L126dImtttqqwvW98847MWfOnNh8883Lvf3zzz+PRo0axZw5c6J169Zx3HHHxaBBg6Ju3bpLzPu8886L9ddfP66//vq46KKLonPnzvnv3y233BJ9+/aNLbfcMi655JKYOHFiXHnllfHKK6/Ee++9t8Qzs/r16xd33HFHHH744bH11lvHv//979hzzz0r3H7zzTdf4R9kCACwqlma4+0JEybEjjvuGPPmzYtzzjknGjVqFNdff/1S9xpL47zzzotp06bF119/nb/qe+E/qh911FHxwgsvLPUf0kuPl7fccsvYeOONo2HDhnHXXXfFmWeeWWa7Y489Nm655ZbYfffdo1+/fjFv3rx46aWX4vXXX89/9sOFF14YF1xwQWy99dZx0UUXRb169eKNN96If//737HLLrssU779+/ePVq1axaBBg6KoqCgi5r/d66uvvhqHHnpotG/fPsaOHRvXXXdd7LDDDvHxxx/n+42f63+6d+8e++23X9xzzz1xxRVXlHmbqLvuuitSSvmTssozd+7ceOutt+Kkk06qcJt11lknP0DZd9994/LLL4/WrVsvMef9998/1lhjjTjttNPisMMOiz322CNf448++ii22267aNKkSZx11llRt27d+Pvf/x477LBDvPDCC0v8/Jcbb7wxTjjhhNh6663j97//fXzxxRexzz77RPPmzaNDhw6Lbd+jR48YPnx4TJ8+PZo0abLENQOscAmgFrr55ptTRCzx30YbbVTmPh07dqxw20suuSS/3eDBg1NEpMmTJ5e779///vcpItJ7771X4frefffdFBFp4MCBKaWUTjvttJ+9z6IiIvXu3Xuptn377bdTRKRnnnkmpZRSSUlJat++fTr11FPLbDdo0KAUEemBBx5Y7DFKSkpSSinddNNNKSLSFVdcUeE2zz33XIqI9Nxzz5W5fcyYMSki0s0335yPHX300Ski0jnnnLPY482cOXOx2CWXXJJyuVz68ssv87Htt98+rb766mViC68npZT+8Ic/pPr166cffvghH5s0aVKqU6dOGjx48GL7WdgNN9yQIiJ98MEHi93229/+Nl1wwQXpn//8Z7rtttvSPvvskyIiHXzwwUt8zFKlz9W33norH5szZ05ac80108Ybb5xmzZqVjz/66KMpItKgQYPysdLnY6kRI0akiEj9+/cvs5/DDz88RUS5uR5//PFptdVWW6r1AgCwuKU93i7tFd544418bNKkSalp06YpItKYMWPy8YqO3Tp27JiOPvro/NflHXvvueeeqWPHjuWutXfv3mWOH5dkzpw5qUWLFum8887Lxw4//PC06aabltnu3//+d4qI9Lvf/W6xxyg9Jh85cmQqKChI++23XyouLi53m5SWPu/S4+htt902zZs3r8y25fURr732WoqIdNttt+VjS9P/PPXUUyki0hNPPFHm9k022eRn+7FRo0aliEhXX331YrcNGTIkDRgwIA0bNizdf//96dRTT0116tRJ6667bpo2bdoSHzelBb3VX//61zLxfffdN9WrVy+NHj06H/vmm2/S6quvnrbffvt8bNHnTWkP0r179zR79uz8dtdff32Fveedd9652PMZoLp4KyqgVrvmmmvimWeeWezfJptsUu72vXr1Knf7ww47bKn3+eOPP0ZExOqrr17hNqW3TZ8+vcx/l3SfRaWUKnW1RuvWrWPHHXeMiPmXdx9yyCFx9913R3FxcX67f/7zn7HpppsudlVD6X1Kt2nZsmWccsopFW6zLMo7i2nhM9eKioriu+++i6233jpSSvHee+9FxPz3ln3xxRfjt7/9bay99toVrueoo46K2bNnx/3335+P3XPPPTFv3ryf/XyM77//PiIimjVrtthtN954YwwePDj233//OPLII2P48OFx3HHHxb333rvUl3sv6u23345JkyZF//79o0GDBvn4nnvuGRtssEE89thjFd738ccfj4iI3/3ud2XiS/rgyGbNmsWsWbNi5syZy7ReAIBV3dIebz/++OPxy1/+Mnr27JmPtWrVaoln/a9ozz///FJfrfHEE0/E999/X6YfOuyww+I///lPfPTRR/nYP//5z8jlcuV+pkjpMflDDz0UJSUlMWjQoCgoKCh3m2Vx3HHHLfaB2wv3EXPnzo3vv/8+unbtGmussUa8++67Zdb9c/1Pnz59om3btmXeWuzDDz+M999/f7n6iFNPPTWuvvrqOPzww+OAAw6IIUOGxK233hojR46Ma6+9dikyX1xxcXE8/fTTse+++8Y666yTj6+11lpx+OGHx8svv5zvPRdV2oOceOKJUa9evXz8mGOOiaZNm5Z7n9K8ynvXA4CqZrAB1Go9e/aMPn36LPavvAPLiIiWLVuWu33p+5cujdLhROmAozyLDj9KL9td0n2WVXFxcdx9992x4447xpgxY2LUqFExatSo6NWrV0ycODGeffbZ/LajR4+OjTfeeImPN3r06Fh//fWjTp0V926GderUKfP5IaXGjRsXxxxzTDRv3jwaN24crVq1it69e0fE/LcZi4j44osvIiJ+dt0bbLBBbLnllmUakmHDhsUvf/nL6Nq161Ktc2kbwNNPPz0iIv71r39FRMScOXNiwoQJZf4t3OAu6ssvv4yIiPXXX7/cPEpvr+i+BQUFi70lWHmPVao0r+VpKAEAVlWVOd7+8ssvY911113sMZZ0rFaT7rjjjujcuXPUr18/n1eXLl2iYcOGZY6rR48eHW3bto3mzZtX+FijR4+OgoKC6Nat2wpdY+fOnReLzZo1KwYNGhQdOnSI+vXrR8uWLaNVq1bxww8/5PuI0jX9XB9RUFAQRxxxRDz00EP5E4GGDRsWDRo0iIMOOmip1ri0fcThhx8ebdq0yfcREbFYH1He5w2Wmjx5csycObPc59OGG24YJSUl8dVXX5V739IeY9HnZ926dcsMSRamjwBqksEGwAq24YYbRsT8z32oSOltpQf1G2ywQUREfPDBByt8Pf/+97/j22+/jbvvvjvWXXfd/L+DDz44IqLCDzVcHhUd2Fb0x/z69esvdtZWcXFx/OpXv4rHHnsszj777HjooYfimWeeyX/weHkfsPhzSt9P+Ouvv47Ro0fH66+//rNnWUVEtGjRIiIipk6dulT7KX3/2SlTpkRExKuvvhprrbVWmX8VNRQ1YerUqdGwYcMV+t7OAACriuo+3l7SCTIr0vTp0+ORRx6JMWPGlMmrW7duMXPmzLjzzjsr9YHXy6uivMs7hj3llFPiz3/+cxx88MFx7733xtNPPx3PPPNMtGjRYpn7iBkzZsRDDz0UKaW48847Y6+99qrwSoZSle0jIub3EqV9REQs1kfcc889lV5/VSnNq2XLljW8EmBV5MPDAVaw3XffPQoLC+P222+v8APEb7vttqhTp07stttuZe5zxx13VOoDxJfGsGHDYs0114xrrrlmsdseeOCBePDBB2Po0KGx2mqrRZcuXeLDDz9c4uN16dIl3njjjZg7d26FH45dekXMDz/8UCa+pCsNFvXBBx/E559/HrfeemuZ7+MzzzxTZrvSs4d+bt0REYceemgMHDgw7rrrrpg1a1bUrVs3DjnkkJ+9X+ngacyYMfGLX/ziZ7cvvYqkVatWERGx6aabLrbuNm3aVHj/0iuEPvvss9hpp53K3PbZZ58t8Qqijh07RklJSf7KmoXvV5ExY8bkB3IAAFROZY63O3bsGCNHjlxsu/KO1Zo1a7bY8fScOXPi22+//dk1rYgz6B944IH46aef4rrrrlvsD9efffZZnH/++fHKK6/EtttuG126dImnnnoqpkyZUuFVG126dImSkpL4+OOPo3v37hXud3nyLnX//ffH0UcfHZdffnk+9tNPPy32uEvT/0TMvzp8s802i2HDhkX79u1j3LhxcfXVV//s/dZee+1YbbXVYsyYMUu17pRSjB07NjbbbLN8bNE+YqONNqrw/q1atYqGDRuW+3z69NNPo6CgoNwPAY9Y0IOMHDmyTA8yd+7cGDNmTGy66aaL3WfMmDFRUFAQ66233pITA6gCrtgAWME6dOgQffv2jX/9619x3XXXLXb70KFD49///ncce+yx+bdf6tChQxx33HHx9NNPl3uAXFJSEpdffnl8/fXX+dinn34a48aNW+JaZs2aFQ888EDstddeceCBBy72b8CAAfHjjz/Gww8/HBERBxxwQPznP/+JBx98cLHHKj0b64ADDojvvvsu/u///q/CbTp27BiFhYXx4osvlrm9Mu8VW/o+uQufBZZSiiuvvLLMdq1atYrtt98+brrppsW+H4ueQdayZcvYfffd44477ohhw4bFbrvttlRnF/Xo0SPq1asXb7/9dpn49OnTY/bs2Yvt809/+lNEROy6664RMb85W/TtzRb+7IxFbbHFFrHmmmvG0KFDyzz+E088EZ988knsueeeFd539913j4iIq666qkx8yJAhFd7n3Xffja233rrC2wEAKF9lj7f32GOPeP311+PNN9/MP8bkyZPLvaqjS5cuix1PX3/99Ut1xUajRo3KvOXSwsaNGxeffvrpzz7GHXfcEeuss06ceOKJi+V1xhlnROPGjfPrPuCAAyKlFBdeeOFij1N6TL7vvvtGQUFBXHTRRYtdNbHwcfvy5F2qsLBwsV7g6quvXuwxlqb/KXXkkUfG008/HUOGDIkWLVrkj7uXpG7durHFFlss1kdEzK/7oq677rqYPHly/gS4iFisj1hrrbUq3F9hYWHssssuMXz48Bg7dmw+PnHixLjzzjtj2223zb8N8qK22GKLaNWqVQwdOjTmzJmTj99yyy2LDYRKvfPOO7HRRhv97JUrAFXBFRsACxk/fnzccccdi8UbN24c++67b5nYFVdcEQ0bNiwTKygoiHPPPTf+9re/xaeffhr9+/ePJ598Mn9g+tRTT8Xw4cOjd+/eZc4eioi4/PLLY/To0fG73/0u3xw1a9Ysxo0bF/fdd198+umnceihh+a333DDDaN3795L/ADxhx9+OH788cfYZ599yr39l7/8ZbRq1SqGDRsWhxxySJx55plx//33x0EHHRS//e1vo0ePHjFlypR4+OGHY+jQobHpppvGUUcdFbfddlsMHDgw3nzzzdhuu+2iqKgo/vWvf0X//v3j17/+dTRt2jQOOuiguPrqqyOXy0WXLl3i0UcfjUmTJi3p21/GBhtsEF26dIkzzjgjxo8fH02aNIl//vOf5V7GfdVVV8W2224bm2++eRx//PHRuXPnGDt2bDz22GMxYsSIMtseddRRceCBB0ZExMUXX7xUa2nQoEHssssu8a9//SsuuuiifPzdd9+Nww47LA477LDo2rVrzJo1Kx588MF45ZVX4vjjj4/NN998qfNdWN26deMvf/lL9O3bN3r37h2HHXZYTJw4Ma688sro1KlTnHbaaRXet3v37nHYYYfFtddeG9OmTYutt946nn322Rg1alS527/zzjsxZcqU+PWvf71MawUAWJVV9nj7rLPOittvvz122223OPXUU6NRo0Zx/fXXR8eOHRd7K9t+/frFiSeeGAcccED86le/iv/85z/x1FNPLfWJOffcc08MHDgwttxyy2jcuHHsvffeEbHg7VmX9DZS33zzTTz33HPxu9/9rtzb69evH7vuumvcd999cdVVV8WOO+4YRx55ZFx11VUxcuTI2G233aKkpCReeuml2HHHHWPAgAHRtWvXOO+88+Liiy+O7bbbLvbff/+oX79+vPXWW9G2bdu45JJLljvvUnvttVfcfvvt0bRp0+jWrVu89tpr8a9//Sv/1lCllqb/KXX44YfHWWedFQ8++GCcdNJJFV69vqhf//rXcd5558X06dPLDBU6duwYhxxySPziF7+IBg0axMsvvxx33313dO/ePU444YSlznVRf/rTn+KZZ56JbbfdNvr37x916tSJv//97zF79uy47LLLKrxf3bp1409/+lOccMIJsdNOO8UhhxwSY8aMiZtvvrncz9iYO3duvPDCC9G/f/9lXivAckkAtdDNN9+cIiK99dZb5d7eu3fvtNFGG5WJdezYMUVEuf86duyY327w4MEVbldYWJjfbvbs2elvf/tb6tGjR2rUqFFq2LBh2nzzzdOQIUPSnDlzyl3XvHnz0g033JC222671LRp01S3bt3UsWPH1Ldv3/Tee++V2TYiUu/evZf4fdh7771TgwYNUlFRUYXbHHPMMalu3brpu+++Syml9P3336cBAwakdu3apXr16qX27duno48+On97SinNnDkznXfeealz586pbt26qU2bNunAAw9Mo0ePzm8zefLkdMABB6SGDRumZs2apRNOOCF9+OGHKSLSzTffnN/u6KOPTo0aNSp3bR9//HHq06dPaty4cWrZsmU67rjj0n/+85/FHiOllD788MO03377pTXWWCM1aNAgrb/++umPf/zjYo85e/bs1KxZs9S0adM0a9asJX7/FvbAAw+kXC6Xxo0bl4998cUX6aCDDkqdOnVKDRo0SA0bNkw9evRIQ4cOTSUlJUv1uEt6rt5zzz1ps802S/Xr10/NmzdPRxxxRPr666/LbFP6fFzYrFmz0u9+97vUokWL1KhRo7T33nunr776KkVEGjx4cJltzz777LT22msv9XoBAFhgWY6333///dS7d+/UoEGD1K5du3TxxRenG2+8MUVEGjNmTP5+xcXF6eyzz04tW7ZMDRs2TLvuumsaNWpU6tixYzr66KPz2z333HMpItJzzz2Xj82YMSMdfvjhaY011lisn+ndu/dix4+Luvzyy1NEpGeffbbCbW655ZYUEWn48OEppfm9zF//+te0wQYbpHr16qVWrVql3XffPb3zzjtl7nfTTTflj3GbNWuWevfunZ555plK572k4+ipU6emvn37ppYtW6bGjRunXXfdNX366aeLPUZKS9f/lNpjjz1SRKRXX311id+/hU2cODHVqVMn3X777WXi/fr1S926dUurr756qlu3buratWs6++yz0/Tp05fqcceMGZMiIv31r39d7LZ333037brrrqlx48apYcOGaccdd1xszeU9b1JK6dprr02dO3dO9evXT1tssUV68cUXU+/evRfrPZ944okUEWnkyJFLtV6AFS2XUjV+0hMA1LB58+ZF27ZtY++9944bb7xxqe9XXFwc3bp1i4MPPnipr/RY2c2ePTs6deoU55xzTpx66qk1vRwAAFip7bfffvHBBx9UeDV0RY499tj4/PPP46WXXqqilVW/fffdN3K5XLlv4wVQHXzGBgCrlIceeigmT55c4Qe7V6SwsDAuuuiiuOaaa2LGjBlVtLrqdfPNN0fdunXjxBNPrOmlAADASu3bb7+Nxx57LI488shK33fw4MHx1ltvxSuvvFIFK6t+n3zySTz66KO15oQvIJtcsQHAKuGNN96I999/Py6++OJo2bJlvPvuuzW9JAAAYCU3ZsyYeOWVV+KGG26It956K0aPHh1t2rSp6WUBrPJcsQHAKuG6666Lk046KdZcc8247bbbano5AABABrzwwgtx5JFHxpgxY+LWW2811ABYSVR6sPHiiy/G3nvvHW3bto1cLhcPPfTQz97n+eefj8033zzq168fXbt2jVtuuWUZlgoAy+6WW26JefPmxdtvvx0bb7xxTS8HgP/SXwCwMjvmmGMipRRffvllHHjggTW9HAD+q9KDjaKioth0003jmmuuWartx4wZE3vuuWfsuOOOMWLEiPj9738f/fr1i6eeeqrSiwUAAGoX/QUAAFBZy/UZG7lcLh588MHYd999K9zm7LPPjsceeyw+/PDDfOzQQw+NH374IZ588sly7zN79uyYPXt2/uuSkpKYMmVKtGjRInK53LIuFwAAWIFSSvHjjz9G27Zto6Bg+d/lVn8BAACrrsr0F3WqejGvvfZa9OnTp0xs1113jd///vcV3ueSSy6JCy+8sIpXBgAArAhfffVVtG/fvlr2pb8AAIDabWn6iyofbEyYMCFat25dJta6deuYPn16zJo1K1ZbbbXF7vOHP/whBg4cmP962rRpsfbaa8fYsWOjSZMmETH/bK6CgoIoKSmJhS86qSheUFAQuVyuwnhxcXGZNZROhEpKSpYqXlhYGCmlMvHStVQUX9q1y0lOcpKTnOQkJznJSU4rY07Tp0+PTp06xeqrrx7VRX+x6j7f5CQnOclJTnKSk5zkVLtzqkx/UeWDjWVRv379qF+//mLxZs2a5RsPAACgZhUWFkbE/KZkZaa/AACAlV9l+ovlfyPcn9GmTZuYOHFimdjEiROjSZMm5Z5NBQAAUBH9BQAAUOWDja222iqeffbZMrFnnnkmttpqq6reNQAAUMvoLwAAgEoPNmbMmBEjRoyIESNGRETEmDFjYsSIETFu3LiImP/+tUcddVR++xNPPDG++OKLOOuss+LTTz+Na6+9Nu6999447bTTVkwGAABAZukvAACAyqr0YOPtt9+OzTbbLDbbbLOIiBg4cGBsttlmMWjQoIiI+Pbbb/NNSERE586d47HHHotnnnkmNt1007j88svjhhtuiF133XUFpQAAAGSV/gIAAKisXFr4489XUtOnT4+mTZvGtGnTfLgfAACsJLJ6nJ7VdQMAQG1WmeP0Kv+MDQAAAAAAgBXFYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAIBlVFRUFLlcLnK5XBQVFdX0clYJdWp6AQAAAGRHUVFRNG7cOCIiZsyYEY0aNarhFQEAtc3B95xU00uolHk/zc3//5H3nxp1GtStwdUsm3sPua6ml1ApBhsAAAA1JGtNe4TGHQCAmmewAQAAAAAAy6hOg7px0N0n1vQyVikGGwAAACw1jTsAADXNh4cDAAAAAACZYbABAAAAq7CioqLI5XKRy+WiqKioppcDAPCzDDYAAAAAahHDqtpHTWsX9YTl5zM2AAAAYAU5+J6TanoJlTbvp7n5/z/y/lOjToO6NbiaZXPvIdfV9BIAgGpksAEAAABQAcOq6lfVgyo1rX5qWlbW6xlhoEzNM9gAAACAVVidBnXjoLtPrOllAAAsNYMNAAAAgFrEsKr2UdPaRT1h+fnwcAAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzFimwcY111wTnTp1igYNGkSvXr3izTffXOL2Q4YMifXXXz9WW2216NChQ5x22mnx008/LdOCAQCA2kV/AQAAVEalBxv33HNPDBw4MAYPHhzvvvtubLrpprHrrrvGpEmTyt3+zjvvjHPOOScGDx4cn3zySdx4441xzz33xLnnnrvciwcAALJNfwEAAFRWncre4Yorrojjjjsu+vbtGxERQ4cOjcceeyxuuummOOeccxbb/tVXX41tttkmDj/88IiI6NSpUxx22GHxxhtvVLiP2bNnx+zZs/NfT58+PSIiiouLo7i4OCIicrlcFBQURElJSaSU8ttWFC8oKIhcLldhvPRxF45HRJSUlCxVvLCwMFJKZeKla6kovrRrl5Oc5CQnOclJTnKSk5xWxpwW3f+yWNX7i/ztkSvzdUmkCuO5iMiVGy8bTRGRlhBf9LFTpEiVXMuKild3TimlKvsZKoicOtVATsXFxVX2WlcQOXWq5pwWfW1e0b9nS/etTtWX08Kvt1XxO3fh9ahT9eRU1cet6lQzOWWpv6jUYGPOnDnxzjvvxB/+8IcySfTp0ydee+21cu+z9dZbxx133BFvvvlm9OzZM7744ot4/PHH48gjj6xwP5dccklceOGFi8VHjx4djRs3joiIpk2bxlprrRUTJ06MadOm5bdp2bJltGzZMsaPHx9FRUX5eJs2bWKNNdaIsWPHxpw5c/Lx9u3bR+PGjWP06NFlvrGdO3eOOnXqxMiRI8usYd1114158+bFmDFjynwP1ltvvSgqKoqvv/46H69Xr16ss846MW3atJgwYUI+3qhRo+jQoUNMmTIlvvvuu3xcTnKSk5zkJCc5yUlOcspSTjNmzIjlob+Y31TWjbqxXp2187GSVBIfFX8RjXMNo3Nh23x8dpoTnxePizVyTaJ94Zr5+IySmTGm5JtoVdAsWhc0z8enlEyL8SWTo21By2he0DQfn1gyJSaVTImOBWtF44KG+fjXxZNiapoeXQs7RP1cvXx8TPE3MSPNjA0LO0dBbsFF/5/PGxdzY25sVKdLmZw+mjd6pc+pqKioyn6GuhZ2UKcayGnkyJFV9lq3UZ0u6lTNOZX+DFbV79nSNalT9eW08OtqVfzOXXj96lQ9OVX1cevCuapT9eQUEZnqL3Jp4RHaz/jmm2+iXbt28eqrr8ZWW22Vj5911lnxwgsvVHiW1FVXXRVnnHFGpJRi3rx5ceKJJ8Z1111X4X7KO6OqNMkmTZrMX3gtO9tNTnKSk5zkJCc5yUlOcspaTtOnT4/mzZvHtGnT8sfplaG/iDjs/gHzb8/AGXy16azEuw6+psp+ho64/xR1qoGchh14dZW91h1x/ynqVM05DTvw6oiout+zR9x/SrXntGi8NtSpMjnd+d+aRlTN79zSmlZnTrWxTpXJ6e6DrqnS49bD7j252nOqjXWqTE73HnJdpvqLSr8VVWU9//zz8T//8z9x7bXXRq9evWLUqFFx6qmnxsUXXxx//OMfy71P/fr1o379+ovFCwsLo7CwsEys9Ju4qMrGF33cZYnncrlKxVfU2uUkJznJaVnicpKTnOS0pLic5LQ0OVW0n6pUW/uL0sZ2aeKljeri8fKiFccrs8+qjld3Trnc/Ga+Kn6GSvelTtWb08I1WNGvdQvvR52qJ75oTVb079lF961OVZ9TVR4nFRYWlrsedaranBwbZaNOlV1LlvqLSg02WrZsGYWFhTFx4sQy8YkTJ0abNm3Kvc8f//jHOPLII6Nfv34REfGLX/wiioqK4vjjj4/zzjuvwic7AABQu+kvAACAZVGpo/569epFjx494tlnn83HSkpK4tlnny1z6fjCZs6cuVhzUTp5qcS7YAEAALWM/gIAAFgWlX4rqoEDB8bRRx8dW2yxRfTs2TOGDBkSRUVF0bdv34iIOOqoo6Jdu3ZxySWXRETE3nvvHVdccUVsttlm+UvF//jHP8bee+9dI5euAwAAKw/9BQAAUFmVHmwccsghMXny5Bg0aFBMmDAhunfvHk8++WS0bt06IiLGjRtX5gyq888/P3K5XJx//vkxfvz4aNWqVey9997x5z//ecVlAQAAZJL+AgAAqKxl+vDwAQMGxIABA8q97fnnny+7gzp1YvDgwTF48OBl2RUAAFDL6S8AAIDK8Ml6AAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbtVxRUVHkcrnI5XJRVFRU08sBAAAAAIDlYrABGWNYBQAAAACsygw2AABY5ThRAAAAILsMNgBqmD+u1T5qCgAAAFB16tT0ArJm79OH1/QSKmXe3J/y/3/gHx6NOnUb1OBqls0jl/+6ppcAAAAAAMBKwhUbAAAAAABAZrhig1Weq3CqX1VehZO1ekaoKZB9XntrhtdeAABgVWWwAcBKzR9Ma4Y/mAIAAAArK4ONWq5O3Qax18CHanoZAJBpRUVF0bhx44iImDFjRjRq1KiGVwQAAACrLoMNAKBauQqnZrgKpywnfwAAAGSXwQZADfPHNVj5+TkFAACAlYfBBmSMP64BAAAAAKsygw0AWMEMIAEAAACqTkFNLwAAAAAAAGBpGWwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmLNNg45prrolOnTpFgwYNolevXvHmm28ucfsffvghTj755FhrrbWifv36sd5668Xjjz++TAsGAABqF/0FAABQGXUqe4d77rknBg4cGEOHDo1evXrFkCFDYtddd43PPvss1lxzzcW2nzNnTvzqV7+KNddcM+6///5o165dfPnll7HGGmusiPUDAAAZpr8AAAAqq9KDjSuuuCKOO+646Nu3b0REDB06NB577LG46aab4pxzzlls+5tuuimmTJkSr776atStWzciIjp16rTEfcyePTtmz56d/3r69OkREVFcXBzFxcUREZHL5aKgoCBKSkoipZTftqJ4QUFB5HK5CuOlj7twPCKipKSk3DUW5Mp+XZIqjuciIrcU8RQRKc2PLbx5SvNvW/SxK4ovaS0rKl6dORUXF1e6ThXFCwsLI6VUJl6QU6fqzqm0jpWpU+nPdkXx0p/t0n2oU/XmtOhr88/VqTLxhferTtWX08Kvtyv6d6461UxOVXVsVBpXp5rJqSqOjSrzWr7o/peF/uK/t0fZIpdEqjCei4hcufGy0RQRaQnxRR87RZr/fKvEWlZUvLpzSilVXX8ROXWqgZxK+8aIFf9aVxA5darmnKqyv0hpQW7qVH05VWV/UVxcXGY96lQ9OVV5f6FONZJTlvqLSg025syZE++880784Q9/KJNEnz594rXXXiv3Pg8//HBstdVWcfLJJ8fw4cOjVatWcfjhh8fZZ58dhYWF5d7nkksuiQsvvHCx+OjRo6Nx48YREdG0adNYa621YuLEiTFt2rT8Ni1btoyWLVvG+PHjo6ioKB9v06ZNrLHGGjF27NiYM2dOPt6+ffto3LhxjB49usw3tnPnzlGnTp0YOXJkmTUU5CLq1Yno1nZBrLgk4j9fRTRpENG19YL4rDkRn3wb0bxxRMcWC+LTZ0WMmhTRpmnEWmssiH/3Y8S4KREdmkW0XH1B/NsfIr6dFrFOq4gmqy2If/l9xPczItZvE7FavQXxURMjpv8U8Yv2EYULvdnYx99EzJkX0X3tst/XEeNW7pxGjhxZ6Tqtu+66MW/evBgzZkw+VlBQEOutt14UFRXF119/nY+v30adqjun0npVpk716tWLddZZJ6ZNmxYTJkzIxxs1ahQdOnSIKVOmxHfffZffhzpVb06lNV3aOpVamtfyhdejTtWX08Kvqyv6d6461UxOVXVstO6660aDuupUEzlFRJUcG1XmtXzGjBmxPPQX85vKulE31quz4IlYkkrio+IvonGuYXQuXPBEnJ3mxOfF42KNXJNoX7jgapYZJTNjTMk30aqgWbQuaJ6PTymZFuNLJkfbgpbRvKBpPj6xZEpMKpkSHQvWisYFDfPxr4snxdQ0PboWdoj6uQVPuDHF38SMNDM2LOwcBbkFP0SfzxsXc2NubFSnS5mcPpo3eqXPqaioqMr6i66FHdSpBnIaOXJklb3WbVSnizpVc05V2V8UFRXl16RO1ZdTVfYXo0ePLrN+daqenKqyv5g3b16ZXNWpenKKyFZ/kUsLj9B+xjfffBPt2rWLV199Nbbaaqt8/KyzzooXXngh3njjjcXus8EGG8TYsWPjiCOOiP79+8eoUaOif//+8bvf/S4GDx5c7n7KO6OqNMkmTZrMX3gNnVG171mPzr+97FBrpTyDr7aclfjAX/aq0is29j/7UXWq5pwe+Mte87+ugunu/mc/WiM5LU98Za1TZeKlNa2KM6r2O+uRGslp0XhtqFNlcnrwsr3ysRX9O/fXZwyvkZxqY50qk9Pwv+5dZWdU7XPGw+pUAzk9cvmva/yMqunTp0fz5s1j2rRp+eP0ytBfRBx2/4D5t2fgDL7adFbiXQdfU2X9xRH3n6JONZDTsAOvrrLXuiPuP0WdqjmnYQdeHRFVd8XGEfefUu05LRqvDXWqTE53/remEVXzO7e0ptWZU22sU2Vyuvuga6r0io3D7j252nOqjXWqTE73HnJdpvqLSr8VVWWVlJTEmmuuGddff30UFhZGjx49Yvz48fHXv/61wsajfv36Ub9+/cXihYWFi52FVfpNXFRl4xWd3VVRvCSVGy43XtrALnX8vw3s8uyzquPVmdPCNahsncqL53K5MvHSfalT9eW0aF2Wpk4/F8+/oC6yb3WqnviiNfm5OlUmXt5+1anqc1pR9VveNVY2vqrVqTI5OTbKRp0qu5aqODb6ufjCz5mK9lOVam1/Ue4zovx4ivhvS7povLxoxfHK7LOq49WdU+6/E8cq6S/+uy91qt6cfq5vXJ7XuoX3o07VE6/K/qK8fatT1edUlf1FYWFhuetRp6rNybFRNupU2bVkqb+o1GCjZcuWUVhYGBMnTiwTnzhxYrRp06bc+6y11lpRt27dMovacMMNY8KECTFnzpyoV69eufcDAABqN/0FAACwLMofoVWgXr160aNHj3j22WfzsZKSknj22WfLXDq+sG222SZGjRpV5jKTzz//PNZaay1NBwAArML0FwAAwLKo1GAjImLgwIHxj3/8I2699db45JNP4qSTToqioqLo27dvREQcddRRZT7876STToopU6bEqaeeGp9//nk89thj8T//8z9x8sknV7QLAABgFaG/AAAAKqvSn7FxyCGHxOTJk2PQoEExYcKE6N69ezz55JPRunXriIgYN25cmffF6tChQzz11FNx2mmnxSabbBLt2rWLU089Nc4+++wVlwUAAJBJ+gsAAKCylunDwwcMGBADBgwo97bnn39+sdhWW20Vr7/++rLsCgAAqOX0FwAAQGVU+q2oAAAAAAAAaorBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBnLNNi45pprolOnTtGgQYPo1atXvPnmm0t1v7vvvjtyuVzsu+++y7JbAACgltJjAAAAS6vSg4177rknBg4cGIMHD4533303Nt1009h1111j0qRJS7zf2LFj44wzzojttttumRcLAADUPnoMAACgMio92LjiiiviuOOOi759+0a3bt1i6NCh0bBhw7jpppsqvE9xcXEcccQRceGFF8Y666yzXAsGAABqFz0GAABQGXUqs/GcOXPinXfeiT/84Q/5WEFBQfTp0ydee+21Cu930UUXxZprrhnHHntsvPTSSz+7n9mzZ8fs2bPzX0+fPj0i5jcvxcXFERGRy+WioKAgSkpKIqWU37aieEFBQeRyuQrjpY+7cDwioqSkpNw1FuTKfl2SKo7nIiK3FPEUESnNjy28eUrzb1v0sSuKL2ktKypenTkVFxdXuk4VxQsLCyOlVCZekFOn6s6ptI6VqVPpz3ZF8dKf7dJ9qFP15rToa/PP1aky8YX3q07Vl9PCr7cr+neuOtVMTlV1bFQaV6eayakqjo0q81q+6P6XRXX0GJnoL6JskUsiVRjPRUSu3HjZaIqItIT4oo+dIs1/vlViLSsqXt05pZSqrr+InDrVQE6lfWPEin+tK4icOlVzTlXZX6S0IDd1qr6cqrK/KC4uLrMedaqenKq8v1CnGskpS/1FpQYb3333XRQXF0fr1q3LxFu3bh2ffvppufd5+eWX48Ybb4wRI0Ys9X4uueSSuPDCCxeLjx49Oho3bhwREU2bNo211lorJk6cGNOmTctv07Jly2jZsmWMHz8+ioqK8vE2bdrEGmusEWPHjo05c+bk4+3bt4/GjRvH6NGjy3xjO3fuHHXq1ImRI0eWWUNBLqJenYhubRfEiksi/vNVRJMGEV0X+tbMmhPxybcRzRtHdGyxID59VsSoSRFtmkastcaC+Hc/RoybEtGhWUTL1RfEv/0h4ttpEeu0imiy2oL4l99HfD8jYv02EavVWxAfNTFi+k8Rv2gfUbjQNTkffxMxZ15E97XLfl9HjFu5cxo5cmSl67TuuuvGvHnzYsyYMflYQUFBrLfeelFUVBRff/11Pr5+G3Wq7pxK61WZOtWrVy/WWWedmDZtWkyYMCEfb9SoUXTo0CGmTJkS3333XX4f6lS9OZXWdGnrVGppXssXXo86VV9OC7+urujfuepUMzlV1bHRuuuuGw3qqlNN5BQRVXJsVJnX8hkzZsTyqo4eY6XuLyIXdaNurFdnwROxJJXER8VfRONcw+hcuOCJODvNic+Lx8UauSbRvnDNfHxGycwYU/JNtCpoFq0LmufjU0qmxfiSydG2oGU0L2iaj08smRKTSqZEx4K1onFBw3z86+JJMTVNj66FHaJ+bsETbkzxNzEjzYwNCztHQW7BD9Hn88bF3JgbG9XpUianj+aNXulzKioqqrL+omthB3WqgZxGjhxZZa91G9Xpok7VnFNV9hdFRUX5NalT9eVUlf3F6NGjy6xfnaonp6rsL+bNm1cmV3WqnpwistVf5NLCI7Sf8c0330S7du3i1Vdfja222iofP+uss+KFF16IN954o8z2P/74Y2yyySZx7bXXxu677x4REcccc0z88MMP8dBDD1W4n/LOqCpNskmTJvMXXkNnVO171qPzby871Fopz+CrLWclPvCXvar0io39z35Unao5pwf+stf8r6tgurv/2Y/WSE7LE19Z61SZeGlNq+KMqv3OeqRGclo0XhvqVJmcHrxsr3xsRf/O/fUZw2skp9pYp8rkNPyve1fZGVX7nPGwOtVATo9c/usaP6Nq+vTp0bx585g2bVr+OL2yqqPHWJn7i8PuHzD/9gycwVebzkq86+Brqqy/OOL+U9SpBnIaduDVVfZad8T9p6hTNec07MCrI6Lqrtg44v5Tqj2nReO1oU6VyenO/9Y0omp+55bWtDpzqo11qkxOdx90TZVesXHYvSdXe061sU6VyeneQ67LVH9RqSs2WrZsGYWFhTFx4sQy8YkTJ0abNm0W23706NExduzY2HvvvfOx0sXXqVMnPvvss+jSpcti96tfv37Ur19/sXhhYWEUFhaWiZV+ExdV2fiij/tz8ZJUbrjceGkDu9Tx/zawy7PPqo5XZ04L16CydSovnsvlysRL96VO1ZfTonVZmjr9XDz/grrIvtWpeuKL1uTn6lSZeHn7Vaeqz2lF1W9511jZ+KpWp8rk5NgoG3Wq7Fqq4tjo5+ILP2cq2k9lVEePkYn+otxnRPnxFPHflnTReHnRiuOV2WdVx6s7p9x/J45V0l/8d1/qVL05/VzfuDyvdQvvR52qJ16V/UV5+1anqs+pKvuLwsLCctejTlWbk2OjbNSpsmvJUn9RqQ8Pr1evXvTo0SOeffbZfKykpCSeffbZMmdXldpggw3igw8+iBEjRuT/7bPPPrHjjjvGiBEjokOHDpXZPQAAUMvoMQAAgMqq1BUbEREDBw6Mo48+OrbYYovo2bNnDBkyJIqKiqJv374REXHUUUdFu3bt4pJLLokGDRrExhtvXOb+a6yxRkTEYnEAAGDVpMcAAAAqo9KDjUMOOSQmT54cgwYNigkTJkT37t3jySefzH/Y37hx4yq85AgAAGBRegwAAKAyKj3YiIgYMGBADBgwoNzbnn/++SXe95ZbblmWXQIAALWYHgMAAFhaTnsCAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIDIMNAAAAAAAgMww2AAAAAACAzDDYAAAAAAAAMsNgAwAAAAAAyAyDDQAAAAAAIDMMNgAAAAAAgMww2AAAAAAAADLDYAMAAAAAAMgMgw0AAAAAACAzDDYAAAAAAIDMMNgAAAAAAAAyw2ADAAAAAADIjGUabFxzzTXRqVOnaNCgQfTq1SvefPPNCrf9xz/+Edttt100a9YsmjVrFn369Fni9gAAwKpHjwEAACytSg827rnnnhg4cGAMHjw43n333dh0001j1113jUmTJpW7/fPPPx+HHXZYPPfcc/Haa69Fhw4dYpdddonx48cv9+IBAIDs02MAAACVUenBxhVXXBHHHXdc9O3bN7p16xZDhw6Nhg0bxk033VTu9sOGDYv+/ftH9+7dY4MNNogbbrghSkpK4tlnn13uxQMAANmnxwAAACqjTmU2njNnTrzzzjvxhz/8IR8rKCiIPn36xGuvvbZUjzFz5syYO3duNG/evMJtZs+eHbNnz85/PX369IiIKC4ujuLi4oiIyOVyUVBQECUlJZFSym9bUbygoCByuVyF8dLHXTgeEVFSUlLuGgtyZb8uSRXHcxGRW4p4ioiU5scW3jyl+bct+tgVxZe0lhUVr86ciouLK12niuKFhYWRUioTL8ipU3XnVFrHytSp9Ge7onjpz3bpPtSpenNa9LX55+pUmfjC+1Wn6stp4dfbFf07V51qJqeqOjYqjatTzeRUFcdGlXktX3T/y6I6eoxM9BdRtsglkSqM5yIiV268bDRFRFpCfNHHTpHmP98qsZYVFa/unFJKVddfRE6daiCn0r4xYsW/1hVETp2qOaeq7C9SWpCbOlVfTlXZXxQXF5dZjzpVT05V3l+oU43klKX+olKDje+++y6Ki4ujdevWZeKtW7eOTz/9dKke4+yzz462bdtGnz59KtzmkksuiQsvvHCx+OjRo6Nx48YREdG0adNYa621YuLEiTFt2rT8Ni1btoyWLVvG+PHjo6ioKB9v06ZNrLHGGjF27NiYM2dOPt6+ffto3LhxjB49usw3tnPnzlGnTp0YOXJkmTUU5CLq1Yno1nZBrLgk4j9fRTRpENF1oW/NrDkRn3wb0bxxRMcWC+LTZ0WMmhTRpmnEWmssiH/3Y8S4KREdmkW0XH1B/NsfIr6dFrFOq4gmqy2If/l9xPczItZvE7FavQXxURMjpv8U8Yv2EYULXZPz8TcRc+ZFdF+77Pd1xLiVO6eRI0dWuk7rrrtuzJs3L8aMGZOPFRQUxHrrrRdFRUXx9ddf5+Prt1Gn6s6ptF6VqVO9evVinXXWiWnTpsWECRPy8UaNGkWHDh1iypQp8d133+X3oU7Vm1NpTZe2TqWW5rV84fWoU/XltPDr6or+natONZNTVR0brbvuutGgrjrVRE4RUSXHRpV5LZ8xY0Ysr+roMVbq/iJyUTfqxnp1FjwRS1JJfFT8RTTONYzOhQueiLPTnPi8eFyskWsS7QvXzMdnlMyMMSXfRKuCZtG6YMFwZ0rJtBhfMjnaFrSM5gVN8/GJJVNiUsmU6FiwVjQuaJiPf108Kaam6dG1sEPUzy14wo0p/iZmpJmxYWHnKMgt+CH6fN64mBtzY6M6Xcrk9NG80St9TkVFRVXWX3Qt7KBONZDTyJEjq+y1bqM6XdSpmnOqyv6iqKgovyZ1qr6cqrK/GD16dJn1q1P15FSV/cW8efPK5KpO1ZNTRLb6i1xaeIT2M7755pto165dvPrqq7HVVlvl42eddVa88MIL8cYbbyzx/pdeemlcdtll8fzzz8cmm2xS4XblnVFVmmSTJk3mL7yGzqja96xH599edqi1Up7BV1vOSnzgL3tV6RUb+5/9qDpVc04P/GWv+V9XwXR3/7MfrZGclie+stapMvHSmlbFGVX7nfVIjeS0aLw21KkyOT142V752Ir+nfvrM4bXSE61sU6VyWn4X/eusjOq9jnjYXWqgZweufzXNX5G1fTp06N58+Yxbdq0/HF6ZVVHj7Ey9xeH3T9g/u0ZOIOvNp2VeNfB11RZf3HE/aeoUw3kNOzAq6vste6I+09Rp2rOadiBV0dE1V2xccT9p1R7TovGa0OdKpPTnf+taUTV/M4trWl15lQb61SZnO4+6JoqvWLjsHtPrvacamOdKpPTvYdcl6n+olJXbLRs2TIKCwtj4sSJZeITJ06MNm3aLPG+//u//xuXXnpp/Otf/1riUCMion79+lG/fv3F4oWFhVFYWFgmVvpNXFRl44s+7s/FS1K54XLjpQ3sUsf/28Auzz6rOl6dOS1cg8rWqbx4LpcrEy/dlzpVX06L1mVp6vRz8fwL6iL7VqfqiS9ak5+rU2Xi5e1Xnao+pxVVv+VdY2Xjq1qdKpOTY6Ns1Kmya6mKY6Ofiy/8nKloP5VRHT1GJvqLcp8R5cdTxH9b0kXj5UUrjldmn1Udr+6ccv+dOFZJf/HffalT9eb0c33j8rzWLbwfdaqeeFX2F+XtW52qPqeq7C8KCwvLXY86VW1Ojo2yUafKriVL/UWlPjy8Xr160aNHjzIfylf6IX0Ln121qMsuuywuvvjiePLJJ2OLLbaozC4BAIBaTI8BAABUVqWu2IiIGDhwYBx99NGxxRZbRM+ePWPIkCFRVFQUffv2jYiIo446Ktq1axeXXHJJRET85S9/iUGDBsWdd94ZnTp1yr+PVuPGjfPvZwsAAKy69BgAAEBlVHqwccghh8TkyZNj0KBBMWHChOjevXs8+eST+Q/7GzduXJnLR6677rqYM2dOHHjggWUeZ/DgwXHBBRcs3+oBAIDM02MAAACVUenBRkTEgAEDYsCAAeXe9vzzz5f5euzYscuyCwAAYBWixwAAAJZWpT5jAwAAAAAAoCYZbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJlhsAEAAAAAAGSGwQYAAAAAAJAZBhsAAAAAAEBmGGwAAAAAAACZYbABAAAAAABkhsEGAAAAAACQGQYbAAAAAABAZhhsAAAAAAAAmWGwAQAAAAAAZIbBBgAAAAAAkBkGGwAAAAAAQGYYbAAAAAAAAJmxTIONa665Jjp16hQNGjSIXr16xZtvvrnE7e+7777YYIMNokGDBvGLX/wiHn/88WVaLAAAUDvpMQAAgKVV6cHGPffcEwMHDozBgwfHu+++G5tuumnsuuuuMWnSpHK3f/XVV+Owww6LY489Nt57773Yd999Y999940PP/xwuRcPAABknx4DAACojEoPNq644oo47rjjom/fvtGtW7cYOnRoNGzYMG666aZyt7/yyitjt912izPPPDM23HDDuPjii2PzzTeP//u//1vuxQMAANmnxwAAACqjTmU2njNnTrzzzjvxhz/8IR8rKCiIPn36xGuvvVbufV577bUYOHBgmdiuu+4aDz30UIX7mT17dsyePTv/9bRp0yIiYurUqVFcXBwREblcLgoKCqKkpCRSSvltK4oXFBRELperMF76uAvHIyJKSkrKxOfOnjn/9lzZNZf89yHLi+ciIrcU8RQRKc2PLbx5SvNvW/SxK4ovaS0rKl6dOU2dOrXSdaooXlhYGCmlMvHiOTPVqZpzmjp16vyvK1Gn0p/tiuKlP9vFc2bWSE7LE19Z61SZeGlNl7ZOlYmX1rS6c1o0XhvqVJmcSmsaUQW/cxeqaXXmVBvrVJmcfvjhhyo5NiooKIi5s2eqUw3kNH369Co5NqrMa/n06dP/u84Uy6o6eoyVur+YOWf+7VG2yCWRKoznIiJXbrxsNEVEWkJ80cdOkeY/3yqxlhUVr+6cpk2bVnX9xcy56lQDOZX2jREr/rWueOZcdarmnKqyv0gpRfHMudWe06Lx2lCnyuRUpf1FcXG+ptWZU22sU2Vyqsr+IiLK1LS6cqqNdapMTlnrLyo12Pjuu++iuLg4WrduXSbeunXr+PTTT8u9z4QJE8rdfsKECRXu55JLLokLL7xwsXinTp0qs1xqieZOvKt11LT2UdPap/k1Nb0CVrRmalrrNF2Javrjjz9G06ZNl+m+1dFj6C9Y1AO/vbGml8AKpqa1i3rWPg+qaa3z0G/Lv7KW7Gq6EtV0afqLSg02qssf/vCHMmdglZSUxJQpU6JFixaRW/RUOH7W9OnTo0OHDvHVV19FkyZNano5rABqWvuoae2jprWPmtY+arr8Ukrx448/Rtu2bWt6KUukv1jx/PzUPmpa+6hp7aOmtYt61j5quvwq019UarDRsmXLKCwsjIkTJ5aJT5w4Mdq0aVPufdq0aVOp7SMi6tevH/Xr1y8TW2ONNSqzVMrRpEkTP1S1jJrWPmpa+6hp7aOmtY+aLp9lvVKjVHX0GPqLquPnp/ZR09pHTWsfNa1d1LP2UdPls7T9RaU+PLxevXrRo0ePePbZZ/OxkpKSePbZZ2OrrbYq9z5bbbVVme0jIp555pkKtwcAAFYdegwAAKCyKv1WVAMHDoyjjz46tthii+jZs2cMGTIkioqKom/fvhERcdRRR0W7du3ikksuiYiIU089NXr37h2XX3557LnnnnH33XfH22+/Hddff/2KzQQAAMgkPQYAAFAZlR5sHHLIITF58uQYNGhQTJgwIbp37x5PPvlk/sP7xo0bl/9U9IiIrbfeOu688844//zz49xzz4111103Hnroodh4441XXBYsUf369WPw4MGLXX5Pdqlp7aOmtY+a1j5qWvuo6cpDj5E9fn5qHzWtfdS09lHT2kU9ax81rV65lFKq6UUAAAAAAAAsjUp9xgYAAAAAAEBNMtgAAAAAAAAyw2ADAAAAAADIDIMNAACqlY94AwAAViQ9xqrHYANquR9//LGmlwBkSOnB4NSpU+Pjjz+u4dVQ2/z000/x1ltvRS6XW2Uaj+Li4kgprTL5ArWf/gKoLD0GVWlV6zH0FwsYbJA3fvz4+Otf/xqfffZZRJh0Zllp7X7729/GvvvuGz/99FMNr4jl8d1338VNN90U/fr1i8suuyz/Mwor0rx58+LFF1/Mv17ccsstcdRRR8XLL79cwyujNnn55Zfj17/+dTzxxBORy+VqejlV6s4774x99tknPvnkk8jlcrU+X6iIHqN20F/ULvoLqoseg+qwqvQY+ovFGWwQEfN/2fTv3z/OPvvs+Pvf/x4Rmo4sGTFiRJx33nnxr3/9KyIi5s6dGxERW2+9dXz88ccxZcqUmlwey+j111+Prl27RuvWreOqq66KmTNnxs033xy77LJLjBgxIiL8nLLiDB8+PA444IB46qmnIiLitNNOi+233z7+8pe/xFtvvRURnm8svz59+sRf//rXOOWUU+LTTz+t6eWsMCUlJTFv3rwoKSnJ/5y0b98+vvrqq3j++efjyiuvjGOPPTbmzZtXwyuF6qXHyC79Re2kv6C66TGoDrWxx9BfLB2DjVXMJ598Escee2xcd911ETH/ByUi4ptvvolZs2bFxhtvnP+FQzb8+OOPce2118Yll1wSAwYMiA8++CDq1asXERF77LFHTJw40Rk4GVP6S2u11VaLb7/9Nm644YYYMWJE3HnnnfHII4/E999/HzfccEOUlJSY0LPc5syZExER66+/fmy66ab5BiMi4ne/+120aNEiBg8eXFPLYyVWXFxc4W2lB+LlOeKII2K99daLCy+8MMaPH19Vy6tWBQUFUadOnSgoKMj/8e+ZZ56JDz74IAYOHBi33XZbtG/fPn/cBbWNHqN20V/UPvoLqpseg2Wlx5hPf7F0DDZquZkzZ+b/f968efHQQw/FzTffHFdffXU888wzUVAw/ynQokWLeOONN+Kkk06KTz75JH744Yf8bawcXnvttdh///3jkUceiYgFB6err756bLfddtG8efNo06ZN/Pa3v41Ro0ZFRETbtm1j7bXXjhdffNFZECuZmTNnxty5c+PCCy+MM844IyIW1LS0mdh0002jU6dO8eWXX8bMmTNjzpw50bVr19hkk01i7NixMW3atBpbP7VH6R8qOnXqFBtuuGG88sor+ds6deoUxx13XDzzzDPx3nvvaXQpo7CwMP//ix5Qlx6IR0RMnz49Hy9tRE4//fT48ssv4/7776+Gla4YxcXFFTZab775ZpxyyinRs2fP2G+//WLevHmx1VZbxbbbbht77713vPTSS3HhhRfmf94g6/QYtYP+onbRX7Ay0WOwrFalHkN/sfwcVdYyY8eOjf/93/+N3r17R7du3eKEE06Il156KSIi6tSpE9tss000aNAgtthii7jsssvim2++iYiIcePGxfrrrx9rrbVWdOjQIR599NGIcElgTSkpKYl77723zJlt1113XYwePTrOPffc+Pbbb8v88u/atWs0atQojj322OjYsWMcd9xx8fbbb0dExI477hjPPvtszJo1q9rzoKySkpK4++67Y+21147jjz8+6tatG6NHj4733nsvfvzxxzI1Lf3l1r1793jvvfdi9uzZUa9evSgqKoqpU6dGmzZtolmzZjWVChlSelZLea/ns2bNin/84x+x0047xemnnx4ppfjmm2/i22+/zd93m222iQ033DBuvfXWVf4yV8oaMmRI/PGPf4yIyP+hsvR59uabb8a+++4brVu3jn333Tf+9re/RcSCRuWXv/xldOvWLZ544okaWPmyKSwsLNNolXrssceiX79+MW7cuDj++ONjzz33jK+++ir22GOP2HfffWPixInxzjvvRMTizRlkhR4j+/QXtZP+gpqix6CqrEo9hv5i+RlsZNy0adPi1VdfjZRSbLHFFrHOOuvEvffeGzvvvHNceuml8dVXX8Xxxx8fY8aMiYj5k/HmzZvH1ltvHfXr149LL700IiI+/vjjaNu2bay//vrRrVu3ePDBByPCD0h1K32xfuqpp+KEE06ImTNn5mswatSoOP/886N79+5x8cUXx6RJk/L369ixY3Tq1Cnef//9uOmmm6JRo0Zx/PHHR1FRURx22GHx9ttv5w8iqD5fffVVvqYppSgoKIi77rorxo8fH9999118//33seOOO8aMGTPiP//5T0Qs/jO38847x6hRo+Kxxx6Lyy+/PLp37x6zZ8+OY489ttrzIZtKz2op70yom2++OS677LLYeuutY5NNNomXX345Ro8enb9UvLTJKH1f3B9++KE6l85KJKW0WOM6d+7cuP322+OHH36I4cOHx0033RS5XC4mT54cZ511VjRu3Dhuuumm6NOnT5x++ulx+eWX559TjRo1ii222CLGjx8fH374YU2ktJiSkpIKj3t++OGH+Mc//hEHHnhgnHvuufHxxx/nb7vuuutigw02iOHDh0e/fv2if//+0blz54iI6NmzZ8ycOTM++eSTiAhnJJIZeozaQ39Ru+gvWFnoMVgRanuPob+oegYbGfT0009Hv379Yu21145mzZrFpZdeGrlcLho2bBhHH310vPjiizFo0KDYZ5994qKLLoqUUv4XSKtWraJ79+7x5ptvxsUXXxyfffZZ/N///V+stdZa8cEHH8SGG24Y2267bbzwwgsRES4VrwbDhw+PHXfcMT766KP8i/Xvfve7GDBgQOy3335RUFAQM2bMiDXXXDPmzZsX559/fkyaNCnfMEZEvpEcPnx4NGnSJG644YbI5XKxzz77xOabbx7FxcXxwQcf1GCWq5bHHnssGjVqFNtuu22+oSj9Rdu0adPYcMMNo06dOvHuu+/GFltsEblcrsx7jkYs+Nnbdddd49tvv43+/fvH448/Hv3794/33nsvttpqq+pNiswpPUB8+eWX48QTT4xddtklbrjhhvjqq68iImLKlClx++23R+/eveNPf/pTnHzyyXHttdfGOuusE08//XRELDhI2m233WLMmDHOploFlR6I53K5MgfNJSUl8frrr8e3334bzZs3jwEDBsSECRMiYv7vtZEjR8Y555wTe+65Z5x77rlxwQUXxF133VXmtW6dddaJ1VdfPT766KPqTWoRpT8rBQUF5R73TJkyJU488cS44oorol27dvHyyy/Hfvvtl3/rljZt2sTIkSPj/vvvjxtvvDFefPHF/B97u3fvHo0bN47PP//c+5az0tNj1B76i9pHf8HKQo/BilDbewz9RfVxRJkhjz76aDRr1iwOOOCAmD17dvzlL3+JiRMnxsMPPxwRETvttFN8+umnZc60+fTTT6Ndu3bRp0+fiJj/YWG77bZbPProo7HZZpvFSSedFBdccEE8/vjjsdFGG0VKKbbddtuYMmVKjBkzZpX/AakOHTt2jKKiovyL7vnnnx+NGzeO8847L7/Nd999FxMmTIi2bdtG165dY8CAATFs2LB4//33IyLyl/5PmDAhvv7662jTpk3cfffd8cUXX8Sf//znaNy4cbzzzjv5DxyianXo0CHWW2+9mDBhQlx11VXx/fffR926dePLL7+MOnXqxO677x6zZs2Kt99+O9Zbb71Yc8018w1K6S+9XC4XKaVo165ddOnSJX7zm9/EY489Fqeddlo0bdrUWzjUcimlxc7uOPvss+PUU0/NfxBfRQ1A6YFfLpeLBx54IPr16xczZsyIbbfdNv7+97/Hb37zm5g9e3ZMnjw5pkyZEnvssUf+vptuumnstttu+beaqFu3bkTM/4NVSUlJzJgxo0rypfos6ayhiPlvU7Hw7aWvSSNHjox77rknRo8eHSUlJVFQUBCrr756FBQU5M/ePvfccyMi4pVXXonNNtss1l133fzj7LbbbrHGGmvk/6gZMf992ktKSqrt9WzcuHHx5ZdflomllPLHOq+88kpceuml8fjjj8fs2bPz2wwfPjweffTRGD58eFx55ZVx5513xvbbbx+nn356REQMGjQomjZtGpdcckk88cQTceyxx8ZGG20UDz/8cDRs2DB+8YtfxPvvvx/jxo2LiPC7mJWOHqP20V/UPvoLVgQ9BlVlVe0x9BcrgURmjBgxIm2++ebpnnvuKff2l156KXXo0CFddNFF6ZRTTknrrLNOyuVyaauttkq33HJLfru33347FRQUpBEjRqSUUjr00ENTLpdLZ555ZkoppXHjxqV11lknXXnllSmllIqLi6s4s1VbUVFR2nnnndM555yTXn/99dSsWbP0yiuvpJRSKikpSSmlNGvWrNSsWbM0Z86c/P023XTT1KdPnzR69OiUUkrvvvtu6tSpU7rhhhvy2zz99NNpr732SrlcLm299dZp4sSJ1ZjZqmvGjBlpv/32S/vss0868MAD0xlnnJFSSmnq1KmpQ4cO6fPPP08HHnhgOvTQQ1NKKZ111llphx12SOPHj08pLaj7vHnzUkopnXzyyWnnnXdOo0aNSin5mazNSmu/sClTpqTp06enBx98MP+6XZ7i4uJ09NFHpx133DGllNJXX32VevTokf7xj3/kt/n2229TixYt0p///Oc0b9681KxZs3TbbbeluXPn5re56KKLUocOHdKnn36aj73zzjupV69e6emnn14RaVLNynte/Zwff/wxpZTShx9+mLbbbrvUqFGjtOGGG6auXbuma6+9Nr/dr3/963TggQemlFL+eXT55ZenLl26pGnTpuX3PXXq1NSnT580cODA/H3nzJmT1lhjjfTSSy8tc25LMn369HTjjTemXXfdNTVr1iytueaaaeutt05/+ctfymz37bffpj59+qQ111wzbbfddmnddddNv/rVr1JK81+H+/fvn/baa68y93nllVdS3bp10wcffJCPzZ07N33++ecppZT23HPPtNtuu6WUUnrkkUdSz54903bbbZe6dOmS9ttvvyrJF5aVHqP20V/UPvoLloceg6qwKvYY+ouVjys2MmTdddeNFi1a5D8gptR//vOfeO2112LbbbeN+vXrx+WXXx4TJkyIQYMGxdtvvx3bb799nHjiiTFkyJCImP9BcJ07d85f4nTxxRfHzTffHCeddFJERDRr1iy6du0at99+e0R4v7aq1rBhw1hnnXXivffei7POOiv69+8fPXv2jIgFl6+NGDEiWrZsGUceeWT07NkzGjRoED/88EN8/vnn8ec//zki5r+38WabbRZPPvlkRMyfiP/qV7+Kiy++ONZee+3o2rVr/swIqlajRo2iZcuWkVKKE088MW677bZ49dVXo7CwMOrUqRP16tWLLbbYIsaNGxfjxo2Lnj17xowZM2LEiBFlHqf0Q6R23333GDVqVIwcOTIi/EzWRqVnWJTW9rnnnosTTjghunbtGi1atIjLLrss9t1339h0003zZ7o8/PDDseWWW8bzzz8fEfPfc/nxxx+P888/PyLmv2fnmDFjYqeddoorrrgitt1229h4442jQYMGsfrqq0dhYWF069YtnnvuuZg5c2Z+LdOnT49vvvmmzOW806ZNi1wuF127dq2ObwcrWOnz6t13342zzz47ttlmm9hhhx3i2muvjalTp0bE/N8ZkyZNikcffTTat28f/fv3j0mTJsU111wTuVwuvvjii3j99ddjv/32iyFDhsTNN98cEfPfp/vll1+OH374IerUqRMREXvssUeMGTMm3n777fy+69SpE2+88Ub88pe/jIj5v9/mzZsXLVu2rJK3pJk4cWIceeSR0a9fv9hmm23in//8Z7z66qux8847x6BBg8o8v4cOHRpff/11vP766/Hiiy/GHXfcEW+99Vb86U9/isLCwpg6dWo0bdq0zNnqa6+9dnTu3DmeffbZiIiYPXt2/PTTT7HuuuvG7NmzY968edGuXbuImP8aftVVV8WWW24ZZ555Zlx//fUrPF9YHnqM2kd/UfvoL1gWegyq0qrWY+gvVlI1OVWh8o477ri08847pyFDhqTdd989tWrVKq2++urpf/7nf1JKKe2///7pgAMOSN9//32Z+/3mN79JW265ZZo6dWoqKSlJ/fr1S9tvv32F+/n000/zZ3dQ9e69997Utm3blMvlUp8+fdKrr75a5vYHH3wwNWrUKO21117pggsuSK+++mqaN29eeuKJJ1LHjh3T9ddfn1JK6dxzz0116tSpiRRYxF133ZV69uyZvvrqq3TmmWemY445Jp1//vnpiCOOSOPHj09PPPFE6tGjR3rwwQfTmDFj0jbbbJMuvvjilNL8sxg+/PDDNHTo0HT33XenoqKitPbaa6cHHnighrNiRfv888/TkUcemZ555pmUUkoffPBB6t69e8rlcuk3v/lNuvXWW9O3336bUkrphx9+SIcffni66KKLUkopffnll+mkk05KjRs3Tvfff3/629/+ljbccMM0bdq0lFJKL774Ymrbtm2qX79+2mGHHdJFF12U3njjjTRr1qz8/v/xj3+kX/ziF+m0005LM2fOTG+99VbaY489Uvv27dPuu++e3+5f//pX6tKlS3V9W1jB5syZk3r16pVyuVzafffd0//93/+lE044IeVyufT73/8+pZTSkUcemdq3b59OOumkdO2116bvv/8+jRgxInXr1q3M2VOzZs1Khx56aNpnn31SSvOPFwoKCtKbb76ZUlpwJujOO++cNttss3TfffelyZMnp/POOy917949ffbZZ/nHeuSRR9Iuu+ySxowZs8y5vfXWW+mPf/xjOvzww9PFF1+cnnvuuZTS/LOVL7jggrTeeuuV2b64uDjlcrn0wgsv5GMbbrhh/vW39IywU045Je20007p+++/T0OGDElbbbVVev311/P3GTlyZNpss83S0KFDU0lJSbr11ltT//7907777pvatGmTevfunT+7CrJAj1H76C9qH/0FS0uPQXWorT2G/iJbDDYy5t57703NmzdP66+/frrgggvSv//97/TDDz/kb7/yyivTJptskt54442U0vxLw2bPnp0OOeSQ1Llz53zsxhtvTLlcrsylx9ScL774Iu2www7pkEMOSf369Uv16tVLxx57bP4y8Pvuuy81b9683PuedtppqUmTJmnKlClp1KhR6eWXX67OpVOBL774IvXs2TPdfvvtadq0aenMM8/Mv21DSimNGTMm9e7dO5133nmppKQkHXLIIWnzzTdPxxxzTOrUqVMqKChIbdq0SUOGDEkpLdtlnqy8Suv56KOPpm7duuXjkyZNSnvuuWfq169fuffbaaed0lFHHVXmdf+Pf/xj2nLLLVP9+vXT3/72t3z8nXfeST179kynn356mceYO3duev/999PUqVPTnDlz0tChQ1OnTp1Shw4dUosWLdKFF16YXnjhhfTuu+/m7/Pggw+mf/zjH2UuJ2fl8nNvIXHggQemvffeO//17Nmz04UXXpjatGmTvvjii/TQQw+lXC6Xjj322Pw2U6dOTXXq1En//ve/yzzWZZddlv/DSkopderUKV144YVlthkzZkzq27dv6tatW2rYsGHq3Llzuu+++1JKC57/r7zySpnLxivjjTfeSNtvv31q2bJl2nPPPdNf//rXdN5556Xf/OY3+e/F8OHD0+qrr56/5H3evHnpggsuSHvttVf+rVPmzJmTevTokc4+++yU0vyGJaWUHnroobTBBhukt99+O3322WepV69e6cADD8w39ddcc03+e5dSSq+++mo64YQT0jnnnFOmqYGs0GPUPvqL2kd/wc/RY7CirUo9hv4imww2MuaLL75IvXr1SkOHDi0TL51efvzxx6l9+/Zp+PDhKaWUZs6cmW6//fa03nrrpauvvjq//aRJk9KLL75YfQtniebMmZP22GOPdMopp6SUUnr44YfTeuutl1q3bp0uu+yydP7556df/vKX+RfPlBa8aE+ZMiX/QsjKo7SmJ5xwQkoppfHjx6cOHTqkI488Mr/NYYcdlnbaaaf0008/pcGDB6fOnTunvfbaK910002LnRFJdlSmSbzrrrvSdtttVyY2cODAtPPOO+df11Oa/9peUlKSLr300rT99tuXaQhSSumqq65KuVwuHXrooWnKlCkppfkHjOecc05q3bp1ev7559PMmTPTTz/9lO6///50yCGHpA8//DB//3fffTc98sgj+fsuasqUKWXWw8pl4ffNrqj5uPnmm1OLFi3St99+m/+D44MPPphyuVwaNWpUGjduXKpTp04aNmxYmfu1bds2/e///m+Z2NVXX5169uyZPvnkk5TS/Ofs2muvnU4//fS0zz77pEsvvTSllNJPP/2U3nnnnQrPzp41a1aZs/vKy6n0/wcOHJguu+yylFJKo0aNSt27d0+HHnpo+uijj/LbLfqH1I8++ihtsMEGqXfv3mmrrbZKDRs2TI0aNUo9e/ZMRx11VPr6669TSin169cv9ejRI6W0oHkbO3Zsql+/fj7Hl156KbVo0SJtv/326Re/+EVq1apVuuqqq8pdO2SRHqP20V/UPvqLVZseg+pW23oM/UXtZLCRMaUHM8cff3xKqfzp6QYbbJC23HLL1KtXr1S/fv3Uvn37NGjQoDR9+vTqXi6VcOaZZ6ZddtklTZ48OaW0YPLbqVOnlMvl0hFHHOHD+TLmzDPPTLvuumv67rvvUkppsV+uf/vb39Lpp5+epkyZkn766aeaWCJV6KefflriQWBK8z+48bjjjivzx4Mbb7wx9ejRI5188slp//33T82aNUudOnVKb7zxRnr++efTZpttlm6//faU0oKDzPPPPz9tuummqXfv3qlHjx5pwoQJ+TXsueeeqWvXrql3796pRYsWqUOHDum8887LPy8XVVJS4gy+DHr33XfThhtuWOHlyWPGjEkFBQXp/fffz8d+85vfpG7duqVvvvkmpTT/+OHcc89NKS04vjjuuOPSlltuWeZ+Bx54YPrlL3+Z/3ry5Mnpf/7nf9JOO+2Uzj333PzZwAsrKSlZ6g8mXfT+48ePTw0bNkyPP/54Siml008/PXXq1KlM41zqiy++yL/dyoQJE1Lfvn1Tw4YN02233Zb+85//pClTpqThw4enTp06pV//+tcppfkfhJvL5dKzzz6bb67PO++81KFDhzI/J6NGjUpXXnll+vvf/+6tdKh19Bi1k/6i9tFfoMegOtWWHkN/UXsZbGTQmWeemXbffff8D8eXX36ZbrjhhrT33nuncePGpQsvvDBtt9126dJLL81PAln5PfXUU2mTTTZJTzzxREppwRlyn3zySXr77bdrcmkso9KaPvnkkyml+b/ES/9Re02dOjU1b948PfjggxVuU/rzfdJJJ6Xdd989FRcX5w/0R4wYkXr06JFatWqVBg8enJ555pn8WU7fffdd6tWrV/7AMKX5f4xq1qxZGj58ePr+++/TNttsk7p165buuOOO/DavvPJKGjp0aJn38CR7ltQMFhcXp06dOqUnnngizZ07t8zrTOn91l133XTQQQel448/PrVt2zY1adIk3X333fntTj755NSrV680b968/H0+/vjj1Lt379SlS5f097//Pf32t79N3bp1S08//XSV5Ni3b9/UunXrNGzYsPzPyfnnn5969eqVioqK0uzZs9OOO+6Y9thjj5TS/Lc9mDVrVhoyZEhq3rx5yuVy+bdWmTNnTrriiitS69atF9vPqaeemjp37pw/q+qwww5L7dq1S/369UuHHHJIateuXbrrrrvKfP+gttNj1D76i9pHf7Hq0mNQVWp7j6G/qN1W7EfEUy369OkTI0eOjJ122im6dOkSnTt3jksvvTTWXnvtqFu3bgwaNChefPHFOPvss2ODDTao6eWylDbZZJPYaKONol69ehERUVhYGBERG2ywQfTo0aMml8YyKq1p3bp1IyKioKAg/4/sSClFSUlJubeVlJREcXFx/uvi4uJYY401YoMNNoj77rsv/vSnP0Xv3r3jzjvvzD9WROSfA5tuumm8//77ZZ4TXbt2jS5dusSOO+4YF1xwQfTp0yeaNWsWKaVo0aJFdOrUKUaNGhUTJkyIiIhbb7011lprrejevXs0b948nnzyydhpp51i4sSJMXv27IiI2HrrreOEE06IXr165dfNyq28510ul4tJkybFTz/9VCY+e/bsKCgoiA022CDeeeedqFOnTv45NWHChMjlchERceCBB8b9/8/efYdHVW19HP/NTEgjtBAgEEpo0gUvTaTq5RKqcFEELEAExIJXREWQjgUbCJYLghQVUKRcFWkizReIoIKoCBgiSA9NEgghIZn9/oFzyCQTSJCUid/P8/Bo1pw5Z+9ZM5Ozs/Y+Z/FiJScn680339Qff/yhnj17Wvvp3Lmzdu3apWPHjlnPqVWrlubNm6e77rpLM2fO1JkzZ/Taa6+pbdu2GdrsdDr/8ntr9uzZeuSRRzRmzBi98MILkqQdO3aoXr16CgwM1JkzZ5ScnKzExERJko+Pj3x9fVWzZk3NnTtXy5Yt07lz5xQdHa1ChQqpXr16SkpK0jfffON2nD179qh48eLW53fmzJl6++23FR8frxIlSmjhwoXq1auX9boDfweMMQoexhcFD+OLgoMxBvLC33GMwfiigMu7mgqu1/Hjx03nzp1Nnz59zJIlS1hiCgD5iGvZaUBAgGnatKl55ZVXrGW46W3atMnY7XbrBmmumRujRo0yLVu2NNHR0VbcNbtk8uTJpmnTptbN1tq0aWMiIyONMeaq16hlVkj+ldUl1J9++qkpU6aM2bx5s/W8tN566y3TsmVL8/zzz5u2bdsam81mevToYT2+bNkyExgYaF2SxBj398zp06eNzWazZhKlPUZuXv/40qVL5qOPPjKhoaGmU6dOJjQ01Pzwww/W4926dTN16tSxlnGnfR1OnDhhKleubN0Yde/eveYf//iHdU3erVu3mqefftrUq1fPutwCs1yByxhjAED+xRgD2cUY4wrGFwUXZX0vVKZMGS1btkzvv/++unfvLj8/v7xuEgAUWAcOHNCgQYM0c+ZMSVJKSoqky7NdvvzyS/Xv31/33nuv5s+fL+nyzKU333xTycnJWrx4sYYNG6ayZct63Hft2rVVtmxZLV682C1ev359paSkaMeOHdaxXLM6GjVqJEm6dOmSJOmTTz7R7NmzJV2ZiSnJbZaXxKyQ/Mg188hms2WYaXn06FF17txZzz77rM6fPy9JWrRokZo2barbbrtNqampstlsio+P19y5c9W7d2+NHz9eW7Zs0dKlS9WqVStt3rxZc+bMsfbZrFkzGWO0detWK5b2PRMcHKyHHnpIwcHBVsz1vnFtl5qaas0KzCk+Pj7q1auXFi9erB07dighIUGlSpWyHm/atKkOHTqkr7/+2mqj67UsXry4qlatqkWLFkmSSpUqpXr16mnEiBEKCQlRq1attGPHDo0ePVo9evSQJGa5An9ijAEAuYcxBnIKY4yMGF8UXLzSAIC/PddJ1O+//67/+7//c3vM6XRq37591lJTHx8fSdLbb7+tIUOGKCEhQTfddJOGDh2qZ555Rk6nU4MHD1bRokW1cuXKqx63RIkS6tWrl+bPn68DBw5YJ3g333yz7Ha71q1bJ8n9pLRly5b65ptv1K5dO0lyOyFLK+3JJPKepxN1V0737t2rOXPm6P/+7/+sE+iQkBD16NFDy5cvV8+ePfX111/ru+++U+/evSVdzm9ycrKGDBmi119/XSEhIVqwYIGqVKmi5557TqNHj1azZs1UuHBh63glS5ZU9erVrZNyT6ZPn269tzxxOBy5NoANDw9XSEiIzp8/r8cff1zffvutJKlLly66+eabNXz4cO3cuVNOp9N6LTdt2qSLFy9al1gpXry47rnnHr388stasWKFLl68qK+++ko9evTgj7YAACBHMcZATmOMkT2MLwqgvFkoAgBA/uBaZrpv3z5js9mMzWYzr7/+utvjTz31lGnZsqVJTEw0xhgTHR1twsPDzYoVK6ztli1bZsqWLWvmz59vjDEmIiLC3H333dc8bnR0tGnZsqUZNWqU9VhKSooZNmyYmT59eqbLu1ne6h0yy19KSopZtmyZadCggQkODja33HKLqVWrlnnggQfcttu/f7+59dZbzc0332xq1aplEhISjDFX8h8XF+e2fe/evc2TTz5pkpKSPB735ZdfNsOHD7/mTQLzkqtt7733nqlfv76JiooyHTp0MOHh4dbNbr/77jtzyy23GD8/P9OnTx/Tt29fU6tWLVOuXDnzxBNPmD/++CMPewAAAP7uGGMgJzHGyB7GFwUXKzYAAH87qamp1jJq18yQqlWrqk6dOqpTp45GjBihjz/+WElJSbLZbKpevbrOnz9vzehYsmSJGjdurNKlS2vixIlq2bKl7r//fmvfktS1a1dFRUVZS3zTcx23WrVqGj58uN577z1t2rRJ0uUZK6+88ooGDRqU6cwVlrfmjavduM4Y47Y83/y5vP/SpUtat26d2w3mUlJSdPjwYQ0YMEAHDx7U9u3btXTpUn300Uf66quvrOeHh4dr0aJFOnr0qPbs2aOoqChJl/PvdDpVtGhRSVfed//617+0bt06nT592mMbn332WU2cOPGqM6Ly+r1ls9lkjNFXX32levXq6dZbb9XSpUvVtGlTdezYUevXr1fDhg21atUqzZ0715oxNmzYMP3888+aMmWKihcvnqd9AAAAfz+MMXC9GGPkLMYXBRffWACAAs14WJ7rcDjkcDhkjNG+fft07tw5SVLNmjXVqFEj9e7dW6+++qpmzJghSapbt64CAgKsQUGFChW0ePFiderUSatXr1bHjh21bt067d+/Xw888IAkqUOHDjp69KjWr18vSTp37px1zdr0OnbsqP79++vjjz/Wrl27rHZf7QQXuev06dNKTk52OyH3dI1h1/L8Y8eOyWaz6dtvv1Xt2rXVt29f9ezZU7NmzVJKSor8/PwUERGhxx57TJK0cOFCvfXWW0pNTdXKlSt1/vx5a2Dwyy+/qGrVqurevbsefvhhvfLKK5LcBweu/2/Xrp3Cw8Ov2hdveF/Fx8dr5cqVioyMlCT5+/vr448/VpcuXTR48GB98803Kl26tHr16qX//ve/mjt3rvr166cSJUrkccsBAMDfAWMM3AiMMXIP44sCKi+WiQAAkFcuXLhgZs6caRo2bGiKFStmOnbsaDZu3GiMMWb69OmmQYMGZsOGDebtt982pUqVMosWLTKpqamme/fupkePHsYYY/bs2WNsNpv54osv3PadnJxsduzYYc6ePWuMMaZ9+/bm5ptvNhUrVjShoaFmy5YtGdrjWhZ74cIFc/DgwQzLfpF3nE6n+eyzz0yZMmVM9erVzfLly83zzz9vZs6cmWHbgwcPmvj4ePPQQw+ZIkWKmPDwcDNt2jTTo0cP8/nnnxtjjHnwwQdNkyZNTFRUlPW8lStXmvr165s6deqYJ5980vTq1ctUq1bNHDp0yNpm9OjRplmzZiY1NdXMnTvXFClSxPTv39/Ex8fn/IuQR55//nlTt25dc+LECWPM5WX1xhhz8eJFa5urLXUHAADITYwxkFWMMfIG44uCiRUbAIACw9MskXPnzmnlypU6ceKEJGnt2rWaMmWKevXqpQ0bNuill16ybo4XERGh2NhYHT58WI8++qgeeeQR9e/fX999951q1qypw4cPKzY2VjVq1FCdOnU0Y8YMff3115Kk8+fPa968eZo5c6YOHz4sSZo1a5aGDx+u1157TXv37lWzZs0yzO5yzZgJCAhQhQoVrGW/yHt//PGHXnzxRT3wwAP68ccf1bFjR61YsUI//vijjh8/rlmzZmn37t06cuSIKlWqpMjISIWEhOj//u//dOedd2rUqFGy2Wzq0qWLJGnw4MEKDAzUihUrJEn79+/XyJEj1bx5c3399deaPHmy+vfvr4MHD+q3336z2jF79mw98sgjstvt6tu3rxYuXKgLFy4oLi7OY7uNMR5nEXoLY4wSEhJ03333WZ9N1yy1tDfky62bmAMAgL83xhi4kRhj5D7GFwVY3tVUAAC4OtcsivScTqdJSUlxuwGZa3ZFYmKi203NvvzyS2Oz2cyGDRtMamqqeeihh0yrVq0yPeYtt9xihg4das6fP2+MMaZnz56mefPmpl+/fqZNmzbms88+M8YYs2XLFvPPf/7T1KlTx7Rq1cqUKFHCVKtWzUydOjXTWS7MAPE+Pj4+5uuvvzbGGLN3717TqlUr43A4jL+/vylZsqRZs2aNOXfunOnRo4cJCgqyZuYdO3bMdOnSxbRp08ba15kzZ0z//v3NP//5T2OMMUeOHDE2m81s2rTJGHN5tlCPHj2MzWYzr732mjHGmBkzZpiqVaua/fv352KvAQAACi7GGMhrjDGAG4MVGwCAfMs1i0Jyv46t6zqjaa//abPZtG/fPgUGBlo3P5OkO+64Q6VKldLevXtlt9tVuXJlxcbGqlevXhoyZIgmT56shQsX6tixY5Kkxo0b68cff9TRo0clSW+++aYaNWqkDz74QDt37tT27dslSc2aNdPnn3+u119/Xd27d9eXX36p6Oho/ec//1GRIkXc+uFqOzNAcp/58zrCmV3z1RijlJQUt/eX6/9/+uknFS9eXD179pTdbtfTTz+tatWqqXz58po5c6ZOnTqltm3bqnDhwmrQoIGKFCmiVq1aSZJCQkJ0++2365dffrH2W6JECdWuXVtHjhzR0aNHVa5cOVWsWFHPPvuspk+frieeeEJNmjRRixYt9Pvvv0uS7rrrLu3bt0/h4eFufTBePGMKAAAgLzHGwF/FGAPIHyhsAADyJafTqenTp6tjx46S3G+iFhMTo+eff169e/fWjBkzlJCQIEmqVq2aAgMDtWvXLuukzOFwqEaNGtq8ebMSEhI0bNgwPfjggypRooTsdrs+++wzDRkyRCNHjpR0ean4wYMHraXepUuX1oQJE9SnTx+dPXtWf/zxh9WOwMBAtW/fXk888YQaNWpktTs9Bht5x2azyW63y263KzExUbGxsW45stls8vHxkc1mU2xsrGJjY2Wz2XT8+HG9+eabcjgcstlsio6O1ueff64XXnhBpUuX1qFDhyRdPvm32Wxq3ry5jh8/br1vfHx8VK9ePSUnJ+vbb7+1jlerVi35+flpw4YNkqQPP/xQNWvW1GuvvaYzZ86oa9eu2rBhg9566y1JUnBwsKTL76v0g2zeVwAAANnDGAM3AmMMIH+gsAEAyHMpKSnavHmzYmJirJjNZpPT6dSqVaskXT6Jk6Rt27apW7duWr16tUqXLq1Ro0bpoYceUnR0tCTp1ltv1dq1a5WQkGCdlEVEROj777/X0aNHZbfbNWzYME2bNk0vvviiNm7cqCFDhmjJkiWSpNatW+vIkSP67rvvrLYULVpUb775puLi4jR16tQM7U87qyXtiSFyX3JysqQrg78ffvhBTzzxhOrWrasGDRro9ddf14EDB6ztL1y4oPHjx6tMmTKqV6+e7r33Xn3wwQcKDQ3VzJkztWzZMsXGxqpw4cKSpLJly6p06dLau3ev4uPjrffYTTfdpNKlS1vvV0mqXLmyqlWrppUrV1qxSpUqKTAwUDt27JAktWzZUtOmTVNMTIw++eQTVa9eXXa7PcPglfcVAABA9jDGwI3CGAPIn3gHAwDyVGpqqvr27auWLVvqvvvuU3x8vKTLg45mzZrJz89PmzZtsrYfPXq0wsLCtGHDBk2dOlUffPCB9u7dqzfffFOS1LVrV0VFRenUqVPWczp37qxff/1V+/fvlyQdPXpUhw4dUqFChfTLL79o48aN6t+/v5KTk1WyZEkNHjxYt956q9tgokiRIipSpAizpfKp6Oho9enTx7rRot1u1//93/9pwIABOnr0qJ566im98cYbatSokQoVKmQ9b9myZZo/f74mTZqkDRs26KabbtKgQYO0fv16SVJ4eLiCgoK0bt066zkNGjTQ77//7nbzvZIlS+q2226zbtonSaVKlVLlypU1b948K3bTTTdp6dKleu2116yYqz2pqanWe45BBgAAwPVjjIEbgTEGkL/xjgYA5CmHw6F69eqpVq1a2rZtm55++mnt3r1bkhQaGqqaNWtaM51cM1jat29vza5q27atunTpok8//VSS1K1bN8XGxlqzqyRZM6t27dol6fKJ5pNPPqkaNWqoSZMm8vPz0xNPPCFfX19J0ssvv6yWLVt6HExwMpi/uE7Sf/31V33//fdq27atJOncuXN65plnVKdOHc2ZM0eRkZHq2LGjevbsqQoVKljPf/HFF/Wvf/1L999/v2rXrq1p06apWbNmmj59us6cOaNSpUqpcePGWr58ufWc5s2b68KFC9aMKEny9fVVq1atrPehdHkW3rBhw/TBBx9YMR8fH4WGhnrsi2tJOgAAAP4axhj4KxhjAN6Bb04AQJ6rWbOmqlatqgEDBujkyZN6/PHHlZKSorJly6pFixbWDJWAgACdP3/eGnBIl0/UGjdurNOnT+vEiROqUKGCGjRooDfffFN79+7VhQsXNH/+fEnS8uXLde7cObVv31533nmnpk2bpjNnzuh///ufKlWq5NYmbpqW97KSA9dJ+rlz51SyZEkrXqRIEe3evVvt2rVTUFCQFU97HeU//vhDTqdTlStXliRdvHhRknTfffdp+/btio2NlSS1b99eX3/9tVJSUiRdvhRB6dKlNX36dL3wwgu67bbb9Ntvv6lt27Z68MEHrRmBktSoUSPdeuut1/sSAAAA4DoxxoAnjDGAgoPCBgAgzzVo0ECXLl1SYGCgpk6dqkOHDql3796SpDZt2ig6OloJCQmqWLGigoODtWvXLl24cEHS5ZPOkydPqmrVqjp+/LgkaeTIkTp9+rQiIiJUpkwZhYWF6fPPP9eQIUNUuHBhVapUSX369FG7du3k6+srp9OZ4QSXWS15z5WDpKQkpaamelyi77Jp0ybVrFnT7YS/devWGjt2rB577DENGjRIDz/8sF555RXNnz/fev/UrFlT27dvl3R5ACtdvibt/v37rcHtHXfcoRMnTujXX3+VJJUoUUKTJk1SrVq1tHnzZv3zn/9U6dKlVbduXb333nsqWrTojX8xAAAAkC2MMeAJYwyg4PC59iYAAOSs8PBwVaxYUbt371a5cuW0ZMkStWjRQqNHj1aHDh1UpkwZLVu2TL169VKHDh20YMECzZo1S48//rguXbqkTz75RNWqVVOtWrUkSd27d1fDhg21efNm1a9fX3Xq1MlwTNcgw2azsfQ7nzp79qyqVq2qWbNmqVu3bh63SU1NlcPhkNPp1OHDhxUUFCSn0ym73a7p06drzpw5+uKLL1ShQgUVKVJES5cu1d69e/XVV19pzpw5atKkiSZPniyn02ldh3bTpk0qXry4AgMDJcmabbV27VrVrl1b0uXr2M6dOzdDe9K+rwAAAJB3GGPAE8YYQMFhM6yDAwDkA1OnTtVHH32kV199Va1atdJHH32k119/XdWqVVNsbKwqVaqk999/X3/88YemTJmit99+W40aNVJ0dLSCgoI0bdo0NW/e/KrHMMZwMpiHjDEyxngc5LlmtLlmNLkGE82bN1d4eLhq1aqlNWvWaNCgQbr33nutXLr+++677+r555/X4cOHPeY5MTFRKSkpKlKkiGbNmqWHH35YMTExKly4sDUwffrpp+Xr66shQ4aoS5cumjBhgvX8Xbt2qVatWhna7prhxcAVAAAg/2GMUfAxxgD+vviEAADyhfr168vhcFhLdnv37q0JEyZo69atioqK0tdffy3p8hLdMWPG6LPPPlPTpk316quvatu2bR4HHCz9zl+uNnPNbrdbAw7p8pLtNWvWKCoqSv/73//0xRdfqFOnTrr99tutfaX9b926dXXs2DEdPnzYY54DAgJUpEgRSVKhQoUUEhKiX3/9VSVLltS8efNUunRpPfjgg+ratasaN26shx56yO35derU8dh2u93OgAMAACCfYoxR8DHGAP6+WLEBAMgXTp48qYceekghISGaOXOmFV+xYoU6d+6sQoUK6eTJk5leW5SZUvnfgQMHNHHiRDVq1EgDBw5USkqKfHx8ZIzRmjVrtHDhQiUmJqpTp0667777lJCQoDlz5mjIkCE6cOCAypcvn+m+//jjD9WrV09PP/20hgwZ4vbY4cOHFRISokKFCmnv3r165plnVKJECc2cOVP+/v6y2WxKSkrS6dOnVa5cuRx+FQAAAJBbGGMUfIwxgL8vyn8AgHyhVKlSCg0N1bfffqtjx45JurxUuGPHjjp8+LCSkpIyDDhcy44lZkrlF658/P777/q///s/t8ecTqf27dunb775RpKsG+e9/fbbGjJkiBISEnTTTTdp6NCheuaZZ+R0OjV48GAVLVpUK1euvOpxS5QooV69emn+/Pk6cOCAFf/yyy/173//WwMGDFDNmjV16623KiAgQMOHD1dAQID1vvHz87MGHKmpqRlm4gEAAMD7MMYoGBhjAPCEwgYAIN+IiIjQoEGDFBAQIEnWsmHXyaCnZd8MNvIP14y2mJgYVa5cWa1bt9akSZOsxytXrqxbbrlF0dHRunjxoiRp3759mjx5siZNmqSPP/5Y48aN06xZszR//nwtW7ZMktSkSRN9+eWXVz2uJD388MMKCAjQrFmzrMeaN2+uvn37qnr16po0aZJiY2O1ePFi1a1bN9P9ORwO3lcAAAAFBGMM78YYA0BmfPK6AQAAuHTr1u2qj3MimP+kpqZKcj9Rr1q1qurUqSNJGjFihMLCwvTvf/9bfn5+ql69utatW6dvv/1WLVu21JIlS9S4cWOVLl1aEydO1IoVK/TTTz8pMDDQ2nfXrl314osv6vz58woKCsrQBtdxq1WrpuHDh6t///6KiIhQixYtVLhwYQ0ePNhte6fTyYAVAADgb4IxhvdhjAEgK1ixAQDIV1iem395yo3D4ZDD4ZAxRvv27dO5c+ckSTVr1lSjRo3Uu3dvvfrqq5oxY4akyzfgCwgI0KZNmyRJFSpU0OLFi9WpUyetXr1aHTt21Lp167R//3498MADkqQOHTro6NGjWr9+vSTp3LlzunTpksc2duzYUf3799fHH3+sXbt2We1Oe0kBu93OgAMAAOBvhDFG/sUYA8D14ubhAADguiQmJmr+/PmaPn269u3bp+bNm+vZZ59Vq1at9O6772r69OmaMmWKfv75Z40fP17//e9/1b17d/Xo0UMOh0OffPKJ9u7dq1q1amnZsmXq1KmTte9Lly5p165dqly5sooVK2YNPM6ePavk5GQtXbpUzZo1c2uPa5l6YmKiTp06pWLFimV6I0gAAAAA+Q9jDABZxaWoAABABk6nU3a7+8LOc+fOadOmTWrYsKFKly6ttWvXasqUKerXr5/atm0rm80mX19fSZevZTx+/HgdPnxYjz76qE6cOKH+/furYsWKqlmzptavX6/Y2FjVqFFDderU0YwZM1SkSBG1atVK58+f16JFi/Tdd9/p0UcfVbFixTRr1ixt3LhRDodD7du3V9GiRa1Bhovr/wMCAlShQoXce7EAAAAAXBNjDAA3Eis2AAD4G0hNTbVulJiWMca6HqxrkOE6mb948aLsdrs1kFizZo0iIiK0fv16tWzZUo888oj27NmjjRs3ejzmP/7xD91+++2aMGGCChcurF69eunw4cOqXr26Dhw4oCeffFJ33nmnoqKiNHr0aB0/flwlS5bUTz/9pJIlS+rxxx9XZGSkihQp4rHdLPUGAAAA8g5jDAB5iXtsAADwN5B2wJF2ToPNZpPD4XCbOWWz2bRv3z4FBgYqKirKit9xxx0qVaqU9u7dK7vdrsqVKys2Nla9evXSkCFDNHnyZC1cuFDHjh2TJDVu3Fg//vijjh49Kkl688031ahRI33wwQfauXOntm/fLklq1qyZPv/8c73++uvq3r27vvzyS0VHR+s///lPhgGHq+0MOAAAAIC8xRgDQF6isAEAQAHndDo1ffp0dezYUdLlmVUuMTExev7559W7d2/NmDFDCQkJkqRq1aopMDBQu3btsm6K53A4VKNGDW3evFkJCQkaNmyYHnzwQZUoUUJ2u12fffaZhgwZopEjR0q6vFT84MGDOnz4sCSpdOnSmjBhgvr06aOzZ8/qjz/+sNoRGBio9u3b64knnlCjRo2sdqfHYAMAAADIe4wxAOQ1ChsAABQgKSkp2rx5s2JiYqyYzWaT0+nUqlWrJEk+PpdvsbVt2zZ169ZNq1evVunSpTVq1Cg99NBDio6OliTdeuutWrt2rRISEqyT/YiICH3//fc6evSo7Ha7hg0bpmnTpunFF1/Uxo0bNWTIEC1ZskSS1Lp1ax05ckTfffed1ZaiRYvqzTffVFxcnKZOnZqh/WlneqW//i4AAACA3McYA0B+xKcZAIACIjU1VX379lXLli113333KT4+XtLlQUezZs3k5+enTZs2WduPHj1aYWFh2rBhg6ZOnaoPPvhAe/fu1ZtvvilJ6tq1q6KionTq1CnrOZ07d9avv/6q/fv3S5KOHj2qQ4cOqVChQvrll1+0ceNG9e/fX8nJySpZsqQGDx6sW2+91W0wUaRIERUpUoTZUgAAAEA+xxgDQH5FYQMAgALC4XCoXr16qlWrlrZt26ann35au3fvliSFhoaqZs2a1kynvXv3Kj4+Xu3bt7dmV7Vt21ZdunTRp59+Kknq1q2bYmNjrdlVkqyZVbt27ZIkLVu2TE8++aRq1KihJk2ayM/PT0888YR1M8CXX35ZLVu29DiYYLYUAAAAkL8xxgCQX/FpBwCgAKlZs6aqVq2qAQMG6OTJk3r88ceVkpKismXLqkWLFlqxYoUkKSAgQOfPn7cGHNLlQUvjxo11+vRpnThxQhUqVFCDBg305ptvau/evbpw4YLmz58vSVq+fLnOnTun9u3b684779S0adN05swZ/e9//1OlSpXc2pR2JhUAAAAA78IYA0B+RGEDAIACpEGDBrp06ZICAwM1depUHTp0SL1795YktWnTRtHR0UpISFDFihUVHBysXbt26cKFC5IuL9E+efKkqlatquPHj0uSRo4cqdOnTysiIkJlypRRWFiYPv/8cw0ZMkSFCxdWpUqV1KdPH7Vr106+vr5yOp0ZBhks/QYAAAC8F2MMAPmRz7U3AQAA3iI8PFwVK1bU7t27Va5cOS1ZskQtWrTQ6NGj1aFDB5UpU0bLli1Tr1691KFDBy1YsECzZs3S448/rkuXLumTTz5RtWrVVKtWLUlS9+7d1bBhQ23evFn169dXnTp1MhzTNciw2Wws/QYAAAAKGMYYAPIjChsAABQwtWvX1s6dO7Vlyxa1atVK06ZN0+uvv65ff/1VNWrU0MqVK9WrVy8NGjRIiYmJGjdunL744gtFR0crKChI06ZNU6FChaz9VapUyePSb9csKWZLAQAAAAUbYwwA+Y3NcFE6AAAKlA0bNmjkyJHq0aOHhgwZIuny9Wofe+wxHTt2TOXKldP+/fslSampqYqKitKXX36pm2++WZ07d5a/v3+GfaYdZAAAAAD4e2GMASC/obABAEABc/LkST300EMKCQnRzJkzrfiKFSvUuXNnFSpUSCdPnlTRokU9Pp8BBgAAAIC0GGMAyG+4SB0AAAVMqVKlFBoaqm+//VbHjh2TdHnWVMeOHXX48GElJSVlGHAYY9yuYwsAAAAALowxAOQ33GMDAIACKCIiQjfffLMCAgIkSQ6HQ5JUrlw5SRlnTDHQAAAAAHA1jDEA5CdcigoAAAAAAAAAAHgNLkUFAEABxdwFAAAAADcSYwwA+QUrNgAAAAAAAAAAgNdgxQYAAAAAAAAAAPAaFDYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeA0KGwAAAAAAAAAAwGtQ2AAAAAAAAAAAAF6DwgYAAAAAAAAAAPAaFDYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeA0KGwAA3GDbtm2Tr6+vfv/99xu632+//Va33XabChcuLJvNph9++CHLz507d65sNpsOHDhwzW3Dw8PVr18/6+dVq1YpKChIJ0+ezH6jAQAA8rE2bdqoTZs21s8HDhyQzWbT3Llz3bZbtWqVGjRoIH9/f9lsNp09ezZX25mdc7mCZNy4cbLZbHndjALP6XSqbt26evHFF2/ofs+fP68BAwYoNDRUNptNQ4YMydbzbTabxo0bd83t0r9PLl26pAoVKui///1vNlsMwJtQ2ADgVVwn9Gn/lS5dWrfffrtWrlyZ1827qi1btmjcuHHZHgRt2LBB3bt3V2hoqHx9fVW6dGl16dJFS5cuzZmGeolhw4bJZrOpZ8+eHh/fsGGDbDabFi9e7PHxwYMHexwkpaamas6cOWrTpo2Cg4Pl5+en8PBwRUZG6rvvvstS20aOHKnevXurUqVKVqxfv34Z3rs2m001a9bM0j4vXbqkHj166MyZM3rjjTf04Ycfuu0/J7Vv317VqlXTxIkTc+V4AAAAmYmJidGgQYNUpUoV+fv7q2jRomrevLmmTp2qxMTEHDnm6dOndc899yggIEDvvPOOPvzwQxUuXDhHjpVbXOOqzM5v27Rpo7p16+Zyq7xDeHi42/l84cKF1aRJE33wwQcZtnWNSWw2m+bNm+dxf82bN5fNZsvweicnJ2vq1Km65ZZbVLRoURUvXlx16tTRQw89pD179ljbeRojp/33zTffXLNPH330kQ4dOqTBgwd7bPv17FOSXnrpJc2dO1ePPPKIPvzwQz3wwANZet5fVahQIQ0dOlQvvviiLl68mCvHBJD7fPK6AQBwPSZMmKDKlSvLGKPY2FjNnTtXHTt21LJly9S5c+e8bp5HW7Zs0fjx49WvXz8VL148S88ZO3asJkyYoOrVq2vQoEGqVKmSTp8+rRUrVuiuu+7S/Pnzde+99+Zsw/MhY4w++ugjhYeHa9myZTp37pyKFCnyl/ebmJio7t27a9WqVWrVqpWee+45BQcH68CBA/rkk0/0/vvv6+DBgypfvnym+/jhhx/01VdfacuWLRke8/Pz03vvvecWK1asWJbaFhMTo99//10zZ87UgAEDstexG2DQoEF6+umnNX78+BvyWgMAAGTX8uXL1aNHD/n5+alPnz6qW7eukpOTtWnTJj3zzDPatWuXZsyY8ZeOUalSJSUmJqpQoUJW7Ntvv9W5c+f0/PPPq23btn+1G9flgQceUK9eveTn55cnx88ro0aN0vDhw/O6GRk0aNBATz31lCTp2LFjeu+999S3b18lJSVp4MCBGbb39/fXggULdP/997vFDxw4oC1btsjf3z/Dc+666y6tXLlSvXv31sCBA3Xp0iXt2bNHX3zxhW677bYME6RcY+T0qlWrds3+vPbaa+rVq5fHscl//vMfNW7cONv7lKR169bp1ltv1dixY7O0/Y0UGRmp4cOHa8GCBXrwwQdz/fgAch6FDQBeqUOHDmrUqJH1c//+/VWmTBl99NFHVy1spKSkyOl0ytfXNzea+ZcsXrxYEyZM0N13360FCxa4Da6eeeYZrV69WpcuXcrDFv41Fy9elK+vr+z27C8e3LBhgw4fPqx169YpIiJCS5cuVd++ff9ym5555hmtWrVKb7zxRoZl0mPHjtUbb7xxzX3MmTNHFStW1K233prhMR8fnwyDmaw6ceKEJGW5KHaj3XXXXXr88ce1aNEiBgYAACDX7d+/X7169VKlSpW0bt06lS1b1nrsscce0759+7R8+fK/fBybzZbhj8w5cR6WkJCQrVUfDodDDofjhh3fW/j4+MjHJ//96SosLMztvL5fv36qUqWK3njjDY+FjY4dO+rzzz/XqVOnFBISYsUXLFigMmXKqHr16vrjjz+s+LfffqsvvvhCL774op577jm3fb399tserwKQfoycVTt27NDOnTs1adIkj4+3bNlSd999d7b3K13+7NSuXfu6nvtXFS9eXO3atdPcuXMZvwAFFJeiAlAgFC9eXAEBAW4nva7r477++uuaMmWKqlatKj8/P/3yyy+SpD179ujuu+9WcHCw/P391ahRI33++edu+z1z5oyefvpp1atXT0FBQSpatKg6dOignTt3ZmjDW2+9pTp16igwMFAlSpRQo0aNtGDBAkmXr/n5zDPPSJIqV65sLeG92jVyR48ereDgYM2ePdutqOESERFhFXGSk5M1ZswYNWzYUMWKFVPhwoXVsmVLrV+/3u05aV+TGTNmWK9J48aN9e2332Y4xp49e3TPPfeoVKlSCggIUI0aNTRy5Ei3bY4cOaIHH3xQZcqUkZ+fn+rUqaPZs2e7beNaxvzxxx9r1KhRCgsLU2BgoOLj462ZR8eOHcv0tUhv/vz5ql27tm6//Xa1bdtW8+fPz/JzM3P48GG9++67+te//uXx2q8Oh0NPP/30VVdrSNKnn36qO+64I9NrAaempio+Pj5bbevXr59at24tSerRo4dsNpvbtaDXrVunli1bqnDhwipevLi6du2q3bt3X3O/xhi98MILKl++vAIDA3X77bdr165dHrctXbq0br75Zn322WfZajsAAMCN8Oqrr+r8+fOaNWuWW1HDpVq1anriiSesn+fMmaM77rhDpUuXlp+fn2rXrq1p06Zd8zjp77HRpk0bawJN48aNZbPZ3O5FtmjRIjVs2FABAQEKCQnR/fffryNHjrjts1+/fgoKClJMTIw6duyoIkWK6L777pN0uZAyePBgffrpp6pbt651Pr1q1Sq3fXi6x8Znn32mTp06qVy5cvLz81PVqlX1/PPPKzU19Zr9vB4pKSl6/vnnrTFEeHi4nnvuOSUlJbltl9l9EdLfx+3SpUsaP368qlevLn9/f5UsWVItWrTQmjVrrG083WMjq6+ZdHkc0qhRI/n7+6tq1ap69913Pe7z1KlT2rNnjy5cuHAdr4xUqlQp1axZUzExMR4f79q1q/z8/LRo0SK3+IIFC3TPPfdkKFq59tO8efMM+3I4HCpZsuR1tdOTTz/9VL6+vmrVqlWm25w7d04pKSlZ3qdr/Ld//34tX748w/j3xIkT1uREf39/1a9fX++//36W9r1p0yY1btzYLaeZ+de//qVNmzbpzJkzWW47AO+R/8reAJAFcXFxOnXqlIwxOnHihN566y2dP3/e42z4OXPm6OLFi3rooYfk5+en4OBg7dq1S82bN1dYWJiGDx+uwoUL65NPPlG3bt20ZMkS/fvf/5Yk/fbbb/r000/Vo0cPVa5cWbGxsXr33XfVunVr/fLLLypXrpwkaebMmfrPf/6ju+++W0888YQuXryoH3/8UVu3btW9996r7t2769dff9VHH32kN954w5qlU6pUKY/9i46O1p49e/Tggw9m6bI/8fHxeu+996xlyufOndOsWbMUERGhbdu2qUGDBm7bL1iwQOfOndOgQYNks9n06quvqnv37vrtt9+sIsqPP/6oli1bqlChQnrooYcUHh6umJgYLVu2zLqpXGxsrG699VZrcFGqVCmtXLlS/fv3V3x8fIYCwfPPPy9fX189/fTTSkpKkq+vr44cOaJatWqpb9++GW7S6ElSUpKWLFliLf3u3bu3IiMjdfz4cYWGhl7z+ZlZuXKlUlJS/tJ1X48cOaKDBw/qH//4h8fHL1y4oKJFi+rChQsqUaKEevfurVdeeUVBQUFX3e+gQYMUFhaml156yVoKXqZMGUnSV199pQ4dOqhKlSoaN26cEhMT9dZbb6l58+bavn27wsPDM93vmDFj9MILL6hjx47q2LGjtm/frnbt2ik5Odnj9g0bNtSnn36apdcCAADgRlq2bJmqVKmi2267LUvbT5s2TXXq1NGdd94pHx8fLVu2TI8++qicTqcee+yxLB935MiRqlGjhmbMmGFd6qdq1aqSLhcbIiMj1bhxY02cOFGxsbGaOnWqNm/erB07drit8EhJSVFERIRatGih119/XYGBgdZjmzZt0tKlS/Xoo4+qSJEievPNN3XXXXfp4MGDV/0D9ty5cxUUFKShQ4cqKChI69at05gxYxQfH6/XXnstS/1zjavS87QyfMCAAXr//fd1991366mnntLWrVs1ceJE7d69W//73/+ydLy0xo0bp4kTJ2rAgAFq0qSJ4uPj9d1332n79u3617/+ddXnZuU127Fjh9q3b6+yZctq/PjxSk1N1YQJEzyOwd5++22NHz9e69evd5tAlFUpKSk6fPiwSpQo4fHxwMBAde3aVR999JEeeeQRSdLOnTu1a9cuvffee/rxxx/dtnfdS2/+/Plq3rx5llateMqlzWa7ZhFky5Ytqlu3rsfJdNLlSzqdP39eDodDLVu21GuvvXbNlSG1atXShx9+qCeffFLly5e3xm6lSpVSYmKi2rRpo3379mnw4MGqXLmyFi1apH79+uns2bNuBcr0fvrpJ7Vr106lSpXSuHHjlJKSorFjx1pjo/QaNmwoY4y2bNmSby9ZDeAvMADgRebMmWMkZfjn5+dn5s6d67bt/v37jSRTtGhRc+LECbfH/vnPf5p69eqZixcvWjGn02luu+02U716dSt28eJFk5qammG/fn5+ZsKECVasa9eupk6dOldt+2uvvWYkmf3791+zn5999pmRZN54441rbmuMMSkpKSYpKckt9scff5gyZcqYBx980K3tkkzJkiXNmTNnMhxv2bJlVqxVq1amSJEi5vfff3fbr9PptP6/f//+pmzZsubUqVNu2/Tq1csUK1bMXLhwwRhjzPr1640kU6VKFSuWvk19+/bNUl8XL15sJJno6GhjjDHx8fHG398/w2vlOuaiRYs87uexxx4zaX8NPvnkk0aS2bFjR5ba4clXX32V4XV0GT58uHn22WfNwoULzUcffWT69u1rJJnmzZubS5cuXXPfmfWnQYMGpnTp0ub06dNWbOfOncZut5s+ffpYMddnx/X+O3HihPH19TWdOnVyy+lzzz2XaT5eeuklI8nExsZes70AAAA3SlxcnJFkunbtmuXnpD/nNMaYiIgIU6VKFbdY69atTevWra2fXeemc+bMsWKu86hvv/3WiiUnJ5vSpUubunXrmsTERCv+xRdfGElmzJgxVsx13jd8+PAMbZJkfH19zb59+6zYzp07jSTz1ltvZWhD2rGEpz4OGjTIBAYGuo1zPMlsXJX2X9rxzQ8//GAkmQEDBrjt5+mnnzaSzLp169z6NHbs2AzHrFSpkts5Zv369U2nTp2u2s6xY8e6nbO79p+V16xLly4mMDDQHDlyxIpFR0cbHx+fDPt0HWf9+vVXbY+rH+3atTMnT540J0+eND/99JN54IEHjCTz2GOPuW2b9hz+iy++MDabzRw8eNAYY8wzzzxjvR9bt27t9no7nU7TunVrI8mUKVPG9O7d27zzzjsZxmbGXD2Xfn5+1+xP+fLlzV133ZUhvnnzZnPXXXeZWbNmmc8++8xMnDjRlCxZ0vj7+5vt27dfc7+u1yp9jqdMmWIkmXnz5lmx5ORk06xZMxMUFGTi4+OtePr3Urdu3Yy/v7/b6/DLL78Yh8ORIafGGHP06FEjybzyyitZai8A78KlqAB4pXfeeUdr1qzRmjVrNG/ePN1+++0aMGCAli5dmmHbu+66y21WzpkzZ7Ru3Trdc889OnfunE6dOqVTp07p9OnTioiIUHR0tLV83M/Pz7oHRGpqqk6fPq2goCDVqFFD27dvt/ZZvHhxHT582OPlnK6H61JFWb1Js8PhsO4b4nQ6debMGaWkpKhRo0Zu7XTp2bOn22yili1bSrq8QkWSTp48qa+//loPPvigKlas6PZc17JtY4yWLFmiLl26yBhjvY6nTp1SRESE4uLiMhy7b9++CggIcIuFh4fLGJOl1RrS5VlLjRo1sm5YV6RIEXXq1OkvX44qu6+5J6dPn5YkjzO1Jk6cqJdffln33HOPevXqpblz5+rFF1/U5s2btXjx4us63rFjx/TDDz+oX79+Cg4OtuI333yz/vWvf2nFihWZPverr75ScnKyHn/8cbel+J4uw+Xi6penWX0AAAA55XrO09Kec7pmsrdu3Vq//fab4uLi/nKbvvvuO504cUKPPvqo2z05OnXqpJo1a3q834drpn56bdu2tVaBSJfP5YoWLWqdm2cmbR9d45qWLVvqwoUL2rNnT5b6kXZclfbfzTff7Lad67xy6NChbnHXTPzrub9J8eLFtWvXLkVHR2f7udd6zVJTU/XVV1+pW7du1ip76fIlyzp06JBhf+PGjZMxJsurNb788kuVKlVKpUqVUr169fThhx8qMjLyqitl2rVrp+DgYH388ccyxujjjz9W7969PW5rs9m0evVqvfDCCypRooQ++ugjPfbYY6pUqZJ69uzp8R4bnnK5cuXKa/bl9OnTHscvt912mxYvXqwHH3xQd955p4YPH65vvvlGNptNI0aMuOZ+M7NixQqFhoa69b1QoUL6z3/+o/Pnz2vjxo0en5eamqrVq1erW7dubmPUWrVqKSIiwuNzGL8ABRuXogLglZo0aeK2/LV379665ZZbNHjwYHXu3Nnt5uCVK1d2e+6+fftkjNHo0aM1evRoj/s/ceKEwsLC5HQ6NXXqVP33v//V/v373a5Xm3ZJ77PPPquvvvpKTZo0UbVq1dSuXTvde++9Hq+JmhVFixaVdHmAklXvv/++Jk2apD179rgtHU/ff0kZihWuEz7XDetcA4K6detmeryTJ0/q7NmzmjFjhmbMmOFxG9eNFq/Wluw4e/asVqxYocGDB2vfvn1WvHnz5lqyZIl+/fVX3XTTTde17+t5zTNjjMnSdk8++aRGjx6tr776Sr169VJqaqpOnjzptk1wcHCmN7v//fffJUk1atTI8FitWrW0evXqTG9M6Xpu9erV3eKlSpXKdAm9q1+Z3T8EAAAgJ1zPedrmzZs1duxYRUVFZbhvQlxcnIoVK/aX2nS187CaNWtq06ZNbjEfH59M79WW/txcunx+nvZm0p7s2rVLo0aN0rp16zLcwy2rxZv046q0x0/7x+Dff/9ddrvdmlzkEhoaquLFi1uvR3ZMmDBBXbt21U033aS6deuqffv2euCBBzIUVTy51mt24sQJJSYmZmivJI+x7GratKleeOEFpaam6ueff9YLL7ygP/74I9PzdunyH+979OihBQsWqEmTJjp06JDuvffeTLf38/PTyJEjNXLkSB07dkwbN27U1KlT9cknn6hQoUKaN2+e2/aZ5TIrsjp+qVatmrp27aqlS5cqNTVVDodDZ86ccbuUbUBAwFU/X7///ruqV69uTSB0qVWrlvW4JydPnlRiYmKG8Yt0+XPoaVIX4xegYGPFBoACwW636/bbb9exY8cyzPhJv0LA6XRKkp5++mmPs5PWrFljney+9NJLGjp0qFq1aqV58+Zp9erVWrNmjerUqWPtR7p8ErZ37159/PHHatGihZYsWaIWLVpo7Nix19WfmjVrSrp8DdGsmDdvnvr166eqVatq1qxZWrVqldasWaM77rjDrZ0u6W9O55LVE1rpyut4//33Z/o6pi/spM9Fdi1atEhJSUmaNGmSqlevbv1zzRxLu2rDNXMuMTHR474uXLjgNrsuu6+5J65i17UGoS4BAQEqWbKkdTO7Q4cOqWzZsm7/tmzZct3tudFc/XLdIwYAACA3FC1aVOXKldPPP/+cpe1jYmL0z3/+U6dOndLkyZO1fPlyrVmzRk8++aQkeTw/zmlpV4Kndz3n5mfPnlXr1q21c+dOTZgwQcuWLdOaNWv0yiuvSMq5Pv6VPxCnv6l5q1atFBMTo9mzZ6tu3bp677339I9//EPvvffeNfd1I8Yzf0VISIjatm2riIgIPfXUU5o3b54+/fRTTZ069arPu/fee/XDDz9o3Lhxql+/vmrXrp2l45UtW1a9evXS119/rerVq+uTTz7J1s28r6ZkyZJZHr9IUoUKFZScnKyEhARJUvfu3d3GL1e7R0ZuY/wCFGys2ABQYLhO7M6fP3/V7apUqSLp8oyZtm3bXnXbxYsX6/bbb9esWbPc4mfPns1wclS4cGH17NlTPXv2VHJysrp3764XX3xRI0aMkL+/f7YGATfddJNq1Kihzz77TFOnTr3mzaUXL16sKlWqaOnSpW7Hud7Cius1utrgsVSpUipSpIhSU1Ov+TreKPPnz1fdunU99uvdd9/VggULNH78eElXbri3d+9ej/vau3evtY0kdejQQQ6HQ/PmzbvuG4i7iiP79+/P0vauSwa4LpUWGhqqNWvWuG1Tv379TJ9/tT7u2bNHISEhHldrpH1udHS0lW/p8kyozAY2+/fvV0hISKY3vQcAAMgpnTt31owZMxQVFaVmzZpdddtly5YpKSlJn3/+udvM/vXr19+w9qQ9D7vjjjvcHkt/npkTNmzYoNOnT2vp0qVq1aqVFc/qeWh2VapUSU6nU9HR0dbMekmKjY3V2bNn3fpbokSJDJdKSk5O1rFjxzLsNzg4WJGRkdYNqlu1aqVx48ZpwIABf6m9pUuXlr+/v9sqbxdPsb+qU6dOat26tV566SUNGjQo03PwFi1aqGLFitqwYYNVhMqOQoUK6eabb1Z0dLROnTql0NDQv9p01axZM1vvm99++03+/v7WGHXSpElu44e0l/7ypFKlSvrxxx/ldDrdin2uy6dl9tkpVaqUAgICPF66LLMxn6tfad+zAAoOVmwAKBAuXbqkL7/8Ur6+vtc8aSldurTatGmjd9991+PJddpLATkcjgyzfhYtWmTdg8PFdW8FF19fX9WuXVvGGOuyUK6TW0/XQ/Vk/PjxOn36tAYMGOBxNs6XX36pL774wmqn5D5DaevWrYqKisrSsdIrVaqUWrVqpdmzZ+vgwYNuj7mO4XA4dNddd2nJkiUeCyDpL6mUmUuXLmnPnj0ec5HWoUOH9PXXX+uee+7R3XffneFfZGSk9u3bp61bt0q6PKupQYMGmjdvXobX/Pvvv9c333zjdn3dChUqaODAgfryyy/11ltvZTi+0+nUpEmTdPjw4UzbGBYWpgoVKui7775zi1+8eNHjpROef/55GWPUvn17SZdXmbRt29btX2aXhUrbx/fff9+tjz///LO+/PJLdezYMdPntm3bVoUKFdJbb73l9r6ZMmVKps/5/vvvr/mHBAAAgJwwbNgwFS5cWAMGDFBsbGyGx2NiYqzZ8p7OjePi4jRnzpwb1p5GjRqpdOnSmj59upKSkqz4ypUrtXv3bnXq1OmGHcsTT31MTk7Wf//73xw5nuu8Mv254uTJkyXJrb9Vq1bV119/7bbdjBkzMqzYSD+GCgoKUrVq1dxez+vlcDjUtm1bffrppzp69KgV37dvn8f7Tpw6dUp79uzJcNmy7Hj22Wd1+vRpzZw5M9NtbDab3nzzTY0dO/aqk6mio6MzjMOky2PJqKgolShR4oZNNmrWrJl+/vnnDK+7p/Hczp079fnnn6tdu3ZWUaJhw4Zu45drrULp2LGjjh8/roULF1qxlJQUvfXWWwoKClLr1q09Ps/hcCgiIkKffvqp22uze/durV692uNzvv/+e9lsNsYwQAHFig0AXmnlypXWjI4TJ05owYIFio6O1vDhw61r8F7NO++8oxYtWqhevXoaOHCgqlSpotjYWEVFRenw4cPauXOnpMszwyZMmKDIyEjddttt+umnnzR//ny3Ge7S5RvBhYaGqnnz5ipTpox2796tt99+W506dbJuctiwYUNJ0siRI9WrVy8VKlRIXbp0yXQ2T8+ePfXTTz/pxRdf1I4dO9S7d29VqlRJp0+f1qpVq7R27VotWLDAaufSpUv173//W506ddL+/fs1ffp01a5d+5orWDLz5ptvqkWLFvrHP/6hhx56SJUrV9aBAwe0fPly/fDDD5Kkl19+WevXr1fTpk01cOBA1a5dW2fOnNH27dv11VdfWZdYupojR46oVq1a6tu371VvIL5gwQIZY3TnnXd6fLxjx47y8fHR/Pnz1bRpU0mXB1oRERFq0KCB+vXrp3Llymn37t2aMWOGypYtm+Gmd5MmTVJMTIz+85//aOnSpercubNKlCihgwcPatGiRdqzZ4969ep11f507dpV//vf/2SMsVbPHD9+XLfccot69+5trepYvXq1VqxYofbt26tr167XfJ0y89prr6lDhw5q1qyZ+vfvr8TERL311lsqVqyYxo0bl+nzSpUqpaeffloTJ05U586d1bFjR+3YsUMrV670uFT7xIkT+vHHH/XYY49dd1sBAACuV9WqVbVgwQL17NlTtWrVUp8+fVS3bl0lJydry5YtWrRokfr16yfp8rm5r6+vunTpokGDBun8+fOaOXOmSpcufc3JNFlVqFAhvfLKK4qMjFTr1q3Vu3dvxcbGaurUqQoPD7cue5VTbrvtNpUoUUJ9+/bVf/7zH9lsNn344Yc5dimm+vXrq2/fvpoxY4Z1Gaxt27bp/fffV7du3XT77bdb2w4YMEAPP/yw7rrrLv3rX//Szp07tXr16gznmLVr11abNm3UsGFDBQcH67vvvtPixYs1ePDgG9LmcePG6csvv1Tz5s31yCOPKDU1VW+//bbq1q1rjWdc3n77bY0fP17r16/P8g3E0+vQoYPq1q2ryZMn67HHHlOhQoU8bte1a9drnv/v3LlT9957rzp06KCWLVsqODhYR44c0fvvv6+jR49qypQpGS7HlXaMnNZtt92WYfyavj3PP/+8Nm7cqHbt2lnxnj17KiAgQLfddptKly6tX375RTNmzFBgYKBefvnlq7b/ah566CG9++676tevn77//nuFh4dr8eLF2rx5s6ZMmWKNnz0ZP368Vq1apZYtW+rRRx+1CiJ16tTRjz/+mGF71+WR094fE0ABYgDAi8yZM8dIcvvn7+9vGjRoYKZNm2acTqe17f79+40k89prr3ncV0xMjOnTp48JDQ01hQoVMmFhYaZz585m8eLF1jYXL140Tz31lClbtqwJCAgwzZs3N1FRUaZ169amdevW1nbvvvuuadWqlSlZsqTx8/MzVatWNc8884yJi4tzO+bzzz9vwsLCjN1uN5LM/v37r9nntWvXmq5du5rSpUsbHx8fU6pUKdOlSxfz2WefWds4nU7z0ksvmUqVKhk/Pz9zyy23mC+++ML07dvXVKpUKUuviSQzduxYt9jPP/9s/v3vf5vixYsbf39/U6NGDTN69Gi3bWJjY81jjz1mKlSoYAoVKmRCQ0PNP//5TzNjxgxrm/Xr1xtJZtGiRRmO62pT3759r/o61KtXz1SsWPGq27Rp08aULl3aXLp0yYp98803pnPnzqZEiRLGx8fHhIWFmQEDBpjDhw973EdKSop57733TMuWLU2xYsVMoUKFTKVKlUxkZKTZsWPHVY9vjDHbt283ksz//d//WbE//vjD3H///aZatWomMDDQ+Pn5mTp16piXXnrJJCcnX3Ofxlz9Nfzqq69M8+bNTUBAgClatKjp0qWL+eWXX9y2cX120r7nUlNTzfjx4633d5s2bczPP/9sKlWqlCEf06ZNM4GBgSY+Pj5L7QUAAMgJv/76qxk4cKAJDw83vr6+pkiRIqZ58+bmrbfeMhcvXrS2+/zzz83NN99s/P39TXh4uHnllVfM7NmzM5wPpT+vd52bzpkzx4q5zqO+/fbbDO1ZuHChueWWW4yfn58JDg429913X4bzzL59+5rChQt77I8k89hjj2WIpz8f83Qut3nzZnPrrbeagIAAU65cOTNs2DCzevVqI8msX7/e8wuYhT4Zc/l1qVOnjlvs0qVLZvz48aZy5cqmUKFCpkKFCmbEiBFur7sxl88xn332WRMSEmICAwNNRESE2bdvX4Y+vfDCC6ZJkyamePHiJiAgwNSsWdO8+OKLbufHY8eONen/dJXV18yYy2OpW265xfj6+pqqVaua9957zzz11FPG39/fbTvXca71urmO06lTJ4+PzZ071+39c7Vz+LTSv96xsbHm5ZdfNq1btzZly5Y1Pj4+pkSJEuaOO+5wG68a43mMnPZf2vdyZm6++WbTv39/t9jUqVNNkyZNTHBwsPHx8TFly5Y1999/v4mOjr7m/lwye61iY2NNZGSkCQkJMb6+vqZevXoe2+lpjLpx40bTsGFD4+vra6pUqWKmT5/u8X1y9uxZ4+vra957770stxeAd7EZk0t3VgIA4G/in//8p8qVK6cPP/wwr5tyw9xyyy1q06aN3njjjbxuCgAAAHDdunXrpl27dnm8V8Pf1YcffqjHHntMBw8eVPHixfO6OTfElClT9OqrryomJkYBAQF53RwAOYB7bAAAcIO99NJLWrhwoX7//fe8bsoNsWrVKkVHR2e4dBcAAACQnyUmJrr9HB0drRUrVlz35aYKqvvuu08VK1bUO++8k9dNuSEuXbqkyZMna9SoURQ1gAKMFRsAAAAAAAAocMqWLat+/fqpSpUq+v333zVt2jQlJSVpx44dql69el43DwDwF3DzcAAAAAAAABQ47du310cffaTjx4/Lz89PzZo100svvURRAwAKgGxfiurrr79Wly5dVK5cOdlsNn366afXfM6GDRv0j3/8Q35+fqpWrZrmzp17HU0FAAAAUFC98847Cg8Pl7+/v5o2bapt27ZddftFixapZs2a8vf3V7169bRixYpcaikAwFvMmTNHBw4c0MWLFxUXF6dVq1bpH//4R143CwBwA2S7sJGQkKD69etn+bp7+/fvV6dOnXT77bfrhx9+0JAhQzRgwACtXr06240FAAAAUPAsXLhQQ4cO1dixY7V9+3bVr19fEREROnHihMftt2zZot69e6t///7asWOHunXrpm7duunnn3/O5ZYDAAAAyAt/6R4bNptN//vf/9StW7dMt3n22We1fPlyt0FGr169dPbsWa1atep6Dw0AAACggGjatKkaN26st99+W5LkdDpVoUIFPf744xo+fHiG7Xv27KmEhAR98cUXVuzWW29VgwYNNH369FxrNwAAAIC8keP32IiKilLbtm3dYhERERoyZEimz0lKSlJSUpL1s9Pp1JkzZ1SyZEnZbLacaioAAACAbDDG6Ny5cypXrpzs9mwvBpckJScn6/vvv9eIESOsmN1uV9u2bRUVFeXxOVFRURo6dKhbLCIiItPL5DK+AAAAAPK/7Iwvcrywcfz4cZUpU8YtVqZMGcXHxysxMVEBAQEZnjNx4kSNHz8+p5sGAAAA4AY4dOiQypcvf13PPXXqlFJTUz2OGfbs2ePxOZmNMY4fP+5xe8YXAAAAgPfIyvgixwsb12PEiBFuM7Di4uJUsWJFHThwQEWLFpV0+TJYdrtdTqdTaa+mlVncbrfLZrNlGk9NTXVrg6si5HQ6sxR3OBwyxrjFXW3JLJ7Vtv+VPiUkJKhChQqSpCNHjqhw4cJe36estL0g9+ncuXNWTg8dOqSiRYt6fZ8KYp6y06fz588rLCxM0uWcBgUFeX2fCmKestOnxMRElStXTtLlnBYuXNjr+1QQ85SdeGJiotvntHDhwl7fp4KYp+z0Ke05UtqcenOfrtb2nOpTfHy8wsPDVaRIEeVn+Xl88dro1X8ex73Nxtgkmb8UN5JkbJLNyOa2rSTZZLO5X5X4+uKe2p5ZPP/0adgLHXLsM/T6mNXkKQ/69PSEiBz7rnt9zGrylMt9enpCxJ/b5Mzv2dfHrM71Pl1vPD/nKTvxZ56PsGI58TvXldPc7FNBzFN2+jTshYgcPW99ddTKXO9TQcxTdto+/KWOXjW+yPHCRmhoqGJjY91isbGxKlq0qMfVGpLk5+cnPz+/DPESJUpYAw9kja+vr/X/xYoVcxu0wzv5+Fz52JYoUYKcFgAOh8P6f3JaMBQqVMj6f3JaMKT9fUpOCwZyemO4fof9lcs5hYSEyOFweBwzhIaGenxOZmOMzLbPz+MLfz/ee3mhWLFiObZvP19ymhdKlCiRY/smp7kvJ/MpkdO8QE4LnuLFi+fo/slp7svr82Ipe+OL67sQbjY0a9ZMa9eudYutWbNGzZo1y+lDAwAAAMjnfH191bBhQ7cxg9Pp1Nq1azMdMzDGAAAAAP7esr1i4/z589q3b5/18/79+/XDDz8oODhYFStW1IgRI3TkyBF98MEHkqSHH35Yb7/9toYNG6YHH3xQ69at0yeffKLly5ffuF7kos1d78rrJmRLYkqK9f9R99yrAJ98efWxq2r+2ZK8bgK8iLd9RiU+pwAADB06VH379lWjRo3UpEkTTZkyRQkJCYqMjJQk9enTR2FhYZo4caIk6YknnlDr1q01adIkderUSR9//LG+++47zZgxIy+7AQAAACCXZPuvZ999951uv/1262fXtWr79u2ruXPn6tixYzp48KD1eOXKlbV8+XI9+eSTmjp1qsqXL6/33ntPERERGfYN5AVv+0M4fwTH3423fUYlPqcAkF09e/bUyZMnNWbMGB0/flwNGjTQqlWrrBuEHzx40LqOryTddtttWrBggUaNGqXnnntO1atX16effqq6devmVRcAAAAA5KJs/6WlTZs2bjeASW/u3Lken7Njx47sHgoAAABegAJk3ihoBcjBgwdr8ODBHh/bsGFDhliPHj3Uo0ePHG4VAAAAgPwox++xAQAAAAAAAAAAcKN439Q0AADg1ZjdnzcK2ux+AAAAAMDfl/eNypEtAT4+2nRn97xuBgAAAAAAAAAANwSFDQDIYxQgAQAAAAAAgKyjsAF4Gf4IDuR/fE4BAAAAAAByDjcPBwAAAAAAAAAAXoMVGwAAAPjbYWUVAAAAAHgvVmwAAAAAAAAAAACvQWEDAAAAAAAAAAB4DQobAAAAAAAAAADAa3CPDQAAgGvgfgwAAAAAAOQfrNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAACBPvfPOOwoPD5e/v7+aNm2qbdu2Zbrtrl27dNdddyk8PFw2m01TpkzJvYYCAAAAyBcobAAAAADIMwsXLtTQoUM1duxYbd++XfXr11dERIROnDjhcfsLFy6oSpUqevnllxUaGprLrQUAAACQH/jkdQMAAAAA/H1NnjxZAwcOVGRkpCRp+vTpWr58uWbPnq3hw4dn2L5x48Zq3LixJHl83JOkpCQlJSVZP8fHx0uSUlNTlZqaKkmy2Wyy2+1yOp0yxljbZha32+2y2WyZxl37TRuXJKfTma515s/jpIsamyTzl+JGkoxNshnZ3LaVJJtsNpN2F9cZ99T2zOL5qU8mW3nKLO5wOGSMcYvbbIY85UGfUlNTs5mny5/tzOJpP9s2myFPudyn9N/NWclTduKutpKn3OtT2u/bnPidm7Zf5Cl3+pRT50aueMbXgDzlfJ+UQ+dGWf8uT3/8q6GwAQAAACBPJCcn6/vvv9eIESOsmN1uV9u2bRUVFXXDjjNx4kSNHz8+QzwmJkZBQUGSpGLFiqls2bKKjY1VXFyctU1ISIhCQkJ05MgRJSQkWPHQ0FAVL15cBw4cUHJyshUvX768goKCFBMT4zZwq1y5snx8fBQdHe3WBptNcvhIJcul+YOM06YThyRff6lEmSvxlGSbTh+TAoKkoiWvxJMTbfrjhFS4mBRU/Eo88ZxN8WekoiWkgCJX4ufP2pQQJxUvJfkGXInHn7Yp8bwUHCr5+F6J/xFrU/JFqVR5yWa/Ej991KbUFKl0RfeB8YmDtnzfp4SEhGzlqXr16kpJSdH+/futmN1u10033aSEhAQdPnzYigeHijzlQZ+io6OzlSdfX19VqVJFcXFxOn78uBUvXLiwKlSooDNnzujUqVOSLh+HPOVun1yfwezkScr6d7mrTeQp9/qU9ns1J37npm0/ecqdPuXUuZHruzxtX8lT7vRJUo6cG2Xnu/z8+fPKKptJW0LLp+Lj41WsWDHFxcWpaNGiedqWzV3vytPj/x01/2xJju6fnOa+nMwp+cwb5LTgIacFDzkteHL6HCkr/up5+tGjRxUWFqYtW7aoWbNmVnzYsGHauHGjtm7detXnh4eHa8iQIRoyZMhVt/O0YsM1iHK1O69WbLw4bPmfx3Fvc/6bwVewZiWOeq1Ljq3YeOnZ5eQpD/r03Cudcmz26EvPLidPudyn517p9Oc2ObNi46Vnl+d6n643np/zlJ34yFc7WbGc+J3rymlu9qkg5ik7fRr1WqccXbHxwjPLcr1PBTFP2Wn7mEl35vmKjfj4eAUHB2dpfMGKDQAAAAAFmp+fn/z8/DLEHQ6HHA6HW8w1SEsvu/H0+808fnkI6nm6me3GxI1NHsPG5iF6PXGP4XzdJ9uffxHIep4yj9tsNrf4lWORp9zsU9ocZCVP14qn/Wy7HZ885Uqf0uckK3nKTjx9W8lTzvfpRuYvvct/SM3YL/KUs33KuXOjPw/r8TUgTzndp5w4N7pWPO17JrPjeMLNwwEAAADkiZCQEDkcDsXGxrrFY2NjuTE4AAAAgExR2AAAAACQJ3x9fdWwYUOtXbvWijmdTq1du9bt0lQAAAAAkBaXogIAAACQZ4YOHaq+ffuqUaNGatKkiaZMmaKEhARFRkZKkvr06aOwsDBNnDhR0uUbjv/yyy/W/x85ckQ//PCDgoKCVK1atTzrBwAAAIDcQ2EDAAAAQJ7p2bOnTp48qTFjxuj48eNq0KCBVq1apTJlykiSDh486Hbd3aNHj+qWW26xfn799df1+uuvq3Xr1tqwYUNuNx8AAABAHqCwAQAAACBPDR48WIMHD/b4WPpiRXh4uExmd2QEAAAA8LfAPTYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeI3rKmy88847Cg8Pl7+/v5o2bapt27Zluu2lS5c0YcIEVa1aVf7+/qpfv75WrVp13Q0GAAAAAAAAAAB/X9kubCxcuFBDhw7V2LFjtX37dtWvX18RERE6ceKEx+1HjRqld999V2+99ZZ++eUXPfzww/r3v/+tHTt2/OXGAwAAAAAAAACAv5dsFzYmT56sgQMHKjIyUrVr19b06dMVGBio2bNne9z+ww8/1HPPPaeOHTuqSpUqeuSRR9SxY0dNmjTpLzceAAAAAAAAAAD8vfhkZ+Pk5GR9//33GjFihBWz2+1q27atoqKiPD4nKSlJ/v7+brGAgABt2rQp0+MkJSUpKSnJ+jk+Pl6SlJqaqtTUVEmSzWaT3W6X0+mUMcbaNrO43W6XzWbLNO7ab9q4JDmdTre4ubKBW9zmdF5+zFPcZpNstmvHjZHNmMzj6fadadzplE3yGPfU9szi+aVPqamp2c5TZnGHwyFjjFvc2O3kKZf75MpjdvLk+mxnFnd9tq1jk6dc7VP67+Zr5Slb8bTHJU+51qe037c3+ncuecqbPuXUuZHdbs+87Tncp4KYp+z0SVKOnBtl57s8/fEBAAAAIDdkq7Bx6tQppaamqkyZMm7xMmXKaM+ePR6fExERocmTJ6tVq1aqWrWq1q5dq6VLl151EDRx4kSNHz8+QzwmJkZBQUGSpGLFiqls2bKKjY1VXFyctU1ISIhCQkJ05MgRJSQkWPHQ0FAVL15cBw4cUHJyshUvX768goKCFBMT4zZwq1y5snx8fBQdHe3eCLtdxtdXibVqWCFbaqoCf/xZziJFdLFalSubXryogN17lRpcQkkVK1hxR/w5+cf8pktlSutS2VAr7nPqtPwOHVZy+TClhJS04oWOHZfv8VglVQ5XatEiVtzv4CH5nD6jizWqy5mmeOS/7zc5zp1TYt3aMg6HFQ/YvVe25GRdqF/PrUuBO3/K132Kjo7Odp6qV6+ulJQU7d+//0rb7XbddNNNSkhI0OHDh634xRrVyVMu98mVr+zkydfXV1WqVFFcXJyOHz9uxQsXLqwKFSrozJkzOnXqlHUM8pS7fXLlNKt5csnKd3na9pCn3OtT2u/VG/07lzzlTZ9y6tyoevXqMv7+5CkP+iQpR86NsvNdfv78eQEAAABAbrOZtFP0ruHo0aMKCwvTli1b1KxZMys+bNgwbdy4UVu3bs3wnJMnT2rgwIFatmyZbDabqlatqrZt22r27NlKTEz0eBxPKzZcg6iiRYtebngerdiI6n6PawO3eH6bwVeQZiU2W/xxjq7YiLq7F3nK5T41W/zxnz/e+NmjUXf3ypM+XSvujXnKTp9cOc2JFRtb7uqZJ33KEC8AecpOn5otWXgldoN/5276d4886VNBzFN2+tTsf4tybMXG5m53k6c86FOLz5bk+YqN+Ph4BQcHKy4uzjpP9wbx8fEqVqxYvmj3hKeW5enx/67GTOqSY/smp3mDnBYsOZlPiZzmBXJa8JDTgienc5oV2TlPz9aKjZCQEDkcDsXGxrrFY2NjFRoa6vE5pUqV0qeffqqLFy/q9OnTKleunIYPH64qVap43F6S/Pz85OfnlyHucDjkSDPLTroySEsvu/H0+80sbg0/0w0Krcc8xY2RPNSPsh33sO/riXtqY2bx/NCntDnIap6uFrfZbG5x17HIU+71KX1espKna8Vdn+30xyZPudOnDN+V18hTduKe2kmePLcxs/j19OlG5c8T8uQ5ntN9yqlzI+kqbc8sTp5uWJ9y4tzoWvG075nMjgMAAAAAOcnzSDYTvr6+atiwodauXWvFnE6n1q5d67aCwxN/f3+FhYUpJSVFS5YsUdeuXa+vxQAAAAAAAAAA4G8rWys2JGno0KHq27evGjVqpCZNmmjKlClKSEhQZGSkJKlPnz4KCwvTxIkTJUlbt27VkSNH1KBBAx05ckTjxo2T0+nUsGHDbmxPAAAAAAAAAABAgZftwkbPnj118uRJjRkzRsePH1eDBg20atUq64biBw8edFuefvHiRY0aNUq//fabgoKC1LFjR3344YcqXrz4DesEAAAAAAAAAAD4e8h2YUOSBg8erMGDB3t8bMOGDW4/t27dWr/88sv1HAYAAAAAAAAAAMBNtu6xAQAAAAAAAAAAkJcobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAgDz3zjvvKDw8XP7+/mratKm2bduW6bYzZ85Uy5YtVaJECZUoUUJt27a96vYAAAAAChYKGwAAAADy1MKFCzV06FCNHTtW27dvV/369RUREaETJ0543H7Dhg3q3bu31q9fr6ioKFWoUEHt2rXTkSNHcrnlAAAAAPIChQ0AAAAAeWry5MkaOHCgIiMjVbt2bU2fPl2BgYGaPXu2x+3nz5+vRx99VA0aNFDNmjX13nvvyel0au3atbnccgAAAAB5wSevGwAAAADg7ys5OVnff/+9RowYYcXsdrvatm2rqKioLO3jwoULunTpkoKDgz0+npSUpKSkJOvn+Ph4SVJqaqpSU1MlSTabTXa7XU6nU8YYa9vM4na7XTabLdO4a79p45LkdDrTtc78eZx0UWOTZP5S3EiSsUk2I5vbtpJkk81m0u7iOuOe2p5ZPD/1yWQrT5nFHQ6HjDFucZvNkKc86FNqamo283T5s51ZPO1n22Yz5CmX+5T+uzkrecpO3NVW8pR7fUr7fZsTv3PT9os85U6fcurcyBXP+BqQp5zvk3Lo3Cjr3+Xpj381FDYAAAAA5JlTp04pNTVVZcqUcYuXKVNGe/bsydI+nn32WZUrV05t27b1+PjEiRM1fvz4DPGYmBgFBQVJkooVK6ayZcsqNjZWcXFx1jYhISEKCQnRkSNHlJCQYMVDQ0NVvHhxHThwQMnJyVa8fPnyCgoKUkxMjNvArXLlyvLx8VF0dLRbG2w2yeEjlSyX5g8yTptOHJJ8/aUSZa7EU5JtOn1MCgiSipa8Ek9OtOmPE1LhYlJQ8SvxxHM2xZ+RipaQAopciZ8/a1NCnFS8lOQbcCUef9qmxPNScKjk43sl/kesTckXpVLlJZv9Svz0UZtSU6TSFd0HxicO2vJ9nxISErKVp+rVqyslJUX79++3Yna7XTfddJMSEhJ0+PBhKx4cKvKUB32Kjo7OVp58fX1VpUoVxcXF6fjx41a8cOHCqlChgs6cOaNTp05Junwc8pS7fXJ9BrOTJynr3+WuNpGn3OtT2u/VnPidm7b95Cl3+pRT50au7/K0fSVPudMnSTlybpSd7/Lz588rq2wmbQktn4qPj1exYsUUFxenokWL5mlbNne9K0+P/3fU/LMlObp/cpr7cjKn5DNvkNOCh5wWPOS04Mnpc6SsuBHn6UePHlVYWJi2bNmiZs2aWfFhw4Zp48aN2rp161Wf//LLL+vVV1/Vhg0bdPPNN3vcxtOKDdcgytXuvFqx8eKw5X8ex73N+W8GX8GalTjqtS45tmLjpWeXk6c86NNzr3TKsdmjLz27nDzlcp+ee6XTn9vkzIqNl55dnut9ut54fs5TduIjX+1kxXLid64rp7nZp4KYp+z0adRrnXJ0xcYLzyzL9T4VxDxlp+1jJt2Z5ys24uPjFRwcnKXxBSs2AAAAAOSZkJAQORwOxcbGusVjY2MVGhp61ee+/vrrevnll/XVV19lWtSQJD8/P/n5+WWIOxwOORwOt5hrkJZeduPp95t5/PIQ1PN0M9uNiRubPIaNzUP0euIew/m6T7Y//yKQ9TxlHrfZbG7xK8ciT7nZp7Q5yEqerhVP+9l2Oz55ypU+pc9JVvKUnXj6tpKnnO/Tjcxfepf/kJqxX+QpZ/uUc+dGfx7W42tAnnK6TzlxbnSteNr3TGbH8YSbhwMAAADIM76+vmrYsKHbjb9dNwJPu4IjvVdffVXPP/+8Vq1apUaNGuVGUwEAAADkE6zYAAAAAJCnhg4dqr59+6pRo0Zq0qSJpkyZooSEBEVGRkqS+vTpo7CwME2cOFGS9Morr2jMmDFasGCBwsPDrWv1BgUFWffMAAAAAFBwUdgAAAAAkKd69uypkydPasyYMTp+/LgaNGigVatWWTcUP3jwoNsS9WnTpik5OVl33323237Gjh2rcePG5WbTAQAAAOQBChsAAAAA8tzgwYM1ePBgj49t2LDB7ecDBw7kfIMAAAAA5FvcYwMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAAAAAK9BYQMAAAAAAAAAAHgNChsAAAAAAAAAAMBrUNgAAAAAAAAAAABeg8IGAAAAAAAAAADwGhQ2AAAAAAAAAACA16CwAQAAAAAAAAAAvMZ1FTbeeecdhYeHy9/fX02bNtW2bduuuv2UKVNUo0YNBQQEqEKFCnryySd18eLF62owAAAAAAAAAAD4+8p2YWPhwoUaOnSoxo4dq+3bt6t+/fqKiIjQiRMnPG6/YMECDR8+XGPHjtXu3bs1a9YsLVy4UM8999xfbjwAAAAAAAAAAPh7yXZhY/LkyRo4cKAiIyNVu3ZtTZ8+XYGBgZo9e7bH7bds2aLmzZvr3nvvVXh4uNq1a6fevXtfc5UHAAAAAAAAAABAej7Z2Tg5OVnff/+9RowYYcXsdrvatm2rqKgoj8+57bbbNG/ePG3btk1NmjTRb7/9phUrVuiBBx7I9DhJSUlKSkqyfo6Pj5ckpaamKjU1VZJks9lkt9vldDpljLG2zSxut9tls9kyjbv2mzYuSU6n0y1urmzgFrc5nZcf8xS32SSb7dpxY2QzJvN4un1nGnc6ZZM8xj21PbN4fulTampqtvOUWdzhcMgY4xY3djt5yuU+ufKYnTy5PtuZxV2fbevY5ClX+5T+u/laecpWPO1xyVOu9Snt9+2N/p1LnvKmTzl1bmS32zNvew73qSDmKTt9kpQj50bZ+S5Pf3wAAAAAyA3ZKmycOnVKqampKlOmjFu8TJky2rNnj8fn3HvvvTp16pRatGghY4xSUlL08MMPX/VSVBMnTtT48eMzxGNiYhQUFCRJKlasmMqWLavY2FjFxcVZ24SEhCgkJERHjhxRQkKCFQ8NDVXx4sV14MABJScnW/Hy5csrKChIMTExbgO3ypUry8fHR9HR0e6NsNtlfH2VWKuGFbKlpirwx5/lLFJEF6tVubLpxYsK2L1XqcEllFSxghV3xJ+Tf8xvulSmtC6VDbXiPqdOy+/QYSWXD1NKSEkrXujYcfkej1VS5XClFi1ixf0OHpLP6TO6WKO6nP7+Vtx/329ynDunxLq1ZRwOKx6we69sycm6UL+eW5cCd/6Ur/sUHR2d7TxVr15dKSkp2r9//5W22+266aablJCQoMOHD1vxizWqk6dc7pMrX9nJk6+vr6pUqaK4uDgdP37cihcuXFgVKlTQmTNndOrUKesY5Cl3++TKaVbz5JKV7/K07SFPudentN+rN/p3LnnKmz7l1LlR9erVZfz9yVMe9ElSjpwbZee7/Pz58wIAAACA3GYzaafoXcPRo0cVFhamLVu2qFmzZlZ82LBh2rhxo7Zu3ZrhORs2bFCvXr30wgsvqGnTptq3b5+eeOIJDRw4UKNHj/Z4HE8rNlyDqKJFi15ueB6t2Ijqfo9rA7d4fpvBV5BmJTZb/HGOrtiIursXecrlPjVb/PGfP9742aNRd/fKkz5dK+6NecpOn1w5zYkVG1vu6pknfcoQLwB5yk6fmi1ZeCV2g3/nbvp3jzzpU0HMU3b61Ox/i3JsxcbmbneTpzzoU4vPluT5io34+HgFBwcrLi7OOk/3BvHx8SpWrFi+aPeEp5bl6fH/rsZM6pJj+yaneYOcFiw5mU+JnOYFclrwkNOCJ6dzmhXZOU/P1oqNkJAQORwOxcbGusVjY2MVGhrq8TmjR4/WAw88oAEDBkiS6tWrp4SEBD300EMaOXKkNchKy8/PT35+fhniDodDjjSz7CR5fP71xNPvN7O4NfxMNyi0HvMUN0byUD/KdtzDvq8n7qmNmcXzQ5/S5iCrebpa3GazucVdxyJPuden9HnJSp6uFXd9ttMfmzzlTp8yfFdeI0/ZiXtqJ3ny3MbM4tfTpxuVP0/Ik+d4Tvcpp86NpKu0PbM4ebphfcqJc6NrxdO+ZzI7DgAAAADkJM8j2Uz4+vqqYcOGWrt2rRVzOp1au3at2wqOtC5cuJBhwOwaAGVjsQgAAAAAAAAAAED2VmxI0tChQ9W3b181atRITZo00ZQpU5SQkKDIyEhJUp8+fRQWFqaJEydKkrp06aLJkyfrlltusS5FNXr0aHXp0oUZXgAAAAAAAAAAIFuyXdjo2bOnTp48qTFjxuj48eNq0KCBVq1aZd1Q/ODBg24rNEaNGiWbzaZRo0bpyJEjKlWqlLp06aIXX3zxxvUCAAAAAAAAAAD8LWS7sCFJgwcP1uDBgz0+tmHDBvcD+Pho7NixGjt27PUcCgAAAAAAAAAAwJKte2wAAAAAAAAAAADkJQobAAAAAAAAAADAa1DYAAAAAAAAAAAAXoPCBgAAAAAAAAAA8BoUNgAAAAAAAAAAgNegsAEAAAAAAAAAALwGhQ0AAAAAAAAAAOA1KGwAAAAAAAAAAACvQWEDAAAAAAAAAAB4DQobAAAAAAAAAADAa1DYAAAAAAAAAAAAXoPCBgAAAAAAAAAA8BoUNgAAAAAAAAAAgNegsAEAAAAAAAAAALwGhQ0AAAAAAAAAAOA1KGwAAAAAAAAAAACvQWEDAAAAAAAAAAB4DQobAAAAAAAAAADAa1DYAAAAAAAAAAAAXoPCBgAAAAAAAAAA8BoUNgAAAAAAAAAAgNegsAEAAAAAAAAAALwGhQ0AAAAAAAAAAOA1KGwAAAAAAAAAAACvQWEDAAAAAAAAAAB4DQobAAAAAAAAAADAa1DYAAAAAAAAAAAAXoPCBgAAAAAAAAAA8BoUNgAAAAAAAAAAgNegsAEAAAAAAAAAALwGhQ0AAAAAAAAAAOA1KGwAAAAAAAAAAACvQWEDAAAAAAAAAAB4DQobAAAAAAAAAADAa1DYAAAAAAAAAAAAXoPCBgAAAAAAAAAA8BoUNgAAAAAAAAAAgNegsAEAAAAgz73zzjsKDw+Xv7+/mjZtqm3btmW67dKlS9WoUSMVL15chQsXVoMGDfThhx/mYmsBAAAA5CUKGwAAAADy1MKFCzV06FCNHTtW27dvV/369RUREaETJ0543D44OFgjR45UVFSUfvzxR0VGRioyMlKrV6/O5ZYDAAAAyAs+ed0AAAAAAH9vkydP1sCBAxUZGSlJmj59upYvX67Zs2dr+PDhGbZv06aN289PPPGE3n//fW3atEkREREZtk9KSlJSUpL1c3x8vCQpNTVVqampkiSbzSa73S6n0yljjLVtZnG73S6bzZZp3LXftHFJcjqd6Vpn/jxOuqixSTJ/KW4kydgkm5HNbVtJsslmM2l3cZ1xT23PLJ6f+mSylafM4g6HQ8YYt7jNZshTHvQpNTU1m3m6/NnOLJ72s22zGfKUy31K/92clTxlJ+5qK3nKvT6l/b7Nid+5aftFnnKnTzl1buSKZ3wNyFPO90k5dG6U9e/y9Me/GgobAAAAAPJMcnKyvv/+e40YMcKK2e12tW3bVlFRUdd8vjFG69at0969e/XKK6943GbixIkaP358hnhMTIyCgoIkScWKFVPZsmUVGxuruLg4a5uQkBCFhIToyJEjSkhIsOKhoaEqXry4Dhw4oOTkZCtevnx5BQUFKSYmxm3gVrlyZfn4+Cg6OtqtDTab5PCRSpZL8wcZp00nDkm+/lKJMlfiKck2nT4mBQRJRUteiScn2vTHCalwMSmo+JV44jmb4s9IRUtIAUWuxM+ftSkhTipeSvINuBKPP21T4nkpOFTy8b0S/yPWpuSLUqnyks1+JX76qE2pKVLpiu4D4xMHbfm+TwkJCdnKU/Xq1ZWSkqL9+/dbMbvdrptuukkJCQk6fPiwFQ8OFXnKgz5FR0dnK0++vr6qUqWK4uLidPz4cSteuHBhVahQQWfOnNGpU6ckXT4OecrdPrk+g9nJk5T173JXm8hT7vUp7fdqTvzOTdt+8pQ7fcqpcyPXd3navpKn3OmTpBw5N8rOd/n58+eVVTaTtoSWT8XHx6tYsWKKi4tT0aJF87Qtm7velafH/ztq/tmSHN0/Oc19OZlT8pk3yGnBQ04LHnJa8OT0OVJW3Ijz9KNHjyosLExbtmxRs2bNrPiwYcO0ceNGbd261ePz4uLiFBYWpqSkJDkcDv33v//Vgw8+6HFbTys2XIMoV7vzasXGi8OW/3kc9zbnvxl8BWtW4qjXuuTYio2Xnl1OnvKgT8+90inHZo++9Oxy8pTLfXrulU5/bpMzKzZeenZ5rvfpeuP5OU/ZiY98tZMVy4nfua6c5mafCmKestOnUa91ytEVGy88syzX+1QQ85Sdto+ZdGeer9iIj49XcHBwlsYXrNgAAAAA4HWKFCmiH374QefPn9fatWs1dOhQValSRW3atMmwrZ+fn/z8/DLEHQ6HHA6HW8w1SEsvu/H0+808fnkI6nm6me3GxI1NHsPG5iF6PXGP4XzdJ9uffxHIep4yj9tsNrf4lWORp9zsU9ocZCVP14qn/Wy7HZ885Uqf0uckK3nKTjx9W8lTzvfpRuYvvct/SM3YL/KUs33KuXOjPw/r8TUgTzndp5w4N7pWPO17JrPjeEJhAwAAAECeCQkJkcPhUGxsrFs8NjZWoaGhmT7PbrerWrVqkqQGDRpo9+7dmjhxosfCBgAAAICCxXMJDQAAAAByga+vrxo2bKi1a9daMafTqbVr17pdmupanE6n2+WmAAAAABRcrNgAAAAAkKeGDh2qvn37qlGjRmrSpImmTJmihIQERUZGSpL69OmjsLAwTZw4UdLlm4E3atRIVatWVVJSklasWKEPP/xQ06ZNy8tuAAAAAMglFDYAAAAA5KmePXvq5MmTGjNmjI4fP64GDRpo1apVKlOmjCTp4MGDbtfeTUhI0KOPPqrDhw8rICBANWvW1Lx589SzZ8+86gIAAACAXERhAwAAAECeGzx4sAYPHuzxsQ0bNrj9/MILL+iFF17IhVYBAAAAyI+4xwYAAAAAAAAAAPAaFDYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeA0KGwAAAAAAAAAAwGtQ2AAAAAAAAAAAAF6DwgYAAAAAAAAAAPAaFDYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeA0KGwAAAAAAAAAAwGtQ2AAAAAAAAAAAAF6DwgYAAAAAAAAAAPAaFDYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeA0KGwAAAAAAAAAAwGtQ2AAAAAAAAAAAAF6DwgYAAAAAAAAAAPAaFDYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeA0KGwAAAAAAAAAAwGtQ2AAAAAAAAAAAAF6DwgYAAAAAAAAAAPAaFDYAAAAAAAAAAIDXoLABAAAAAAAAAAC8BoUNAAAAAAAAAADgNShsAAAAAAAAAAAAr0FhAwAAAAAAAAAAeA0KGwAAAAAAAAAAwGtcV2HjnXfeUXh4uPz9/dW0aVNt27Yt023btGkjm82W4V+nTp2uu9EAAAAAAAAAAODvKduFjYULF2ro0KEaO3astm/frvr16ysiIkInTpzwuP3SpUt17Ngx69/PP/8sh8OhHj16/OXGAwAAAAAAAACAvxef7D5h8uTJGjhwoCIjIyVJ06dP1/LlyzV79mwNHz48w/bBwcFuP3/88ccKDAy8amEjKSlJSUlJ1s/x8fGSpNTUVKWmpkqSbDab7Ha7nE6njDHWtpnF7Xa7bDZbpnHXftPGJcnpdLrFzZUN3OI2p/PyY57iNptks107boxsxmQeT7fvTONOp2ySx7intmcWzy99Sk1NzXaeMos7HA4ZY9zixm4nT7ncJ1ces5Mn12c7s7jrs20dmzzlap/SfzdfK0/Ziqc9LnnKtT6l/b690b9zyVPe9Cmnzo3sdnvmbc/hPhXEPGWnT5Jy5NwoO9/l6Y8PAAAAALkhW4WN5ORkff/99xoxYoQVs9vtatu2raKiorK0j1mzZqlXr14qXLhwpttMnDhR48ePzxCPiYlRUFCQJKlYsWIqW7asYmNjFRcXZ20TEhKikJAQHTlyRAkJCVY8NDRUxYsX14EDB5ScnGzFy5cvr6CgIMXExLgN3CpXriwfHx9FR0e7N8Jul/H1VWKtGlbIlpqqwB9/lrNIEV2sVuXKphcvKmD3XqUGl1BSxQpW3BF/Tv4xv+lSmdK6VDbUivucOi2/Q4eVXD5MKSElrXihY8flezxWSZXDlVq0iBX3O3hIPqfP6GKN6nL6+1tx/32/yXHunBLr1pZxOKx4wO69siUn60L9em5dCtz5U77uU3R0dLbzVL16daWkpGj//v1X2m6366abblJCQoIOHz5sxS/WqE6ecrlPrnxlJ0++vr6qUqWK4uLidPz4cSteuHBhVahQQWfOnNGpU6esY5Cn3O2TK6dZzZNLVr7L07aHPOVen9J+r97o37nkKW/6lFPnRtWrV5fx9ydPedAnSTlybpSd7/Lz588LAAAAAHKbzaSdoncNR48eVVhYmLZs2aJmzZpZ8WHDhmnjxo3aunXrVZ+/bds2NW3aVFu3blWTJk0y3c7Tig3XIKpo0aKXG55HKzaiut/j2sAtnt9m8BWkWYnNFn+coys2ou7uRZ5yuU/NFn/85483fvZo1N298qRP14p7Y56y0ydXTnNixcaWu3rmSZ8yxAtAnrLTp2ZLFl6J3eDfuZv+nW7VJnnKlT41+9+iHFuxsbnb3eQpD/rU4rMleb5iIz4+XsHBwYqLi7PO071BfHy8ihUrli/aPeGpZXl6/L+rMZO65Ni+yWneIKcFS07mUyKneYGcFjzktODJ6ZxmRXbO07N9Kaq/YtasWapXr95VixqS5OfnJz8/vwxxh8MhR5pZdtKVQVp62Y2n329mcWv4mW5QaD3mKW6M5KF+lO24h31fT9xTGzOL54c+pc1BVvN0tbjNZnOLu45FnnKvT+nzkpU8XSvu+mynPzZ5yp0+ZfiuvEaeshP31E7y5LmNmcWvp083Kn+ekCfP8ZzuU06dG0lXaXtmcfJ0w/qUE+dG14qnfc9kdhwAAAAAyEmeR7KZCAkJkcPhUGxsrFs8NjZWoaGhmTzrsoSEBH388cfq379/9lsJAAAAAAAAAACgbBY2fH191bBhQ61du9aKOZ1OrV271u3SVJ4sWrRISUlJuv/++6+vpQAAAAAAAAAA4G8v25eiGjp0qPr27atGjRqpSZMmmjJlihISEhQZGSlJ6tOnj8LCwjRx4kS3582aNUvdunVTyZIlPe0WAAAAAAAAAADgmrJd2OjZs6dOnjypMWPG6Pjx42rQoIFWrVqlMmXKSJIOHjyY4VrNe/fu1aZNm/Tll1/emFYDAAAAAAAAAIC/peu6efjgwYM1ePBgj49t2LAhQ6xGjRoyHm6OCAAAAAAAAAAAkB3ZuscGAAAAAAAAAABAXqKwAQAAAAAAAAAAvAaFDQAAAAAAAAAA4DUobAAAAAAAAADA/7d33/E1n/0fx9/nJASxIhEhRuwYNWpEbKW1VYcbdZcapTVK1Y2WahVdWlXa0irVoa1WB4qWW2uvWq1blcQMGUYQgkRyrt8f+Z2vHJJWVMaJ1/Px6KPyyfd7vteV64zvdT7XAOA2SGwAAAAAAAAAAAC3QWIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAAAAAAAOA2SGwAAAAAAAAAAAC3QWIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAA2e7dd99VUFCQ8uXLp5CQEG3btu2mzvvyyy9ls9nUtWvXzC0gAAAAgByDxAYAAACAbLVw4UKNHDlSL7zwgnbu3KnatWurbdu2Onny5F+ed+TIEY0aNUrNmjXLopICAAAAyAlIbAAAAADIVtOmTdPjjz+uvn37qnr16po9e7YKFCigefPmpXtOcnKyevXqpYkTJ6pChQpZWFoAAAAA2c0zuwsAAAAA4M6VmJioHTt26Nlnn7Vidrtdbdq00ebNm9M976WXXpK/v7/69++v9evX/+U1EhISlJCQYP0cFxcnKSU5kpycLEmy2Wyy2+1yOBwyxljHphe32+2y2Wzpxp2PmzouSQ6H47rSmf+/znVRY5Nk/lHcSJKxSTYjm8uxkmSTzWZSP8QtxtMqe3rxnFQnk6F2Si/u4eEhY4xL3GYztFM21Ck5OTmD7ZTy2k4vnvq1bbMZ2imL63T9e/PNtFNG4s6y0k5ZV6fU77eZ8Zmbul60U9bUKbPujZzxG/8GtFPm10mZdG908+/l11//r5DYAAAAAJBtTp8+reTkZJUoUcIlXqJECf35559pnrNhwwbNnTtXu3fvvqlrvPLKK5o4ceIN8YMHD6pgwYKSpCJFiqhkyZKKiYnR+fPnrWP8/Pzk5+enEydOKD4+3ooHBASoaNGiOnLkiBITE6146dKlVbBgQR08eNCl41a+fHl5enoqLCzMpQw2m+ThKfmWSvWFjMOmkxFS3nyST4lr8aREm85ESfkLSoV9r8UTL9t09qTkXUQqWPRa/PIFm+JipcI+Uv5C1+IXz9kUf14qWlzKm/9aPO6MTZcvSsUCJM+81+JnY2xKvCIVLy3Z7NfiZyJtSk6S/Mu6doxPHrPl+DrFx8dnqJ0qV66spKQkHT582IrZ7XZVqVJF8fHxOn78uBUvFiDaKRvqFBYWlqF2yps3rypUqKDz588rOjraint7e6tMmTKKjY3V6dOnJaVch3bK2jo5X4MZaSfp5t/LnWWinbKuTqnfVzPjMzd1+WmnrKlTZt0bOd/LU9eVdsqaOknKlHujjLyXX7x4UTfLZlKn0HKouLg4FSlSROfPn1fhwoWztSwb738oW69/J2qy+JtMfXzaNOtlZpvSntmDNs19aNPchzbNfTL7Hulm3I779MjISAUGBmrTpk0KDQ214qNHj9batWu1detWl+MvXLigWrVq6b333lP79u0lSY899pjOnTun77//Ps1rpDVjw9mJcpY7u2ZsTBm97P+v41rmnDeCL3eNShw/tXOmzdh4ecwy2ikb6vTcax0zbfToy2OW0U5ZXKfnXuv4/8dkzoyNl8csy/I63Wo8J7dTRuLjXu9oxTLjM9fZpllZp9zYThmp0/ipHTN1xsbk/yzN8jrlxnbKSNknvNkl22dsxMXFqVixYjfVv2DGBgAAAIBs4+fnJw8PD8XExLjEY2JiFBAQcMPxBw8e1JEjR9S5c2cr5uwgeXp6av/+/apYsaLLOV5eXvLy8rrhsTw8POTh4eESc3bSrpfR+PWPm348pQua9nAz2+2JG5vSDBtbGtFbiacZztF1sv3/NwI3307px202m0v82rVop6ysU+o2uJl2+rt46te2y/Vppyyp0/VtcjPtlJH49WWlnTK/Trez/a6X8kXqjfWinTK3Tpl3b/T/l03zb0A7ZXadMuPe6O/iqZ8z6V0nLWweDgAAACDb5M2bV/Xq1dPq1autmMPh0OrVq11mcDgFBwdrz5492r17t/Vfly5d1KpVK+3evVtlypTJyuIDAAAAyAbM2AAAAACQrUaOHKk+ffqofv36atiwoaZPn674+Hj17dtXktS7d28FBgbqlVdeUb58+VSzZk2X84sWLSpJN8QBAAAA5E4kNgAAAABkq+7du+vUqVOaMGGCoqOjVadOHf3444/WhuLHjh1Ld1kDAAAAAHceEhsAAAAAst3QoUM1dOjQNH+3Zs2avzx3/vz5t79AAAAAAHIshj0BAAAAAAAAAAC3QWIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAAAAAAAOA2SGwAAAAAAAAAAAC3QWIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAAAAAAAOA2PLO7AAAAAAAAAAAAuKvEq1c0Zea/JEnjhn2lvHnyZXOJcj9mbAAAAAAAAAAAALdBYgMAAAAAAAAAALgNEhsAAAAAAAAAAMBtkNgAAAAAAAAAAABug8QGAAAAAAAAAABwGyQ2AAAAAAAAAACA2yCxAQAAAAAAAAAA3AaJDQAAAAAAAAAA4DY8s7sAAAAAAAAAAAA4TXizc3YXIUPi4+M1ZWbKv599pYO8vb2zt0B3AGZsAAAAAAAAAAAAt0FiAwAAAAAAAAAAuA0SGwAAAAAAAAAAwG2Q2AAAAAAAAAAAAG6DxAYAAAAAAAAAAHAbJDYAAAAAAAAAAIDbILEBAAAAAAAAAADchmd2FwAAAAAAAAAAAHfl7e0tY0x2F+OOwowNAAAAAAAAAADgNkhsAAAAAAAAAAAAt0FiAwAAAAAAAAAAuA0SGwAAAAAAAAAAwG2Q2AAAAAAAAAAAAG6DxAYAAAAAAAAAAHAbJDYAAAAAAAAAAIDbILEBAAAAAAAAAADcBokNAAAAAAAAAADgNkhsAAAAAAAAAAAAt0FiAwAAAAAAAAAAuA0SGwAAAAAAAAAAwG2Q2AAAAAAAAAAAAG6DxAYAAAAAAAAAAHAbJDYAAAAAAAAAAIDbILEBAAAAAAAAAADcBokNAAAAAAAAAADgNkhsAAAAAAAAAAAAt0FiAwAAAAAAAAAAuA0SGwAAAAAAAAAAwG2Q2AAAAAAAAAAAAG6DxAYAAAAAAAAAAHAbt5TYePfddxUUFKR8+fIpJCRE27Zt+8vjz507pyFDhqhkyZLy8vJSlSpVtHz58lsqMAAAAAAAAAAAuHN5ZvSEhQsXauTIkZo9e7ZCQkI0ffp0tW3bVvv375e/v/8NxycmJuree++Vv7+/Fi1apMDAQB09elRFixa9HeUHAAAAAAAAAAB3kAwnNqZNm6bHH39cffv2lSTNnj1by5Yt07x58zR27Ngbjp83b55iY2O1adMm5cmTR5IUFBT0l9dISEhQQkKC9XNcXJwkKTk5WcnJyZIkm80mu90uh8MhY4x1bHpxu90um82Wbtz5uKnjkuRwOFzi5toBLnGbw5Hyu7TiNptks/193BjZjEk/ft1jpxt3OGST0oynVfb04jmlTsnJyRlup/TiHh4eMsa4xI3dTjtlcZ2c7ZiRdnK+ttOLO1/b1rVppyyt0/XvzX/XThmKp74u7ZRldUr9fnu7P3Npp+ypU2bdG9nt9vTLnsl1yo3tlJE6ScqUe6OMvJdff30AAAAAyAoZSmwkJiZqx44devbZZ62Y3W5XmzZttHnz5jTPWbJkiUJDQzVkyBAtXrxYxYsX1yOPPKIxY8bIw8MjzXNeeeUVTZw48Yb4wYMHVbBgQUlSkSJFVLJkScXExOj8+fPWMX5+fvLz89OJEycUHx9vxQMCAlS0aFEdOXJEiYmJVrx06dIqWLCgDh486NJxK1++vDw9PRUWFuZaCLtdJm9eXa5W1QrZkpNV4Pf/yVGokK5UqnDt0CtXlH/ffiUX81FC2TJW3CPugvIdPKSrJfx1tWSAFfc8fUZeEceVWDpQSX6+VjxPVLTyRscooXyQkgsXsuJexyLkeSZWV6pWliNfPiueL/yQPC5c0OWa1WVS/Y3z79svW2KiLtW+y6VKBX7bk6PrFBYWluF2qly5spKSknT48OFrZbfbVaVKFcXHx+v48eNW/ErVyrRTFtfJ2V4Zaae8efOqQoUKOn/+vKKjo624t7e3ypQpo9jYWJ0+fdq6Bu2UtXVytunNtpPTzbyXpy4P7ZR1dUr9vnq7P3Npp+ypU2bdG1WuXFkmXz7aKRvqJClT7o0y8l5+8eJFAQAAAEBWs5nUQ/T+RmRkpAIDA7Vp0yaFhoZa8dGjR2vt2rXaunXrDecEBwfryJEj6tWrlwYPHqzw8HANHjxYTz31lF544YU0r5PWjA1nJ6pw4cIpBc+mGRubH/yX8wCXeE4bwZebRiWGLvoyU2dsbH64B+2UxXUKXfTl//94+0ePbn64R7bU6e/i7thOGamTs00zY8bGpoe6Z0udbojngnbKSJ1Cv1l4LXabP3M3PNAtW+qUG9spI3UK/e7rTJuxsbHrw7RTNtSp6eJvsn3GRlxcnIoVK6bz589b9+nuIC4uTkWKFMkR5X7pmaXZev071YQ3O2faY9Om2YM2zV0ysz0l2jQ70Ka5T2a3Ke5MGblPz/BSVBnlcDjk7++vDz74QB4eHqpXr55OnDihqVOnppvY8PLykpeX1w1xDw+PG2Z52K/v5N1iPL3ZI9fHre7ndZ1C63dpxY2R0sgfZTiexmPfSjytMqYXzwl1St0GN9tOfxW32Wwucee1aKesq9P17XIz7fR3cedr+/pr005ZU6cb3iv/pp0yEk+rnLRT2mVML34rdbpd7ZcW2inteGbXKbPujaS/KHt6cdrpttUpM+6N/i6e+jmT3nUAAAAAIDNlKLHh5+cnDw8PxcTEuMRjYmIUEBCQ5jklS5ZUnjx5XDo91apVU3R0tBITE5U3b95bKDYAAAAAAAAAALgTpT1ELx158+ZVvXr1tHr1aivmcDi0evVql6WpUmvSpInCw8NdprEfOHBAJUuWJKkBAAAAAAAAAAAyJEOJDUkaOXKk5syZo48//lj79u3Tk08+qfj4ePXt21eS1Lt3b5fNxZ988knFxsZq+PDhnzjR6AAAW3RJREFUOnDggJYtW6aXX35ZQ4YMuX21AAAAAAAAAAAAd4QM77HRvXt3nTp1ShMmTFB0dLTq1KmjH3/8USVKlJAkHTt2zGXd3TJlyuinn37S008/rVq1aikwMFDDhw/XmDFjbl8tAAAAAAAAAADAHeGWNg8fOnSohg4dmubv1qxZc0MsNDRUW7ZsuZVLAQAAAAAAAAAAWDK8FBUAAAAAAAAAAEB2IbEBAAAAAAAAAADcxi0tRQUAAAAAAAAAyLjEq1c0Zea/JEnjhn2lvHnyZXOJAPfDjA0AAAAA2e7dd99VUFCQ8uXLp5CQEG3bti3dY+fPny+bzebyX758fCEAAAAA3ClIbAAAAADIVgsXLtTIkSP1wgsvaOfOnapdu7batm2rkydPpntO4cKFFRUVZf139OjRLCwxAAAAgOzEUlQAAAAAstW0adP0+OOPq2/fvpKk2bNna9myZZo3b57Gjh2b5jk2m00BAQE39fgJCQlKSEiwfo6Li5MkJScnKzk52Xo8u90uh8MhY4zLddKK2+122Wy2dOPOx00dlySHw3Fd6cz/X+e6qLFJMv8obiTJ2CSbkc3lWEmyyWYzqR/iFuNplT29eE6qk8lQO6UX9/DwkDHGJW6zGdopG+qUnJycwXZKeW2nF0/92rbZDO2UxXW6/r35ZtopI3FnWWmnrKtT6vfbzPjMTV0vd2gn5+d/Snlu9jmZs+qUWfdGmfVefitx6pT1dbr++n+FxAYAAACAbJOYmKgdO3bo2WeftWJ2u11t2rTR5s2b0z3v4sWLKleunBwOh+6++269/PLLqlGjRprHvvLKK5o4ceIN8YMHD6pgwYKSpCJFiqhkyZKKiYnR+fPnrWP8/Pzk5+enEydOKD4+3ooHBASoaNGiOnLkiBITE6146dKlVbBgQR08eNCl41a+fHl5enoqLCzMpQw2m+ThKfmWSvWFjMOmkxFS3nyST4lr8aREm85ESfkLSoV9r8UTL9t09qTkXUQqWPRa/PIFm+JipcI+Uv5C1+IXz9kUf14qWlzKm/9aPO6MTZcvSsUCJM+81+JnY2xKvCIVLy3Z7NfiZyJtSk6S/Mu6fvly8pgtx9cpPj4+Q+1UuXJlJSUl6fDhw1bMbrerSpUqio+P1/Hjx614sQDRTtlQp7CwsAy1U968eVWhQgWdP39e0dHRVtzb21tlypRRbGysTp8+LSnlOu7WTl4FL2v46B6SpKkvLdSlc/lzRDvdbJ2cr8GMtJN08+/lzjJldzvl1NdTZtQp9ftqZnzmpi6/O7TT6ahr5Spe2sjLK+V32d1OGalTZt0bZdZ7uZR993vU6ebrdPHiRd0sm0mdmsmh4uLiVKRIEZ0/f16FCxfO1rJsvP+hbL3+najJ4m8y9fFp06yXmW1Ke2YP2jT3oU1zH9o098nse6SbcTvu0yMjIxUYGKhNmzYpNDTUio8ePVpr167V1q1bbzhn8+bNCgsLU61atXT+/Hm98cYbWrdunfbu3avSpUvfcHxaMzacnShnubNrtNuU0cv+/zquZWaUb+bWafzUzpk2KvHlMctop2yo03Ovdcy00aMvj1nmdu10NemKJs/oLkka/9RC5fHMf8PxOblOz73W8f+PyZyRyy+PWZbldbrVeE5up4zEx73e0Yplxmeus02zsk7/pJ0SEq9oysxrr1Hn5uHZ3U4ZqdP4qR3daibArcSpU9bXKS4uTsWKFbup/gUzNgAAAAC4ldDQUJckSOPGjVWtWjW9//77mjRp0g3He3l5ycvL64a4h4eHPDw8XGLOTtr1Mhq//nHTj6d8VZD2cDPb7Ykbm9IMG1sa0VuJpxnO0XWy/f83NzffTunHbTabS/zatWinrKxT6ja4mXb6u3jq17bL9d2mna49/t8+J3Ngna5vk5tpp4zEry8rr6fMr9PtbL/rpXyRemO9cnY7ub5Gb/45mWY4W+qUefdG6cf/yXt5ZsSp0+2vU3rXSQubhwMAAADINn5+fvLw8FBMTIxLPCYm5qb30MiTJ4/q1q2r8PDwzCgiAAAAgByGGRsAAAAAsk3evHlVr149rV69Wl27dpWUMqV99erVGjp06E09RnJysvbs2aMOHTpkYkkBAMg+iVevaMrMf0mSxg37ylq6CCkmvNk5u4uQIfHx8ZoyM+Xfz77SQd7e3tlbIMANkdgAAAAAkK1GjhypPn36qH79+mrYsKGmT5+u+Ph49e3bV5LUu3dvBQYG6pVXXpEkvfTSS2rUqJEqVaqkc+fOaerUqTp69KgGDBiQndUAAAAAkEVIbAAAAADIVt27d9epU6c0YcIERUdHq06dOvrxxx9VokQJSdKxY8dc1t49e/asHn/8cUVHR8vHx0f16tXTpk2bVL169eyqAgAAAIAsRGIDAAAAQLYbOnRouktPrVmzxuXnt956S2+99VYWlAoAAABATsTm4QAAAAAAAAAAwG0wYwMAAAAAACAd7rYpscTGxACA3I8ZGwAAAAAAAAAAwG2Q2AAAAAAAAAAAAG6DpagAAAAAAABwx2B5MQBwfyQ2AAAAAAAAACCLeHt7yxiT3cUA3BpLUQEAAAAAAAAAALdBYgMAAAAAAAAAALgNEhsAAAAAAAAAAMBtkNgAAAAAAAAAAABug8QGAAAAAAAAAABwG57ZXQAAAAAAAADcPt7e3jLGZHcxcBvRpgDgihkbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBvssQEAAAAAwB0s8eoVTZn5L0nSuGFfKW+efNlcIgAAgL/GjA0AAAAAAAAAAOA2SGwAAAAAAAAAAAC3QWIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAAAAAAAOA2SGwAAAAAAAAAAAC3QWIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAbsMzuwsAAAAAAEBuMeHNztldhAyLj4/XlJkp/372lQ7y9vbO3gIBAAD8DWZsAAAAAAAAAAAAt0FiAwAAAAAAAAAAuA0SGwAAAAAAAAAAwG2wxwYAAAAA4KYlXr2iKTP/JUkaN+wr5c2TL5tLBAAAgDsNMzYAAAAAAAAAAIDbILEBAAAAAAAAAADcBokNAAAAAAAAAADgNkhsAAAAAAAAAAAAt0FiAwAAAAAAAAAAuA0SGwAAAAAAAAAAwG2Q2AAAAAAAAAAAAG6DxAYAAAAAAAAAAHAbJDYAAAAAAAAAAIDb8MzuAgAAAADAnWrCm52zuwgZFh8frykzU/797Csd5O3tnb0Fwj/m7e0tY0x2FwMAAOCmMWMDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAAAAAAAOA2SGwAAAAAAAAAAAC34ZndBQAAAAAAuA9vb28ZY7K7GAAAALiDMWMDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAAAAAAAOA2SGwAAAAAAAAAAAC3QWIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAAAAAAAAAboPEBgAAAAAAAAAAcBskNgAAAAAAAAAAgNsgsQEAAAAAAAAAANwGiQ0AAAAAAAAAAOA2SGwAAAAAAAAAAAC3QWIDAAAAQLZ69913FRQUpHz58ikkJETbtm37y+PPnTunIUOGqGTJkvLy8lKVKlW0fPnyLCotAAAAgOzmmd0FAAAAAHDnWrhwoUaOHKnZs2crJCRE06dPV9u2bbV//375+/vfcHxiYqLuvfde+fv7a9GiRQoMDNTRo0dVtGjRrC88AAAAgGxBYgMAAABAtpk2bZoef/xx9e3bV5I0e/ZsLVu2TPPmzdPYsWNvOH7evHmKjY3Vpk2blCdPHklSUFBQVhYZAAAAQDa7pcTGu+++q6lTpyo6Olq1a9fWzJkz1bBhwzSPnT9/vtVJcfLy8tKVK1du5dIAAAAAconExETt2LFDzz77rBWz2+1q06aNNm/enOY5S5YsUWhoqIYMGaLFixerePHieuSRRzRmzBh5eHikeU5CQoISEhKsn+Pi4iRJycnJSk5OliTZbDbZ7XY5HA4ZY6xj04vb7XbZbLZ0487HTR2XJIfDcVNxDw8PGWNc4s6ypBe/2bJTJ+pEnagTdaJO1Ik6USfqlBPrdP31/0qGExsZnSouSYULF9b+/ftdCg0AAADgznb69GklJyerRIkSLvESJUrozz//TPOcQ4cO6eeff1avXr20fPlyhYeHa/Dgwbp69apeeOGFNM955ZVXNHHixBviBw8eVMGCBSVJRYoUUcmSJRUTE6Pz589bx/j5+cnPz08nTpxQfHy8FQ8ICFDRokV15MgRJSYmWvHSpUurYMGCOnjwoEvHrXz58vL09FRYWJhLGSpXrqykpCQdPnzYitntdlWpUkXx8fE6fvy4Fc+bN68qVKig8+fPKzo62op7e3urTJkyio2N1enTp604daJO1Ik6USfqRJ2oE3WiTu5Up4sXL+pm2Uzq1MxNCAkJUYMGDfTOO+9ISsnKlClTRsOGDUtzqvj8+fM1YsQInTt37qavcf2IqvPnz6ts2bI6cuSIChcunFLwbMpGbevVx3mAS9zmcMikF7fZpOuSOWnGjZHNmPTj1z12unGHQzYpzXhaZU8vnlPq1PDTjzI1a7jt0b60UxbXqeGnH/3/j7c/u7vt0b7ZUqe/i7tjO2WkTs42zYyRBVv//Vi21OmGeC5op4zUqeFn86/FbvNn7uZHemdLnXJjO2WkTg0//yTTRups6fko7ZQNdQr98rNsH1EVFxenoKAgnTt3TkWKFFFGRUZGKjAwUJs2bVJoaKgVHz16tNauXautW7fecE6VKlV05coVHT582JqhMW3aNE2dOlVRUVFpXicn9y8YwUedqBN1ok7UiTpRJ+pEnahTxvsXGZqxcStTxaWUTEu5cuXkcDh099136+WXX1aNGjXSPT69EVWsnXuHKlYsu0uA2402zX1o09yHNs19fHyyuwS43W4hkZBZLly4cEuJDT8/P3l4eCgmJsYlHhMTo4CAgDTPKVmypPLkyeOy7FS1atUUHR2txMRE5c2b94ZzvLy85OXlZf3sXIqK/gUAAACQ89xM/yJDiY1bmSpetWpVzZs3T7Vq1dL58+f1xhtvqHHjxtq7d69Kly6d5jnPPvusRo4caf3scDgUGxsrX19fsYxVxsXFxalMmTKKiIiwRqTBvdGmuQ9tmvvQprkPbZr70Kb/nDFGFy5cUKlSpW7p/Lx586pevXpavXq1unbtKinl3n/16tUaOnRomuc0adJEn3/+uRwOhzVS7MCBAypZsmSaSY20lCpVShERESpUqBD9i1vE6yf3oU1zH9o096FNcxfaM/ehTf+5jPQvbmnz8IwIDQ11mVbeuHFjVatWTe+//74mTZqU5jnXj6iSpKJFi2ZmMe8IhQsX5kWVy9CmuQ9tmvvQprkPbZr70Kb/zK3M1Eht5MiR6tOnj+rXr6+GDRtq+vTpio+PV9++KctL9u7dW4GBgXrllVckSU8++aTeeecdDR8+XMOGDVNYWJhefvllPfXUUzd9Tbvdnu4gK2QMr5/chzbNfWjT3Ic2zV1oz9yHNv1nbrZ/kaHExq1MFb9enjx5VLduXYWHh2fk0gAAAAByoe7du+vUqVOaMGGCoqOjVadOHf3444/WLPFjx45ZMzMkqUyZMvrpp5/09NNPq1atWgoMDNTw4cM1ZsyY7KoCAAAAgCyWocTGrUwVv15ycrL27NmjDh06ZLiwAAAAAHKfoUOHptufWLNmzQ2x0NBQbdmyJZNLBQAAACCnyvBSVBmdKv7SSy+pUaNGqlSpks6dO6epU6fq6NGjGjBgwO2tCdLl5eWlF1544YblveC+aNPchzbNfWjT3Ic2zX1oU+DW8frJfWjT3Ic2zX1o09yF9sx9aNOsZTPGmIye9M4772jq1KnWVPEZM2YoJCREktSyZUsFBQVp/vz5kqSnn35a3377raKjo+Xj46N69epp8uTJqlu37m2tCAAAAAAAAAAAyP1uKbEBAAAAAAAAAACQHex/fwgAAAAAAAAAAEDOQGIDAAAAAAAAAAC4DRIbAAAAAAAAAADAbZDYAAAAQJZiizcAAAAAwD9BYgOWEydOaOrUqdq/f78kvnTILS5cuJDdRQDgRpzv/WfPntUff/yRzaVBbnPlyhX9+uuvstlsd8x9RnJysowxd0x9gevRx8h96F8AyCj6GJg0aZKGDx+uI0eOSEq5R8atoX9xDYkNSJKSkpI0ePBgjRkzRu+//74kOh3uzNl2/fr1U9euXXXlypVsLhH+idOnT2vevHkaMGCAXn/9deuLAeB2SkpK0rp166z3i/nz56t3797asGFDNpcMucmGDRt0//33a8WKFbLZbNldnEz1+eefq0uXLtq3b59sNluury+QFvoYuQf9i9yF/gWyCn0MOBMYDz/8sC5fvqzBgwdLkjw8PLKzWG6J/sWNSGzcYfbt26f+/ftr1qxZkiSHwyFJioyM1OXLl1WzZk399NNP2VlE3ILdu3dr3Lhx+u9//ytJunr1qiSpcePG+uOPPxQbG5udxcMt2rJliypVqqQSJUpoxowZunTpkj766CPdd9992r17tyS+HMDts3jxYj300EPWZ8DTTz+t5s2b67XXXtOvv/4qiecb/rk2bdpo6tSpGjZsmP7888/sLs5t43A4lJSUJIfDYb1OSpcurYiICK1Zs0Zvv/22+vfvr6SkpGwuKZA56GPkPvQvcif6F8hq9DHubMYYK4FRrVo1TZgwQfv27dO7776bzSXL+ehf3BwSG7ncpUuXrH8nJSXp+++/10cffaSZM2dq1apVsttTngK+vr7aunWrnnzySe3bt0/nzp2zfoec7cKFC3rvvff0yiuvaOjQodqzZ4/y5s0rSerQoYNiYmIYgeNmnB9a+fPnV1RUlD788EPt3r1bn3/+uZYuXaozZ87oww8/lMPhIEOPfywxMVGSVLVqVdWuXdvqYEjSU089JV9fX73wwgvZVTzkYH81fdx5I56WXr16qUqVKpo4caJOnDiRWcXLUna7XZ6enrLb7daXf6tWrdKePXs0cuRIffLJJypdurT1ZS/g7uhj5G70L3If+hfIavQx7jzGmBvudW02m06ePGnN2CldurQGDRqk+fPna8+ePdZ5uBH9i5vDXWUuc+TIEb3xxhtq0aKFqlevrkGDBmn9+vWSJE9PTzVp0kT58uVT/fr19frrrysyMlKSdOzYMVWtWlUlS5ZUmTJl9MMPP0jiDSYn2bx5sx588EEtXbpU0rW2KVSokJo1a6ZixYopICBA/fr1U3h4uCSpVKlSKlu2rNatW0db5jCXLl3S1atXNXHiRI0aNUrStTZ1diZq166toKAgHT16VJcuXVJiYqIqVaqkWrVq6ciRIzp//ny2lR+5h/OLiqCgIFWrVk0bN260fhcUFKTHH39cq1at0q5du+jowkXq6ePX31A7b8QlKS4uzoo7kx3PPPOMjh49qkWLFmVBSW+P5OTkdJM527Zt07Bhw9SwYUM98MADSkpKUmhoqJo2barOnTtr/fr1mjhxovV6A9wNfYzcif5F7kL/AjkJfYzcLb0kxvWDFxYvXqxatWpp586dVuzBBx+UJC1YsMB6rDsV/Yt/jsSGmzt//rw2bdokY4zq16+vChUq6KuvvlLr1q316quvKiIiQgMHDtThw4clpXyAFCtWTI0bN5aXl5deffVVSdIff/yhUqVKqWrVqqpevbq+++47STd+UYGs4XA49NVXX7lM2Z81a5YOHjyo5557TlFRUS4f/pUqVZK3t7f69++vcuXK6fHHH9f27dslSa1atdLq1at1+fLlLK8HXDkcDn355ZcqW7asBg4cqDx58ujgwYPatWuXLly44NKmzg+3OnXqaNeuXUpISFDevHkVHx+vs2fPKiAgQD4+PtlVFbgR58j5tG4YL1++rDlz5uiee+7RM888I2OMIiMjFRUVZZ3bpEkTVatWTR9//PEdP80VrqZPn67nn39ekqxOjPN5tm3bNnXt2lUlSpRQ165d9dZbb0m6lgxp1KiRqlevrhUrVmRDyW+Nh4dHmmsBL1u2TAMGDNCxY8c0cOBAdezYUREREerQoYO6du2qmJgY7dixQxL3VXAf9DFyH/oXuRP9C2QX+hh3Jufnd1pJjBMnTqhTp04aM2aMLl68KEn6+uuvFRISosaNG1vvQeXLl1erVq30zTffZG3hcyD6F/8ciQ03tHLlSg0YMEBly5aVj4+PXn31VdlsNhUoUEB9+vTRunXrNGHCBHXp0kUvvfSSjDHWtL/ixYurTp062rZtmyZNmqT9+/frnXfeUcmSJbVnzx5Vq1ZNTZs21dq1ayWJqeJZzHlT8NNPP2nQoEG6dOmS9SYVHh6u8ePHq06dOpo0aZJOnjxpnVeuXDkFBQXp999/17x58+Tt7a2BAwcqPj5ePXv21Pbt262bCGSdiIgIq02NMbLb7friiy904sQJnT59WmfOnFGrVq108eJF/fbbb5Ju/FBq3bq1wsPDtWzZMr355puqU6eOEhIS1L9//yyvD9yTc+R8WiOhPvroI73++utq3LixatWqpQ0bNujgwYPWZ4azk+FcF/fcuXNZWXTkIMaYGzquV69e1aeffqpz585p8eLFmjdvnmw2m06dOqXRo0erYMGCmjdvntq0aaNnnnlGb775pvWc8vb2Vv369XXixAn973//y44q3cDhcKTbMTh37pzmzJmjhx9+WM8995z++OMP63ezZs1ScHCwFi9erAEDBmjw4MEqX768JKlhw4a6dOmS9u3bJ0mMSESORh8jd6J/kbvQv0BOQR8j90sraeX8/N6/f78++ugjrV+/3nqPKV68uLp166Zly5ape/fuWrt2rbZv366ePXtKujbAKU+ePGrZsqWOHDmiS5cu5ep7AvoXmS/3PntyoR9++EE+Pj566KGHlJCQoNdee00xMTFasmSJJOmee+7Rn3/+6XJD+ueffyowMFBt2rSRlLKmZrt27fTDDz+obt26evLJJ/Xiiy9q+fLlqlGjhowxatq0qWJjY3X48OE7/gWSFRYvXqxWrVpp79691hdCTz31lIYOHaoHHnhAdrtdFy9elL+/v5KSkjR+/HidPHnSGgknyRoht3jxYhUuXFgffvihbDabunTporvvvlvJycnW+oXIfMuWLZO3t7eaNm1qdSicN29FihRRtWrV5OnpqZ07d6p+/fqy2Wwua45K124Y2rZtq6ioKA0ePFjLly/X4MGDtWvXLoWGhmZtpeB2nDeiGzZs0BNPPKH77rtPH374oSIiIiRJsbGx+vTTT9WiRQtNnjxZQ4YM0XvvvacKFSpo5cqVkq7dJLVr106HDx9mNNUdKPWorNT3BA6HQ1u2bFFUVJSKFSumoUOHKjo6WlLK51pYWJjGjh2rjh076rnnntOLL76oL774wuW9rkKFCipUqJD27t2btZW6jvO1Yrfb0+xYxcbG6oknntC0adMUGBioDRs26IEHHrCWbgkICFBYWJgWLVqkuXPnat26ddYo9jp16qhgwYI6cOAA65Yjx6KPkfvQv8h96F8gp6CPkftdv4SdU3JysvU536RJE82cOVODBg3SY489Jill+bE+ffrohx9+0NmzZ/XUU0/JbrerS5cuN1yjSJEiKlu2rMsSVbkJ/YusQ2LDjZQpU0YVKlTQ3Llz9emnn6pnz54qXry49fs2bdroxIkT+vjjj/XUU0+pYsWKeuKJJ3T58mXrxSFJoaGhOnPmjH777Td17dpV9957r15++WUFBwfLZrOpQoUKKl++vHXOnT6tKbOVK1dO8fHx1hc748ePV8GCBTVu3DjrmNOnTys6OlqlSpVSpUqVNHToUC1YsEC///67JFlrGkdHR+v48eMKCAjQl19+qUOHDmnKlCkqWLCgduzYYW04hMxVpkwZValSRdHR0ZoxY4bOnDmjPHny6OjRo/L09FT79u11+fJlbd++XVWqVJG/v7/VQXF+6NlsNhljFBgYqIoVK+rf//63li1bpqefflpFihS5o9ehvBM41yxN/f47ZswYDR8+3NqIL70OgPPLZZvNpm+//VYDBgzQxYsX1bRpU73//vv697//rYSEBJ06dUqxsbHq0KGDdW7t2rXVrl07a6mJPHnySEoZfeNwOKwpxXBffzVqSErpsKT+vfM9KSwsTAsXLtTBgwflcDhkt9tVqFAh2e12a1ma5557TpK0ceNG1a1bV5UrV7Yep127dipatKg1WltKWafd4XBk2fvZsWPHdPToUZeYMcbqDGzcuFGvvvqqli9froSEBOuYxYsX64cfftDixYv19ttv6/PPP1fz5s31zDPPSJImTJigIkWK6JVXXtGKFSvUv39/1ahRQ0uWLFGBAgV011136ffff9exY8ckic9i5Dj0MXIf+he5D/0L3A70Me5Mf/V5a4xx2ePBeW989epV/fzzz9qyZYv1u6SkJB0/flwDBgzQ0aNHtXPnTn333Xf64osv9N///tc6PygoSF999ZWioqL0559/avPmzS6P4fx/2bJldfbsWes8d0T/IgcwcBvx8fHm3nvvNaNHj3aJ796922zatMkYY0ylSpVMkSJFTLdu3cz8+fPNjh07zJgxY0y+fPnMW2+9ZYwx5ty5c6ZixYpm0qRJxhhjwsLCzPz5882hQ4eMMcZcuHDB3HfffaZ+/frGGGMcDkcW1fDOFB8fb1q3bm3Gjh1rtmzZYnx8fMzGjRuNMdf+9pcvXzY+Pj4mMTHROq927dqmTZs25uDBg8YYY3bu3GmCgoLMhx9+aB2zcuVK06lTJ2Oz2Uzjxo1NTExMFtbsznXx4kXzwAMPmC5dupiHH37YjBo1yhhjzNmzZ02ZMmXMgQMHzMMPP2x69OhhjDFm9OjRpmXLlubEiRPGmGvtnpSUZIwxZsiQIaZ169YmPDzcGGNMcnJyVlcJWSSt99vY2FgTFxdnvvvuO7N79+50z01OTjZ9+vQxrVq1MsYYExERYerVq2fmzJljHRMVFWV8fX3NlClTTFJSkvHx8TGffPKJuXr1qnXMSy+9ZMqUKWP+/PNPK7Zjxw4TEhJiVq5ceTuqiSx2K5/jFy5cMMYY87///c80a9bMeHt7m2rVqplKlSqZ9957zzru/vvvNw8//LAxxljPozfffNNUrFjRnD9/3rr22bNnTZs2bczIkSOtcxMTE03RokXN+vXrb7lufyUuLs7MnTvXtG3b1vj4+Bh/f3/TuHFj89prr7kcFxUVZdq0aWP8/f1Ns2bNTOXKlc29995rjEl5Hx48eLDp1KmTyzkbN240efLkMXv27LFiV69eNQcOHDDGGNOxY0fTrl07Y4wxS5cuNQ0bNjTNmjUzFStWNA888ECm1Be4VfQxch/6F7kP/Qv8E/Qx7kynT582CQkJLjHne0BaIiMjjTHGbNu2zVSqVMmULl3alC1b1nz44YdWWzo/0y9evGi+/PJLM3jwYGOz2czIkSOt/oMxxvz0008mJCTEPPTQQ6ZSpUrm1VdfdbnWb7/9ZsqUKWOio6NvS12zCv2LnIcZG26kQIECCgoK0o4dO/T222+rQ4cO8vf3V7NmzbRmzRpJUq1atdSmTRvNnj1bffr00d13361XX31VDz/8sD7//HOdO3dOhQsXVqtWrbRq1SpJKRvD9enTx1qvrWDBgpoxY4YWL14sifXaMluBAgVUoUIF7dq1S6NHj9bgwYPVsGFDSdey1rt375afn58effRRNWzYUPny5dO5c+d04MABTZkyRVLKpo1169bVjz/+KCll1O29996rSZMmqWzZsqpUqZI1MgKZy9vbW35+fjLG6IknntAnn3yiTZs2ycPDQ56ensqbN6/q16+vY8eO6dixY2rYsKEuXryo3bt3uzyOcw3K9u3bKzw8XGFhYZJ4TeZGzhEWzrb95ZdfNGjQIFWqVEm+vr56/fXX1bVrV9WuXdsacbNkyRI1aNDAev+PiIjQ8uXLNX78eEkpa3YePnxY99xzj6ZNm6amTZuqZs2aypcvnwoVKiQPDw9Vr15dv/zyiy5dumSVJS4uTpGRkS7LF5w/f142m02VKlXKij8HbjPn82rnzp0aM2aMmjRpopYtW+q9996zRkklJyfr5MmT+uGHH1S6dGkNHjxYJ0+e1LvvviubzaZDhw5py5YteuCBBzR9+nR99NFHklLW6d6wYYPOnTsnT09PSVKHDh10+PBhbd++3bq2p6entm7dqkaNGklK+XxLSkqSn59fpqyrGxMTo0cffVQDBgxQkyZN9M0332jTpk1q3bq1JkyY4PL8nj17to4fP64tW7Zo3bp1+uyzz/Trr79q8uTJ8vDw0NmzZ1WkSBGXZXjKli2r8uXLa/Xq1ZKkhIQEXblyRZUrV1ZCQoKSkpIUGBgoKeU9fMaMGWrQoIH+85//6IMPPrjt9QX+CfoYuQ/9i9yH/gVuBX2MO48xRkuWLFFAQIAaN26s//73v5o8ebI+/PBDSXLZqPrYsWO6cOGCBg0apMKFCys0NFSzZ8/W1KlTNW3aNEVERKhNmzb64IMPrNk25cuX148//qgmTZpo8uTJ8vLyUvfu3bVkyRKXvVI2bNggu92ur776SuPHj9eUKVPUv39/XbhwQVLKslVSylKW7oL+Rc5EYsPN3Hvvvdq1a5dmzZqlkJAQLVy4UBEREXr22WclSS1atFBYWJjCw8MlpbypJSYm6urVqzp9+rSKFi0qKWWq+Pr169OdrlS1alWVKlUqS+qElHbds2eP1q9fr61bt1pviM4ve6KjoxUZGan4+Hh17NhRv/zyiw4ePKj3339fq1ev1pw5c+Tj46Nq1arp+++/l3TtA6tOnTo6cuSIPv74Y/n4+GRL/e5E99xzj2JiYlS1alX16dNHc+bMsTZQy5Mnj2rXrq2EhATt3LlT9erVk5eXl7W+pHPpgPfff18LFy5Uq1atZIzR5cuXJdHxyE3CwsLUu3dva3me//3vf6pbt65at26tS5cuacKECYqMjNSkSZN0/vx59erVy/qyoU6dOmrQoIE6d+6sb775Rt999538/PxUv359SdLZs2eVL18+Va9eXUuXLlXbtm21fPlyhYeHa9iwYZKkxx57TNu3b9eLL75oLV/wxx9/qGTJkvr888+tcjocDp06dcr6cgru5erVq2rUqJHq16+vPXv26JFHHlFwcLCGDh2ql156SZLUt29f1atXT8uXL9e4ceM0ffp0RUVFae3aterRo4f8/f1VuHBhvfTSS7r77rutz5r77rtPJ0+etL4YSU5OVnBwsFq1aqVRo0Zp0aJFOn36tF599VVVrFhRtWvXlpTyPrZ69WpVqFDhH91vbN++XRMmTFCvXr00efJkqxNeqFAhazms559/Xq1atVLFihX14osvKjEx0Xo/laSvvvpKvXr1Uvny5ZWUlKSGDRvq0Ucf1S+//KLY2FiFhITo0KFD1rq2knTlyhV5e3srX758MsZo4cKFGjNmjB544AEFBQXpypUrGjNmjKSUz+OQkBC9+eabGjRokPz8/G65vkBmoY+R+9C/yH3oX+Bm0ce4c509e1ZTpkzRo48+qt9++00dOnTQ8uXL9fvvvys6Olpz587Vvn37dOLECQUFBalv377y8/PT+vXrdf/992v8+PGy2Wzq3LmzJGno0KEqUKCAli9fLkk6fPiwxo0bpyZNmmjt2rWaNm2a+vfvr2PHjunQoUNWOebNm6cnn3xSdrtdffr00cKFC3X58mVrUNXq1avVr1+/HJkcp3/hZrJnoghu1aFDh0xISIiZPXu2S9w5neyPP/4wpUuXNosXLzbGGHPp0iXz6aefmipVqpiZM2dax588edKsW7cu6wqOv3To0CHTsmVL0717dzNgwACTN29e079/f2sa+Ndff22KFSuW5rlPP/20KVy4sImNjTXh4eFmw4YNWVl0pOPQoUOmYcOG5tNPPzXnz583//nPf4zNZjOhoaHGGGMOHz5sWrRoYcaNG2ccDofp3r27ufvuu81jjz1mgoKCjN1uNwEBAWb69OnGGJZryG2c7fnDDz+Y6tWrW/GTJ0+ajh07mgEDBqR53j333GN69+5tzp07Z8Wef/5506BBA+Pl5WUtB2JMytTuhg0bmmeeecblMa5evWp+//13c/bsWZOYmGhmz55tgoKCTJkyZYyvr6+ZOHGiWbt2rdm5c6d1znfffWfmzJnjMp0cOcvfLSHx8MMPm86dO1s/JyQkmIkTJ5qAgABz6NAh8/333xubzWb69+9vHXP27Fnj6elpfv75Z5fHev31103Dhg1NRESEMcaYoKAgM3HiRJdjDh8+bPr27WuqV69uChQoYMqXL2++/vprY8y15//GjRtdlqbKiK1bt5rmzZsbPz8/07FjRzN16lQzbtw48+9//9v6WyxevNgUKlTImhaflJRkXnzxRdOpUydr6ZTExERTr149M2bMGGNMyvItxhjz/fffm+DgYLN9+3azf/9+ExISYh5++GFz/vx5Y4wx7777rvW3M8aYTZs2mUGDBpmxY8eatWvX3lKdgOxEHyP3oX+R+9C/wN+hjwFjjPH09LQ+i/fv32+aN29uPDw8TL58+Yyvr69ZtWqVuXDhgunWrZspWLCgde8aFRVlOnfubFq2bGk9VmxsrOnfv79p3bq1McaYEydOGJvNZn0uXLlyxXTr1s3YbDYzdepUY4wxH3zwgalYsaI5fPhwumXcuXPnX/4+O9C/cE8kNtxMYmKi6dChgxk4cKAxJu0vMoKDg02DBg1MSEiI8fLyMqVLlzYTJkwwcXFxWV1c3CRnuw4bNswYY8ySJUtMlSpVTIkSJczrr79uxo8fbxo1auSyZqHzpiU2NtZ6I0TO4WzTQYMGGWNSbgDKlCljHn30UeuYnj17mnvuucdcuXLFvPDCC6Z8+fKmU6dOZt68eebMmTPZVXT8QxnpJH7xxRemWbNmLrGRI0ea1q1bu6x/+scffxiHw2FeffVV07x5c5cOgTHGzJgxw9hsNtOjRw8TGxtrjEn5Unrs2LGmRIkSZs2aNebSpUvmypUrZtGiRaZ79+7mf//7n3X+zp07zdKlS61zrxcbG/uX67Eie6VeNzu9BMdHH31kfH19TVRUlLWe+nfffWdsNpsJDw83x44dM56enmbBggUu55UqVcq88cYbLrGZM2eahg0bmn379hljUp6zZcuWNc8884zp0qWLtYbulStXzI4dO6z1va93+fJlc/ny5b+sk/PfI0eONK+//roxxpjw8HBTp04d06NHD7N3717ruNTrxBtjzN69e01wcLBp0aKFCQ0NNQUKFDDe3t6mYcOGpnfv3ub48ePGGGMGDBhg6tWrZ4y5dl915MgR4+XlZdVx/fr1xtfX1zRv3tzcddddpnjx4mbGjBlplh1wR/Qxch/6F7kP/Ys7G32MO4fD4TDJycnp3tc7HA5z9erVG+6XjTHm999/N35+fqZkyZLGZrOZzp07m379+ply5cqZTz/91OX4KVOmmJIlS1qxq1evmmnTphl/f3+X67355psmODjYuqcvV66cadKkiZk1a5YZNGiQmTp1qmnWrJkZOnSoMca4vNekroPD4cjWhCr9i9yJpajcTJ48eVSjRg1FREQoOTlZdrtdx44d09y5c9WlSxdFRESoZ8+eypcvnx544AHt3r1bERERmjhxogoVKpTdxUc6nO26f/9+nT59Wp07d9Yff/yhJ598Uu+9956mTJmiihUruqxR6Zwu7OPjo8KFC2dX0ZEOZ5seOXJEZ86cUalSpXTgwAF98skn1jENGzZU3bp1denSJT377LM6dOiQli5dqr59+6pYsWLZWHr8E87XZkJCgpKTk631atOyYcMGBQcHKy4uzorVqFFD586d0/Dhw/XQQw+pWLFi6tChg3799Vc1atRIFy5c0N69eyXJevyTJ0+qVq1aioqK0r333quYmBgVLVpUL774ourXr68BAwaoffv2CgwM1NNPP61KlSopICDAumbdunXVqVMn+fj4yKQMenApp4+Pj8t6rMhZbDabdu3apbvuuksHDx5M85iWLVvq7NmzOnXqlDXl+5tvvlG1atVUoEABlSlTRpUqVbKeW87nbceOHbVw4ULt2bPHeqy1a9fKbrcrODhYkvTss8/qiSee0K5du1SzZk1169ZNkuTl5aW7775bpUqVkjHmhtdCvnz5lC9fvhvKeujQIZclMaKiojR79mzVrFlTkjRr1iydO3dO48ePV/Xq1a3j8uTJo8OHD2vz5s2SJF9fX4WGhurXX3/Vk08+qc2bNysiIkLjxo3TunXrNGTIEEnSv/71L+3cuVM///yz9dyfM2eO/P39Vbx4cUlS06ZNtXXrVj300EMaOnSodu/ebS23AOQG9DFyH/oXuQ/9izsbfYw7h81mk91ul91u1+XLlxUTE+PS3jabTZ6enrLZbIqJiVFMTIxsNpuio6M1Y8YMeXh4yGazKSwsTEuWLNHkyZPl7++viIgISSnLSdpsNjVp0kTR0dE6fvy4pJQ98e666y4lJia67BdRrVo1eXl5WUsyffrppwoODtbUqVMVGxur+++/X2vWrNHMmTMlyXqvcTgcLnvp2Wy2bFv2jv5FLpZ9ORXcqp9++slUqlTJNG/e3FSoUMHY7XZTqVIlM2TIEBMVFZXdxcMt+umnn0ytWrXMihUrjDHXpv7v27fPbN++PTuLhlvkbNMff/zRGGOsURd/t2QM3NvZs2dNsWLFzHfffZfuMc7X95NPPmnat29vkpOTrREku3fvNvXq1TPFixc3L7zwglm1apU1yun06dMmJCTEPPfcc9ZjJSYmGh8fH7N48WJz5swZ06RJE1O9enXz2WefWcds3LjRzJ4922zZsiUTaoys8lcjnJKTk01QUJBZsWKFuXr16g2jo4wxpnLlyqZbt25m4MCBplSpUqZw4cLmyy+/tI4bMmSICQkJMUlJSdY5f/zxh2nRooWpWLGief/9902/fv1M9erVzcqVKzOljn379jUlSpQwCxYssF4n48ePNyEhISY+Pt4kJCSYVq1amQ4dOhhjUkaWXb582UyfPt0UK1bM2Gw2a2mVxMREM23aNFOiRIkbrjN8+HBTvnx5a1RVz549TWBgoBkwYIDp3r27CQwMNF988YXL3w/I7ehj5D70L3If+hd3LvoYuVdCQoIx5trI/l27dpmnnnrK1KhRw1SpUsWMGjXKWkbQmJSljV588UXj7+9vihcvbu655x7z8ccfW7/ftm2b8fDwcPns7tixo+nTp4/LbLzIyEhTokQJM2fOHCsWHh5u6tev77LM7N69e01oaKgZNWqUFbt+JkPq8uc09C9yN2ZsuKHatWsrODhYQUFBmjp1qi5duqSwsDC98847LtlxuJdatWqpRo0ayps3r6Rrm/MFBwerXr162Vk03CJnmzpHRztHXaQetYCcz6Qx0tzJ4XAoOTnZ+jk5OVlFixZVcHCwvv76a02ePFktWrSwNskz/z9aw/kcqF27tn7//XeX50SlSpVUsWJFtWrVSi+++KLatGljjXLy9fVVUFCQwsPDFR0dLUn6+OOPVbJkSdWpU0fFihXTjz/+aG0umZCQIElq3LixBg0apJCQEKvcyNnSet7ZbDadPHlSV65ccYknJCRYMyh27NghT09Pl81hnaOTHn74YS1atEiJiYmaMWOGzp49q+7du1uP06lTJ+3du1dRUVHWOdWqVdNnn32mhx56SHPmzFFsbKymTp2qNm3a3FBmh8Pxj59bzo0GJ0yYoMmTJ0uSNRulQIECio2Nddmcz9PTU3nz5lVwcLDmz5+vpUuX6sKFCwoLC1OePHl01113KSEhQVu2bHG5zp9//qmiRYtar985c+bonXfeUVxcnHx8fLRw4UL16NHD+rsDdwL6GLkP/Yvch/5F7kEfA85N3tetWycppf3Wr1+vAQMGKDIyUs8884zeeust1a9f32WT7aVLl2rBggV68803tWbNGlWpUkWDBg3SL7/8IkkKCgpSwYIF9fPPP1vn1KlTR0ePHnXZ4NvX11eNGze2NgaXpOLFi6t8+fL67LPPrFiVKlX07bffaurUqVbMWZ7k5OQbnn85Df2LXC77cioAAOQ+K1euNDabzeTPn9+EhISY1157zURGRqZ57IYNG4zdbrc2YXaO3Bg/frxp1qyZCQsLs+LO0SXTpk0zISEh1obOLVu2NH379jXGmL9co5ZRITmXcx3dv/P999+bEiVKmI0bN1rnpTZz5kzTrFkzM2nSJNOmTRtjs9lMt27drN8vXbrUFChQwJw6dcqKpX7OnDlzxthsNmskUeprZOX6x1evXjVffPGFCQgIMB07djQBAQFm9+7d1u+7du1qatSoYU6fPu1SRmNSNscsX768tTHq/v37zd13323t+7F161YzatQoc9ddd1nrDOfU0WUAAABO9DFyj/Q2eY+LizMhISGmd+/eLvsfXe+uu+4ygwcPdom1atXK/Otf/7L2t2jTpo155JFHrN8vX77cNGzY0MybN8/lvLfeesvYbDaX2K+//mo2b958a5XLoehf5F45M50GAEAOceTIEQ0aNEhz5syRJCUlJUlKGRm1cuVK9e/fX4888ogWLFggKWXk0owZM5SYmKhFixZp9OjRKlmyZJqPXb16dZUsWVKLFi1yideuXVtJSUnatWuXdS3nqI769etLkq5evSpJ+uqrrzRv3jxJclmjNvUoL4lRITmRc1Sbcx3d1CIjI9WpUyeNGTNGFy9elCR9/fXXCgkJUePGjZWcnCybzaa4uDjNnz9fPXv21MSJE7Vp0yZ9++23at68uTZu3KiPPvrIeszQ0FAZY7R161Yrlvo5U6xYMQ0cONBlDW7n88Z5XOpRWZnF09NTPXr00KJFi7Rr1y7Fx8dba9FKUkhIiCIiIqzRbTabzfpbFi1aVBUrVtTXX38tKWXU2V133aVnn31Wfn5+at68uXbt2qXnn3/e2gskp44uAwAAuRd9jNzpZu6TnX+zCxcuyNfX14oXKlRI+/bt03333aeCBQta8dR/87Nnz8rhcKh8+fKSZM3k7tWrl3bu3KmYmBhJUrt27bRu3TrredWoUSP5+/tr9uzZmjx5sho3bqxDhw6pTZs26tevn8t+LPXr11ejRo1u9U+QI9G/yL34SwMA7njOG9CjR49q/fr1Lr9zOBwKDw+3ppp6enpKkt555x2NGDFC8fHxqlKlikaOHKn//Oc/cjgcGjp0qAoXLqwVK1b85XV9fHzUo0cPLViwQEeOHLFucmvVqiW73W5NH079xXezZs20ZcsW3XfffZLkckOWGhvx5SxpdXKcbbp//3599NFHWr9+vXUD7efnp27dumnZsmXq3r271q1bp+3bt6tnz56SUto3MTFRI0aM0BtvvCE/Pz99/vnnqlChgp577jk9//zzCg0Nlbe3t3U9X19fVa5c2bopT8vs2bOt51ZanJsRZoWgoCD5+fnp4sWLGjZsmLWJYefOnVWrVi2NHTtWv/32m8vGhBs2bNCVK1esJVaKFi2qf/3rX3r11Ve1fPlyXblyRf/973/VrVs3eXl5ZUk9AADAnYk+xp3nn27y3qJFC73wwgsaMmSIBg0apCeeeEKvvfaaFixYoEuXLklKWU5w586dkq61R7NmzXT48GHreXTPPffo5MmTOnDggKSU58Sbb76patWqaePGjWrdurX8/f1Vs2ZNffjhhypcuPDt/2PkQPQvcqHsmioCAEBO4JxmGh4ebmw2m7HZbOaNN95w+f0zzzxjmjVrZi5fvmyMMSYsLMwEBQWZ5cuXW8ctXbrUlCxZ0ixYsMAYY0zbtm3Nww8//LfXDQsLM82aNTPjx4+3fpeUlGRGjx5tZs+ene70bqa3uof02i8pKcksXbrU1KlTxxQrVszUrVvXVKtWzTz66KMuxx0+fNg0atTI1KpVy1SrVs3Ex8cbY661f+oNAI1J2aTu6aeftjYhvN6rr75qxo4d+7cbkWcnZ9k+/PBDU7t2bbN582bTvn17ExQUZG12u337dlO3bl3j5eVlevfubfr06WOqVatmSpUqZYYPH27Onj2bjTUAAAB3OvoYd6Zb3eTd+Xc/ceKEmTx5smnUqJHp1q2b6devn6lXr54pWLCgeeyxx4wxxrz22mumRIkSLm01d+5c4+vra21cffbsWZM3b14zY8aMvy2zw+HI9UuK0b/IvUhsAADuOElJSWmuFVuzZk1Ts2ZNkydPHvPFF1+YK1euGGOMmT17tqlbt65Zt26dMSbly+Fu3bqZ7du3m5dfftk0bdrUFClSxJQsWdJ88sknxhhj3nvvPRMYGPiX66M6LVu2zAQEBJj169ffxloiM/xVZy/1OsXOn40xJjEx0axevdplrdorV66YWbNmmXfeecdcvHjRGGPMvn37jKenp1m1apXL+REREcbPz8/YbDbz3//+N82yOK87b948U7t27XTXXHYXDofD9OjRw/z73/82xhhz+fJl0717d+Pv72+t/RwTE2O++OIL8+STT5o+ffqYjz76yMTGxmZnsQEAwB2MPkbu81d74SUnJ7u0t/PfjRs3No888oiZNGmSad68uZWUct7bO/8/e/ZsExgY6BJL7dKlSyYuLs4Yk/KFvKenpzl69Kg5ffq0CQwMNPfdd59ZuXKlWbNmjalTp455/vnnXc7/3//+l2bZUydS7iT0L3InlqICAORqJo0lgDw8POTh4SFjjMLDw3XhwgVJKdN669evr549e+r111/XBx98IEmqWbOm8ufPrw0bNkiSypQpo0WLFqljx4766aef1KFDB/388886fPiwHn30UUlS+/btFRkZqV9++UVSyhqqzjVrr9ehQwf1799fX375pfbu3WuV+6+mLiNrnTlzRomJiS7rpaa1xrBzOnhUVJRsNpt+/fVXVa9eXX369FH37t01d+5cJSUlycvLS23bttWQIUMkSQsXLtTMmTOVnJysFStW6OLFi9ZU9j/++EMVK1bUgw8+aE1Hl1zXbnX++7777lNQUNBf1sUdnldxcXFasWKF+vbtK0nKly+fvvzyS3Xu3FlDhw7Vli1b5O/vrx49eui9997T/Pnz9dhjj8nHxyebSw4AAO4E9DHuDGnthedkt9tdluby8PDQqlWrtHnzZn333Xf64Ycf1LFjR7Vq1cp6rNT/r1mzpqKionT8+PE0l3rNnz+/ChUqJEnKkyeP/Pz8dODAAfn6+uqzzz6Tv7+/+vXrp/vvv18NGjTQwIEDXc6vUaNGmmW32+135B4Q9C9yqezMqgAAkNUuXbpk5syZY+rVq2eKFCliOnToYNauXWuMSRk1U6dOHbNmzRrzzjvvmOLFi5uvv/7aJCcnmwcffNB069bNGGPMn3/+aWw2m/nhhx9cHjsxMdHs2rXLnDt3zhhjTLt27UytWrVM2bJlTUBAgNm0adMN5XGOzrl06ZI5duzYDUsLIfs4HA6zePFiU6JECVO5cmWzbNkyM2nSJDNnzpwbjj127JiJi4szAwcONIUKFTJBQUFm1qxZplu3bmbJkiXGGGP69etnGjZs6DJzY8WKFaZ27dqmRo0a5umnnzY9evQwlSpVMhEREdYxzz//vAkNDTXJyclm/vz5plChQqZ///7WCK7caNKkSaZmzZrm5MmTxphrI+CcIxyNSX+ZLwAAgKxGHyN3Onz4sBk4cKD54IMPjDHGXL161RiT8vf96aefTL9+/UzPnj3NZ599Zowx5uLFi2bmzJnGw8PD5X4+LbGxsSYwMNC89dZbN/wuIiLCXL582SQlJZm9e/eaDh06mF69eplLly5ZbXvlyhVz4sSJ21jb3I3+Re5056XoAAC5Vlqjjy5cuKAVK1bo5MmTkqTVq1dr+vTp6tGjh9asWaOXX37Z2hyvbdu2iomJ0fHjxzV48GA9+eST6t+/v7Zv367g4GAdP35cMTExqlq1qmrUqKEPPvhA69atkyRdvHhRn332mebMmaPjx49LkubOnauxY8dq6tSp2r9/v0JDQ28Y3eUcnZM/f36VKVPmjtm4zR2cPXtWU6ZM0aOPPqrff/9dHTp00PLly/X7778rOjpac+fO1b59+3TixAmVK1dOffv2lZ+fn9avX68uXbpo/Pjxstls6ty5syRp6NChKlCggJYvXy5JOnz4sMaNG6cmTZpo3bp1mjZtmvr3769jx47p0KFDVjnmzZunJ598Una7XX369NHChQt16dIlnT9/Ps1ym5SlRjP/D5RJjDGKj49Xr169rNemczRc6g35smoTcwAAcGejj5G7mRyyybvTypUr9cADD2jAgAEKDg5Wo0aNlD9/fo0dO1b58+e32tbLy0ulSpWSlDKT3J3v/zMb/YtcLPtyKgAA/LW01qg15tpeBqnXBnWOrrh8+bLLxskrV640NpvNrFmzxiQnJ5uBAwea5s2bp3vNunXrmpEjR1r7HnTv3t00adLEPPbYY6Zly5Zm8eLFxhhjNm3aZFq3bm1q1Khhmjdvbnx8fEylSpXM22+/ne5IekaAuB9PT09r3eP9+/eb5s2bGw8PD5MvXz7j6+trVq1aZS5cuGC6detmChYsaI3Mi4qKMp07dzYtW7a0His2Ntb079/ftG7d2hiTsjmgzWYzGzZsMMakjBbq1q2bsdlsZurUqcYYYz744ANTsWJFc/jw4SysNQAAQO5FHwNOOXGTd+esjxdffNEsXrzYXLp06fZVGMhlmLEBAMixUq9ZalKNQHHuZZB6bVCbzabw8HAVKFBAmzdvtuL33HOPihcvrv3798tut6t8+fKKiYlRjx49NGLECE2bNk0LFy5UVFSUJKlBgwb6/fffFRkZKUmaMWOG6tevr08++US//fabdu7cKUkKDQ3VkiVL9MYbb+jBBx/UypUrFRYWpqeeespaC/X6sjMCJOuZ/19HOL21hI0xSkpKcnl+Of+9Z88eFS1aVN27d5fdbteoUaNUqVIllS5dWnPmzNHp06fVpk0beXt7q06dOipUqJCaN28uSfLz81OrVq30xx9/WI/r4+Oj6tWr68SJE4qMjFSpUqVUtmxZjRkzRrNnz9bw4cPVsGFDNW3aVEePHpUkPfTQQwoPD1dQUJBLHYybz8oAAADILvQx7mzJycnWXnnOv13FihVVo0YN1ahRQ88++6y+/PJLJSQkyGazqXLlyrp48aJ+/fVXSdI333yjBg0ayN/fX6+88oqaNWumf//739ZjS9L999+vzZs36+LFi2mWwXndSpUqaezYsfrwww+tvVa8vb01dOhQvfDCC+rSpYvy588vh8PBvT+QBhIbAIAcyeFwaPbs2erQoYMk142aDx48qEmTJqlnz5764IMPFB8fLynlxrBAgQLau3ev9cWvh4eHqlatqo0bNyo+Pl6jR49Wv3795OPjI7vdrsWLF2vEiBEaN26cpJSp4seOHbOmevv7++ull15S7969de7cOZ09e9YqR4ECBdSuXTsNHz5c9evXt8p9PTob2ce54Z/dbtfly5cVExPj0kY2m02enp6y2WyKiYlRTEyMbDaboqOjNWPGDHl4eMhmsyksLExLlizR5MmT5e/vr4iICEkpHUqbzaYmTZooOjraet54enrqrrvuUmJiotUJkqRq1arJy8tLa9askSR9+umnCg4O1tSpUxUbG6v7779fa9as0cyZMyVJxYoVk5TyvLq+k83zCgAAIGPoY9xZ0koGuMsm76kHMtntdtobSEuWzg8BACANV69eNRs2bDDh4eFWzOFwmHfffdfYbDaXY7du3Wpq1qxpmjRpYp566ilTvHhx88gjj5gDBw4YY4xp3bq1efDBB82FCxescyZPnmxq1KhhHePknNb76quvmsKFCxtjjDl9+rTx9vY2r7/+usuxcXFxTP92A84lApxLCOzatcs89dRTpkaNGqZKlSpm1KhR5uDBg9bx8fHx5sUXXzT+/v6mePHi5p577jEff/yx9ftt27YZDw8PExUVZcU6duxo+vTp47IJY2RkpClRooTLxuLh4eGmfv36ZuLEiVZs7969JjQ01IwaNcqKJSYm3lCP1EsgAAAAIOPoY+B6bPIO5C7M2AAAZKvk5GT16dNHzZo1U69evRQXFycpZQRSaGiovLy8rNExkvT8888rMDBQa9as0dtvv61PPvlE+/fv14wZMyRdm/Z7+vRp65xOnTrpwIEDOnz4sCQpMjJSERERypMnj/744w+tXbtW/fv3V2Jionx9fTV06FA1atTIZYRPoUKFVKhQIUZL5VBhYWHq3bu3tdGi3W7X+vXrNWDAAEVGRuqZZ57RW2+9pfr16ytPnjzWeUuXLtWCBQv05ptvas2aNapSpYoGDRpkja4KCgpSwYIF9fPPP1vn1KlTR0ePHnXZ4NvX11eNGze2NgaXpOLFi6t8+fL67LPPrFiVKlX07bffaurUqVbMWZ7Um/6lnp0BAACAjKGPcedhk3fgzkOvGQCQrTw8PHTXXXepWrVq2rZtm0aNGqV9+/ZJkgICAhQcHKxvvvlGkrR//37FxcWpXbt28vT0lCS1adNGnTt31vfffy9J6tq1q2JiYhQWFmZdIz4+XjabzZrau3TpUj399NOqWrWqGjZsKC8vLw0fPlx58+aVJL366qtq1qxZmp0JvnDOWZydgwMHDmjHjh1q06aNpJROzH/+8x/VqFFDH330kfr27asOHTqoe/fuKlOmjHX+lClTdO+99+rf//63qlevrlmzZik0NFSzZ89WbGysihcvrgYNGmjZsmXWOU2aNNGlS5e0a9cuK5Y3b141b97ceh5KUuHChTV69Gh98sknVszT01MBAQFp1sW57BUAAAD+GfoY7i/1MmGpGWOUnJx8w/5zdrtdV65cUWJiohXfsmWLOnbsqH379snhcGjp0qXy9fXVqFGjVKdOHdWuXVvVqlWTlDKgKSAgQDt37tSlS5c0ceJEtW/fXiNHjlRkZKS8vLy0detWSbKWKhs8eLBatGihsmXL6uWXX1bVqlVVtmxZSVKpUqXUs2dP/etf/1LhwoWtJWwB3D68cwIAsl1wcLAqVqyoAQMG6NSpUxo2bJiSkpJUsmRJNW3a1BoFnz9/fl28eNHqcEgpnZYGDRrozJkzOnnypMqUKaM6depoxowZ2r9/vy5duqQFCxZIkpYtW6YLFy6oXbt26tKli2bNmqXY2Fh99913KleunEuZrh9Ng6x3M23g7BxcuHBBvr6+VrxQoULat2+f7rvvPhUsWNCKp+4gnT17Vg6HQ+XLl5ckXblyRZLUq1cv7dy5UzExMZKkdu3aad26dUpKSpIkNWrUSP7+/po9e7YmT56sxo0b69ChQ2rTpo369etnjQiUpPr166tRo0a3+icAAADALaKP4d7Y5B3A3yGxAQDIdnXq1NHVq1dVoEABvf3224qIiFDPnj0lSS1btlRYWJji4+NVtmxZFStWTHv37tWlS5ckpdwgnjp1ShUrVlR0dLQkady4cTpz5ozatm2rEiVKKDAwUEuWLNGIESPk7e2tcuXKqXfv3rrvvvuUN29eORyOdKcFI/s42yAhIeGGUVnX27Bhg4KDg12SCi1atNALL7ygIUOGaNCgQXriiSf02muvacGCBdbzJzg42OqgODtPzZo10+HDh63O7T333KOTJ0/qwIEDkiQfHx+9+eabqlatmjZu3KjWrVvL399fNWvW1IcffsgUcgAAgByAPob7YpN3ADeDxAYAINsFBQWpbNmy2rdvn0qVKqVvvvlGq1at0vPPP6+SJUuqRIkSWrp0qSSpffv22rhxo+bOnStJunr1qr766itVqlTJmkb84IMP6osvvtDLL7+sLVu26LnnnlPbtm3VqVMna2SP82ZXSpn6zQ1nznPu3Dn5+vpqxYoVN4zKcnJ2chwOh44fP66CBQtaHYrZs2erb9++2rlzp86ePaurV6/q22+/1RNPPKEhQ4bIx8dHDRs21M8//yyHw2HtdbFhwwYVLVpUBQoUkCRrRsfq1aut61apUkXz58/XihUrNGnSJGtWSOrnFQAAALIPfQz3kJSUpI0bN+rgwYNWzGazyeFw6Mcff5Qka8DRtm3b1LVrV/3000/y9/fX+PHjNXDgQGuJsEaNGmn16tXWMmFSSrJix44dioyMlN1u1+jRozVr1ixNmTJFa9eu1YgRI6xlyVq0aKETJ05o+/btVlkKFy6sGTNm6Pz583r77bdvKH/qe3+WFAOyFq84AECOUL16dZ0/f16bNm1SzZo1NWvWLC1fvlxvv/22qlatqhUrVkiSBg0apAceeEAvvvii2rZtq6pVqyoiIkKjRo1y2RS6XLlyeuSRR1SjRg0rdv0UZjoaWcsYk+6sC4fD4TISKzk5WUWLFlVwcLC+/vprTZ48WS1atNDnn39uPZZ0rfNQu3Zt/f777y4dyFKlSmncuHHavHmzPv74Y02fPl3bt2/X9OnT9dlnn+nYsWPq37+/PD091b59e61atUpr167VzJkzNXjwYAUGBkqSihYtqp07d2rIkCFpljt1nXheAQAA5Bz0MXI2NnkH8E+Q2AAA5Ai1a9eWh4eHtSxQz5499dJLL2nr1q3avHmz1q1bJyllGaAJEyZo8eLFCgkJ0euvv65t27apSZMmNzwmU79zFpvNlu4oJrvd7rKOroeHh1atWqXNmzfru+++0w8//KCOHTuqVatW1mOl/n/NmjUVFRWl48ePp9nO+fPnt9a7zZMnj/z8/HTgwAH5+vrqs88+k7+/v/r166f7779fDRo00MCBA13Or1GjRpplt9vtjMwCAADIoehj5Gxs8g7gn/D8+0MAAMh8NWrUkL+/v3XDKUkdO3aUzWZTp06dFBkZqbi4OBUuXFgeHh5q2rSpmjZtah1rjLnh5pNORs5y5MgRvfLKK6pfv74ef/xxJSUlydPTU8YYrVq1SgsXLtTly5fVsWNH9erVS40bN9aMGTM0YsQILVq0SKVLl073satXr66SJUtq0aJFGjFihMvvjh8/Lj8/P+XJk0f79+/XwoUL1bp1azVp0kTGGLVs2VKhoaE6c+aMSpUqlcl/BQAAAGQV+hg5n3OT9yZNmlibvP/4448um7y/9dZbf7nJ+2uvvXbDJu9ly5ZVmTJlXDZ5HzBggNq1a6f8+fNr4MCBatmypZXQSC2tdgeQ85BqBADkCMWLF1dAQIB+/fVXRUVFSUqZmtyhQwcdP35cCQkJN2zKnHoNW248cwZnexw9elTr1693+Z3D4VB4eLi2bNki6dpaue+8845GjBih+Ph4ValSRSNHjtR//vMfORwODR06VIULF7aWCUiPj4+PevTooQULFujIkSNWfOXKlXrggQc0YMAABQcHq1GjRsqfP7/Gjh2r/PnzW88bLy8vK6mRnJzMPhkAAAC5AH2MnI9N3gHcKmZsAAByjLZt26pWrVrKnz+/JFlLEzm/cL5+5Aw3nDmLs30OHjyoypUrS5KmTp2qZ555RlLKJtx169bVtm3bdOXKFeXLl0/h4eGaNm2a3nvvPbVv316SVL9+fQ0cOFB169bVI488ooYNG2rlypV6/PHH//K6TzzxhLZt26a5c+dq0qRJkqQmTZqoT58+OnPmjP71r3/p3nvvtZ5f6Um9JBYAAADcG32MnC2tTd6bNm2q559/Xu3bt7c2ee/Ro4fat2+vzz//XHPnztWwYcPS3eS9Xr162rhxo2rXru2yH4pT6sQVy0sB7ovEBgAgx+jatetf/p5ORs7j3PDbw8PDap+KFStaHYhnn31WgYGBeuCBB+Tl5aXKlSvr559/1q+//qpmzZrpm2++UYMGDeTv769XXnlFy5cv1549e1SgQAHrse+//35NmTJFFy9eVMGCBW8og/O6lSpV0tixY9W/f3+1bdtWTZs2lbe3t4YOHepyvMPhYGNHAACAOwR9jJyvevXq+u2337Rp0yY1b95cs2bN0htvvKEDBw5Ym7z36NFDgwYN0uXLl/Xiiy/qhx9+UFhYmAoWLKhZs2bdsMl7uXLlXK6ROoFFmwO5A2lJAECOwhJAOVdabePh4SEPDw8ZYxQeHq4LFy5ISlkrt379+urZs6def/11ffDBB5JSNvnOnz+/NmzYIEkqU6aMFi1apI4dO+qnn35Shw4d9PPPP+vw4cN69NFHJUnt27dXZGSkfvnlF0nShQsXdPXq1TTL2KFDB/Xv319ffvmltZayczkBZ/ntdjudGQAAgDsIfYycjU3eAdwKm+HdHQAA3ILLly9rwYIFmj17tsLDw9WkSRONGTNGzZs31/vvv6/Zs2dr+vTp+t///qeJEyfqvffe04MPPqhu3brJw8NDX331lfbv369q1app6dKl6tixo/XYV69e1d69e1W+fHkVKVLESm6cO3dOiYmJ+vbbbxUaGupSHucorMuXL+v06dMqUqTIDWsmAwAAAMhZTp06pYEDB8rPz09z5syx4suXL1enTp2UJ08enTp1Kt17ezb7Bu5MLEUFAABu4HA4blhv9sKFC9qwYYPq1asnf39/rV69WtOnT9djjz2mNm3ayGazKW/evJJS1jKeOHGijh8/rsGDB+vkyZPq37+/ypYtq+DgYP3yyy+KiYlR1apVVaNGDX3wwQcqVKiQmjdvrosXL+rrr7/W9u3bNXjwYBUpUkRz587V2rVr5eHhoXbt2qlw4cLproecP39+lSlTJuv+WAAAAABumXOT982bNysqKkolS5Z02eTduR9Kaqn3ySCpAdyZmLEBAMAdIDk5Oc1NsY0x1p4TzkSGM2Fw5coV2e12K1mxatUqtW3bVr/88ouaNWumJ598Un/++afWrl2b5jXvvvtutWrVSi+99JK8vb3Vo0cPHT9+XJUrV9aRI0f09NNPq0uXLtq8ebOef/55RUdHy9fXV3v27JGvr6+GDRumvn37qlChQmmWmw4MAAAAkDt8//33ioqKUs+ePVW0aNEbfs/9P4DrMWMDAIA7QOqkxvUb512f8LDZbAoPD1eVKlX0yy+/qEWLFpKke+65R8WLF9f+/fvVokULlS9fXmvXrlWPHj0UEBCgsmXLKjAwUM2bN1fJkiXVoEED/f7774qMjFTlypU1Y8YMvfzyy5o5c6aKFCminTt3qkuXLgoNDdWSJUu0bt067d+/X2+++abq16+fZj2cZadTAwAAAOQebPIOIKPYPBwAgFzO4XBo9uzZ6tChg6SU2RtOBw8e1KRJk9SzZ0998MEHio+PlyRVqlRJBQoU0N69e62Ntz08PFS1alVt3LhR8fHxGj16tPr16ycfHx/Z7XYtXrxYI0aM0Lhx4ySlLEd17NgxHT9+XJLk7++vl156Sb1799a5c+d09uxZqxwFChRQu3btNHz4cCup4XA4bqgLHRoAAAAgd2JRGQAZQWIDAIBcJCkpSRs3btTBgwetmM1mk8Ph0I8//ihJ8vRMmbC5bds2de3aVT/99JP8/f01fvx4DRw4UGFhYZKkRo0aafXq1YqPj7cSCm3bttWOHTsUGRkpu92u0aNHa9asWZoyZYrWrl2rESNG6JtvvpEktWjRQidOnND27dutshQuXFgzZszQ+fPn9fbbb99Q/tSdmev3+AAAAACQezGICUBG8I0BAAC5RHJysvr06aNmzZqpV69eiouLk5TSQQgNDZWXl5c2bNhgHf/8888rMDBQa9as0dtvv61PPvlE+/fv14wZMyRJ999/vzZv3qzTp09b53Tq1EkHDhzQ4cOHJUmRkZGKiIhQnjx59Mcff2jt2rXq37+/EhMT5evrq6FDh6pRo0YuCYtChQqpUKFCzMgAAAAAAAC3hMQGAAC5hIeHh+666y5Vq1ZN27Zt06hRo7Rv3z5JUkBAgIKDg63ZFPv371dcXJzatWtnzeBo06aNOnfurO+//15Syjq3MTEx1gwOSdbsjb1790qSli5dqqefflpVq1ZVw4YN5eXlpeHDh1sbjr/66qtq1qxZmgkLZmQAAAAAAIBbwTcKAADkIsHBwapYsaIGDBigU6dOadiwYUpKSlLJkiXVtGlTLV++XJKUP39+Xbx40UpqSCmJkQYNGujMmTM6efKkypQpozp16mjGjBnav3+/Ll26pAULFkiSli1bpgsXLqhdu3bq0qWLZs2apdjYWH333XcqV66cS5lYKxcAAAAAANxOJDYAAMhF6tSpo6tXr6pAgQJ6++23FRERoZ49e0qSWrZsqbCwMMXHx6ts2bIqVqyY9u7dq0uXLklKWQbq1KlTqlixoqKjoyVJ48aN05kzZ9S2bVuVKFFCgYGBWrJkiUaMGCFvb2+VK1dOvXv31n333ae8efPK4XDckMhgeSkAAAAAAHA7ef79IQAAwF0EBQWpbNmy2rdvn0qVKqVvvvlGTZs21fPPP6/27durRIkSWrp0qXr06KH27dvr888/19y5czVs2DBdvXpVX331lSpVqqRq1apJkh588EHVq1dPGzduVO3atVWjRo0brulMZNhsNpaXAgAAAAAAmY7EBgAAuUz16tX122+/adOmTWrevLlmzZqlN954QwcOHFDVqlW1YsUK9ejRQ4MGDdLly5f14osv6ocfflBYWJgKFiyoWbNmKU+ePNbjlStXLs3lpZwzMZiRAQAAAAAAspLNsPA1AAC5ypo1azRu3Dh169ZNI0aMkJSyJ8aQIUMUFRWlUqVK6fDhw5Kk5ORkbd68WStXrlStWrXUqVMn5cuX74bHTJ3IAAAAAAAAyE4kNgAAyGVOnTqlgQMHys/PT3PmzLHiy5cvV6dOnZQnTx6dOnVKhQsXTvN8khgAAAAAACAnYyFsAABymeLFiysgIEC//vqroqKiJKXMzOjQoYOOHz+uhISEG5IaxhiXvTIAAAAAAAByKvbYAAAgF2rbtq1q1aql/PnzS5I8PDwkSaVKlZJ046wMkhkAAAAAAMBdsBQVAAAAAAAAAABwGyxFBQBALsXYBQAAAAAAkBuR2AAAIJdieSkAAAAAAJAbkdgAAAAAAAAAAABug8QGAAAAAAAAAABwGyQ2AAAAAAAAAACA2yCxAQAAAAAAAAAA3AaJDQAAAAAAAAAA4DZIbAAAAAAAAAAAALdBYgMAAAAAAAAAALgNEhsAAAAAAAAAAMBt/B+UYSheD4vvQQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not all_results.empty:\n",
    "    def agg_metrics(df):\n",
    "        return df.groupby([\"dataset\", \"model\", \"metric\"]).agg(\n",
    "            mean=(\"value\", \"mean\"),\n",
    "            std=(\"value\", \"std\"),\n",
    "            mean_time_s=(\"time_s\", \"mean\")\n",
    "        ).reset_index()\n",
    "\n",
    "    summary = agg_metrics(all_results)\n",
    "    summary_nonan = summary.dropna(subset=[\"mean\"]).copy()\n",
    "\n",
    "    print(summary_nonan.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "    # Quick plots\n",
    "    datasets = summary_nonan[\"dataset\"].unique()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # HELOC Accuracy\n",
    "    if \"HELOC\" in datasets:\n",
    "        s = summary_nonan[(summary_nonan.dataset==\"HELOC\") & (summary_nonan.metric==\"Accuracy\")]\n",
    "        axes[0].bar(s[\"model\"], s[\"mean\"], yerr=s[\"std\"], color=\"#4c72b0\")\n",
    "        axes[0].set_title(\"HELOC: Accuracy (5-fold)\")\n",
    "        axes[0].set_ylim(0, 1)\n",
    "        axes[0].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "        axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "    # Adult Accuracy\n",
    "    if \"Adult\" in datasets:\n",
    "        s = summary_nonan[(summary_nonan.dataset==\"Adult\") & (summary_nonan.metric==\"Accuracy\")]\n",
    "        axes[1].bar(s[\"model\"], s[\"mean\"], yerr=s[\"std\"], color=\"#55a868\")\n",
    "        axes[1].set_title(\"Adult: Accuracy (5-fold)\")\n",
    "        axes[1].set_ylim(0, 1)\n",
    "        axes[1].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "        axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "    # Breast Cancer AUC\n",
    "    if \"BreastCancer\" in datasets:\n",
    "        s = summary_nonan[(summary_nonan.dataset==\"BreastCancer\") & (summary_nonan.metric==\"AUC\")]\n",
    "        axes[2].bar(s[\"model\"], s[\"mean\"], yerr=s[\"std\"], color=\"#c44e52\")\n",
    "        axes[2].set_title(\"Breast Cancer: AUC (5-fold)\")\n",
    "        axes[2].set_ylim(0.5, 1)\n",
    "        axes[2].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "        axes[2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "    # California RMSE\n",
    "    if \"California\" in datasets:\n",
    "        s = summary_nonan[(summary_nonan.dataset==\"California\") & (summary_nonan.metric==\"RMSE\")]\n",
    "        axes[3].bar(s[\"model\"], s[\"mean\"], yerr=s[\"std\"], color=\"#8172b2\")\n",
    "        axes[3].set_title(\"California Housing: RMSE (5-fold)\")\n",
    "        axes[3].invert_yaxis()  # lower is better\n",
    "        axes[3].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "        axes[3].tick_params(axis='x', rotation=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to summarize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3098dd",
   "metadata": {
    "_cell_guid": "d564952c-d8a5-448c-b027-67028e1f0976",
    "_uuid": "14aa81d0-8dcf-47e7-b9b7-7a47a87209aa",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Interpretability Showcase\n",
    "Single held-out split per dataset to demonstrate:\n",
    "- NousNet: honest LOO, MSE, fidelity-driven pruning, per-sample explanation; Softmax+Prototype: prototype analytics.\n",
    "- XGBoost: SHAP global top features.\n",
    "- EBM: global and local explanations.\n",
    "- MLP: global permutation importance.\n",
    "- KAN: global importance from learned bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97da124d",
   "metadata": {
    "_cell_guid": "d1ec5e66-7cb6-4791-a146-7cbf49344cdb",
    "_uuid": "4325ae82-1474-40e5-a3a4-4925e0b7281f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def showcase_interpretability_classification(\n",
    "    X: pd.DataFrame, y: pd.Series, feature_names: List[str], class_names: List[str], dataset_tag: str\n",
    "):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"Interpretability Showcase — {dataset_tag} (classification)\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X.values, y.values, test_size=0.2, random_state=42, stratify=y.values)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.2, random_state=42, stratify=y_tr)\n",
    "\n",
    "    # NousNet (Fixed)\n",
    "    y_proba_fixed, info_fixed = fit_nous(\n",
    "        X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"classification\",\n",
    "        epochs=1000, patience=200, batch_size=64, lr=1e-3,\n",
    "        rule_method=\"fixed\", use_calibrators=False, use_prototypes=False\n",
    "    )\n",
    "    y_pred_fixed = np.argmax(y_proba_fixed, axis=1)\n",
    "    auc_fixed = safe_auc(y_te, y_proba_fixed)\n",
    "    print(f\"NousNet (Fixed) Accuracy: {accuracy_score(y_te, y_pred_fixed):.4f} | AUC: {auc_fixed:.4f}\")\n",
    "\n",
    "    # NousNet (Softmax + prototypes)\n",
    "    y_proba_soft, info_soft = fit_nous(\n",
    "        X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"classification\",\n",
    "        epochs=1000, patience=200, batch_size=64, lr=1e-3,\n",
    "        rule_method=\"softmax\", use_calibrators=True, use_prototypes=True\n",
    "    )\n",
    "    y_pred_soft = np.argmax(y_proba_soft, axis=1)\n",
    "    auc_soft = safe_auc(y_te, y_proba_soft)\n",
    "    print(f\"NousNet (Softmax+Proto) Accuracy: {accuracy_score(y_te, y_pred_soft):.4f} | AUC: {auc_soft:.4f}\")\n",
    "\n",
    "    # XGBoost\n",
    "    y_proba_xgb = fit_xgboost(X_tr, y_tr, X_val, y_val, X_te, \"classification\")\n",
    "    y_pred_xgb = np.argmax(y_proba_xgb, axis=1)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_score(y_te, y_pred_xgb):.4f} | AUC: {safe_auc(y_te, y_proba_xgb):.4f}\")\n",
    "\n",
    "    # EBM\n",
    "    y_proba_ebm, ebm_model = fit_ebm(X_tr, y_tr, X_val, y_val, X_te, \"classification\")\n",
    "    y_pred_ebm = np.argmax(y_proba_ebm, axis=1)\n",
    "    print(f\"EBM Accuracy: {accuracy_score(y_te, y_pred_ebm):.4f} | AUC: {safe_auc(y_te, y_proba_ebm):.4f}\")\n",
    "\n",
    "    # MLP\n",
    "    mlp_model, _ = train_mlp(X_tr, y_tr, X_val, y_val, \"classification\", max_epochs=1000, patience=200, lr=1e-3, batch_size=64)\n",
    "    y_proba_mlp = predict_mlp(mlp_model, X_te, \"classification\")\n",
    "    y_pred_mlp = np.argmax(y_proba_mlp, axis=1)\n",
    "    auc_mlp = safe_auc(y_te, y_proba_mlp)\n",
    "    print(f\"MLP Accuracy: {accuracy_score(y_te, y_pred_mlp):.4f} | AUC: {auc_mlp:.4f}\")\n",
    "\n",
    "    # KAN (binary only)\n",
    "    if len(np.unique(y_tr)) == 2:\n",
    "        kan_model, _ = train_manual_kan(\n",
    "            X_tr, y_tr, X_val, y_val, task_type=\"classification\",\n",
    "            num_classes=2, K1=8, K2=8, M=16, lr=3e-3, weight_decay=1e-4, max_epochs=300, patience=50, batch_size=128\n",
    "        )\n",
    "        y_proba_kan = predict_manual_kan(kan_model, X_te, \"classification\")\n",
    "        print(f\"KAN Accuracy: {accuracy_score(y_te, np.argmax(y_proba_kan, axis=1)):.4f} | AUC: {safe_auc(y_te, y_proba_kan):.4f}\")\n",
    "        imp = kan_model.global_importance()\n",
    "        order = np.argsort(-imp)[:10]\n",
    "        print(\"KAN top-10 global features:\")\n",
    "        for j in order:\n",
    "            print(f\"  {feature_names[j]}: {imp[j]:.4f}\")\n",
    "\n",
    "    # Per-sample explanations with NousNet (Softmax+Proto)\n",
    "    idx = 0\n",
    "    print(\"\\n— NousNet (Softmax+Proto) explanation (held-out sample) —\")\n",
    "    print(generate_enhanced_explanation(info_soft[\"model\"], X_te[idx], y_te[idx], feature_names, class_names, use_pruning=False))\n",
    "    t_prune = select_pruning_threshold_global_bs(info_soft[\"model\"], X_val, target_fidelity=0.99, task_type=\"classification\", max_samples=200, device=DEVICE)\n",
    "    print(\"\\nWith pruning:\")\n",
    "    print(generate_enhanced_explanation(info_soft[\"model\"], X_te[idx], y_te[idx], feature_names, class_names, use_pruning=True, pruning_threshold=t_prune))\n",
    "\n",
    "    # MSE + fidelity metrics\n",
    "    fid = explanation_fidelity_metrics(info_soft[\"model\"], X_te[idx], feature_names, class_names=class_names)\n",
    "    print(\"\\nNousNet explanation fidelity metrics:\", fid)\n",
    "\n",
    "    # Prototype analytics (global and per-sample)\n",
    "    try:\n",
    "        pr = prototype_report_global(info_soft[\"model\"], X_tr, y=y_tr, class_names=class_names, top_k_rules=8, top_k_facts_per_rule=2, trace_to_base=True)\n",
    "        print(\"\\nPrototype report (top by top1_count):\")\n",
    "        pr_small = pr[[\"proto\", \"primary_class\", \"class_entropy\", \"mean_activation\", \"top1_count\"]].sort_values(\"top1_count\", ascending=False).head(5)\n",
    "        print(pr_small.to_string(index=False))\n",
    "        top_proto = int(pr_small.iloc[0][\"proto\"])\n",
    "        print(\"\\nDescribe best prototype:\")\n",
    "        print(describe_prototype(info_soft[\"model\"], proto_id=top_proto, feature_names=feature_names, class_names=class_names, top_k_rules=8, top_k_facts_per_rule=2, trace_to_base=True))\n",
    "        contrib_df = prototype_contribution_df(info_soft[\"model\"], X_te[idx], class_names=class_names, top_k=5)\n",
    "        print(\"\\nPrototype contributions for the sample:\")\n",
    "        print(contrib_df.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(\"Prototype analytics failed:\", e)\n",
    "\n",
    "    # XGBoost SHAP (global)\n",
    "    print(\"\\n— XGBoost SHAP (global) —\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=2000, learning_rate=0.03, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "        random_state=42, tree_method=\"hist\", n_jobs=0,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "    xgb_model.fit(X_tr, y_tr)\n",
    "    expl = shap.TreeExplainer(xgb_model)\n",
    "    shap_vals = expl.shap_values(X_te)\n",
    "    if isinstance(shap_vals, list):\n",
    "        sh = np.sum([np.abs(sv).mean(axis=0) for sv in shap_vals], axis=0)\n",
    "    else:\n",
    "        sh = np.abs(shap_vals).mean(axis=0)\n",
    "    order = np.argsort(-sh)[:10]\n",
    "    print(\"Top-10 global features by mean |SHAP|:\")\n",
    "    for i in order:\n",
    "        print(f\"  {feature_names[i]}: {sh[i]:.4f}\")\n",
    "\n",
    "    # EBM global and local\n",
    "    print(\"\\n— EBM (global & local) —\")\n",
    "    g = ebm_model.explain_global()\n",
    "    try:\n",
    "        imp = np.array(g.data()[\"overall\"][\"importance\"])\n",
    "        names = np.array(g.data()[\"overall\"][\"names\"])\n",
    "        order = np.argsort(-imp)[:10]\n",
    "        print(\"Top-10 global EBM terms:\")\n",
    "        for i in order:\n",
    "            print(f\"  {names[i]}: {imp[i]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"EBM global explain failed:\", e)\n",
    "    l = ebm_model.explain_local(X_te[idx:idx+1], [int(y_te[idx])])\n",
    "    try:\n",
    "        scores = l.data(0)[\"scores\"]\n",
    "        terms = l.data(0)[\"names\"]\n",
    "        order = np.argsort(-np.abs(scores))[:10]\n",
    "        print(\"Top-10 local EBM contributions:\")\n",
    "        for i in order:\n",
    "            print(f\"  {terms[i]}: {scores[i]:+.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"EBM local explain failed:\", e)\n",
    "\n",
    "    # MLP permutation importance (global)\n",
    "    print(\"\\n— MLP permutation importance (global) —\")\n",
    "    base_proba = predict_mlp(mlp_model, X_val, \"classification\")\n",
    "    base_acc = accuracy_score(y_val, np.argmax(base_proba, axis=1))\n",
    "    rng = np.random.default_rng(42)\n",
    "    pi = []\n",
    "    for j in range(X_val.shape[1]):\n",
    "        Xv_perm = X_val.copy()\n",
    "        rng.shuffle(Xv_perm[:, j])\n",
    "        proba_perm = predict_mlp(mlp_model, Xv_perm, \"classification\")\n",
    "        acc_perm = accuracy_score(y_val, np.argmax(proba_perm, axis=1))\n",
    "        pi.append(base_acc - acc_perm)\n",
    "    order = np.argsort(-np.array(pi))[:10]\n",
    "    print(\"Top-10 features by permutation importance (ΔAccuracy):\")\n",
    "    for j in order:\n",
    "        print(f\"  {feature_names[j]}: {pi[j]:.4f}\")\n",
    "\n",
    "\n",
    "def showcase_interpretability_regression(\n",
    "    X: pd.DataFrame, y: pd.Series, feature_names: List[str], dataset_tag: str\n",
    "):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"Interpretability Showcase — {dataset_tag} (regression)\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.2, random_state=42)\n",
    "\n",
    "    # NousNet (Fixed)\n",
    "    y_pred_fixed, info_fixed = fit_nous(\n",
    "        X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"regression\",\n",
    "        epochs=1000, patience=200, batch_size=64, lr=1e-3, rule_method=\"fixed\", use_calibrators=False, use_prototypes=False\n",
    "    )\n",
    "    print(f\"NousNet (Fixed): RMSE={rmse(y_te, y_pred_fixed):.4f} MAE={mean_absolute_error(y_te, y_pred_fixed):.4f} R²={r2_score(y_te, y_pred_fixed):.4f}\")\n",
    "\n",
    "    # NousNet (Softmax)\n",
    "    y_pred_soft, info_soft = fit_nous(\n",
    "        X_tr, y_tr, X_val, y_val, X_te, feature_names, task_type=\"regression\",\n",
    "        epochs=1000, patience=200, batch_size=64, lr=1e-3, rule_method=\"softmax\", use_calibrators=True, use_prototypes=False\n",
    "    )\n",
    "    print(f\"NousNet (Softmax): RMSE={rmse(y_te, y_pred_soft):.4f} MAE={mean_absolute_error(y_te, y_pred_soft):.4f} R²={r2_score(y_te, y_pred_soft):.4f}\")\n",
    "\n",
    "    # XGBoost\n",
    "    y_pred_xgb = fit_xgboost(X_tr, y_tr, X_val, y_val, X_te, \"regression\")\n",
    "    print(f\"XGBoost: RMSE={rmse(y_te, y_pred_xgb):.4f} MAE={mean_absolute_error(y_te, y_pred_xgb):.4f} R²={r2_score(y_te, y_pred_xgb):.4f}\")\n",
    "\n",
    "    # EBM\n",
    "    y_pred_ebm, ebm_model = fit_ebm(X_tr, y_tr, X_val, y_val, X_te, \"regression\")\n",
    "    print(f\"EBM: RMSE={rmse(y_te, y_pred_ebm):.4f} MAE={mean_absolute_error(y_te, y_pred_ebm):.4f} R²={r2_score(y_te, y_pred_ebm):.4f}\")\n",
    "\n",
    "    # MLP\n",
    "    mlp_model, _ = train_mlp(X_tr, y_tr, X_val, y_val, \"regression\", max_epochs=1000, patience=200, lr=1e-3, batch_size=64)\n",
    "    y_pred_mlp = predict_mlp(mlp_model, X_te, \"regression\")\n",
    "    print(f\"MLP: RMSE={rmse(y_te, y_pred_mlp):.4f} MAE={mean_absolute_error(y_te, y_pred_mlp):.4f} R²={r2_score(y_te, y_pred_mlp):.4f}\")\n",
    "\n",
    "    # KAN\n",
    "    kan_model, _ = train_manual_kan(\n",
    "        X_tr, y_tr, X_val, y_val, task_type=\"regression\",\n",
    "        num_classes=1, K1=8, K2=8, M=16, lr=3e-3, weight_decay=1e-4, max_epochs=300, patience=50, batch_size=64\n",
    "    )\n",
    "    y_pred_kan = predict_manual_kan(kan_model, X_te, \"regression\")\n",
    "    print(f\"KAN: RMSE={rmse(y_te, y_pred_kan):.4f} MAE={mean_absolute_error(y_te, y_pred_kan):.4f} R²={r2_score(y_te, y_pred_kan):.4f}\")\n",
    "    imp = kan_model.global_importance()\n",
    "    order = np.argsort(-imp)[:10]\n",
    "    print(\"KAN top-10 global features:\")\n",
    "    for j in order:\n",
    "        print(f\"  {feature_names[j]}: {imp[j]:.4f}\")\n",
    "\n",
    "    # Per-sample NousNet explanation (Softmax)\n",
    "    idx = 0\n",
    "    print(\"\\n— NousNet (Softmax) explanation (held-out sample) —\")\n",
    "    print(generate_enhanced_explanation(info_soft[\"model\"], X_te[idx], y_te[idx], feature_names, y_scaler=None, use_pruning=False))\n",
    "    t_prune = select_pruning_threshold_global_bs(info_soft[\"model\"], X_val, task_type=\"regression\", tol_reg=0.05, max_samples=200, device=DEVICE)\n",
    "    print(\"\\nWith pruning:\")\n",
    "    print(generate_enhanced_explanation(info_soft[\"model\"], X_te[idx], y_te[idx], feature_names, y_scaler=None, use_pruning=True, pruning_threshold=t_prune))\n",
    "\n",
    "    # XGBoost SHAP (global)\n",
    "    print(\"\\n— XGBoost SHAP (global) —\")\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=2000, learning_rate=0.03, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "        random_state=42, tree_method=\"hist\", n_jobs=0,\n",
    "        eval_metric=\"rmse\"\n",
    "    )\n",
    "    xgb_model.fit(X_tr, y_tr)\n",
    "    expl = shap.Explainer(xgb_model, X_tr)\n",
    "    shap_vals = expl(X_te[:256])\n",
    "    sv = np.abs(shap_vals.values).mean(axis=0)\n",
    "    order = np.argsort(-sv)[:10]\n",
    "    print(\"Top-10 global features by mean |SHAP|:\")\n",
    "    for i in order:\n",
    "        print(f\"  {feature_names[i]}: {sv[i]:.4f}\")\n",
    "\n",
    "    # EBM (global & local)\n",
    "    print(\"\\n— EBM (global & local) —\")\n",
    "    g = ebm_model.explain_global()\n",
    "    try:\n",
    "        imp_ebm = np.array(g.data()[\"overall\"][\"importance\"])\n",
    "        names = np.array(g.data()[\"overall\"][\"names\"])\n",
    "        order = np.argsort(-imp_ebm)[:10]\n",
    "        print(\"Top-10 global EBM terms:\")\n",
    "        for i in order:\n",
    "            print(f\"  {names[i]}: {imp_ebm[i]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"EBM global explain failed:\", e)\n",
    "    l = ebm_model.explain_local(X_te[idx:idx+1], [float(y_te[idx])])\n",
    "    try:\n",
    "        scores = l.data(0)[\"scores\"]\n",
    "        terms = l.data(0)[\"names\"]\n",
    "        order = np.argsort(-np.abs(scores))[:10]\n",
    "        print(\"Top-10 local EBM contributions:\")\n",
    "        for i in order:\n",
    "            print(f\"  {terms[i]}: {scores[i]:+.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"EBM local explain failed:\", e)\n",
    "\n",
    "    # MLP permutation importance (global - ΔRMSE)\n",
    "    print(\"\\n— MLP permutation importance —\")\n",
    "    base_pred = predict_mlp(mlp_model, X_val, \"regression\")\n",
    "    base_rmse = rmse(y_val, base_pred)\n",
    "    rng = np.random.default_rng(42)\n",
    "    pi = []\n",
    "    for j in range(X_val.shape[1]):\n",
    "        Xv_perm = X_val.copy()\n",
    "        rng.shuffle(Xv_perm[:, j])\n",
    "        pred_perm = predict_mlp(mlp_model, Xv_perm, \"regression\")\n",
    "        pi.append(base_rmse - rmse(y_val, pred_perm))\n",
    "    order = np.argsort(-np.array(pi))[:10]\n",
    "    print(\"Top-10 features by permutation importance (ΔRMSE positive = worse when permuted):\")\n",
    "    for j in order:\n",
    "        print(f\"  {feature_names[j]}: {pi[j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8dec3a",
   "metadata": {
    "_cell_guid": "d867bb92-d1c8-4b70-a62f-30502d6919a7",
    "_uuid": "6444702a-ec9b-4949-8d15-4fa50b859c68",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Run interpretability showcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d1c3fa",
   "metadata": {
    "_cell_guid": "5d2d946e-ffa6-471f-a238-d7df3f8a2ae2",
    "_uuid": "24316b39-89c8-4c3a-abb1-14fa702f9bb7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HELOC from https://raw.githubusercontent.com/benoitparis/explainable-challenge/refs/heads/master/heloc_dataset_v1.csv\n",
      "\n",
      "====================================================================================================\n",
      "Interpretability Showcase — HELOC (classification)\n",
      "====================================================================================================\n",
      "Epoch [1/1000] train=0.6141 val=0.5483 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5638 val=0.5424 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5574 val=0.5405 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.5557 val=0.5400 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5559 val=0.5397 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5537 val=0.5400 l0=0.0000\n",
      "Epoch [15/1000] train=0.5497 val=0.5389 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.5476 val=0.5360 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5449 val=0.5386 l0=0.0000\n",
      "Epoch [30/1000] train=0.5339 val=0.5414 l0=0.0000\n",
      "Epoch [40/1000] train=0.5245 val=0.5555 l0=0.0000\n",
      "Epoch [50/1000] train=0.5136 val=0.5663 l0=0.0000\n",
      "Epoch [60/1000] train=0.5046 val=0.5702 l0=0.0000\n",
      "Epoch [70/1000] train=0.4930 val=0.5816 l0=0.0000\n",
      "Epoch [80/1000] train=0.4859 val=0.5980 l0=0.0000\n",
      "Epoch [90/1000] train=0.4789 val=0.6044 l0=0.0000\n",
      "Epoch [100/1000] train=0.4667 val=0.6158 l0=0.0000\n",
      "Epoch [110/1000] train=0.4600 val=0.6387 l0=0.0000\n",
      "Epoch [120/1000] train=0.4492 val=0.6605 l0=0.0000\n",
      "Epoch [130/1000] train=0.4469 val=0.6724 l0=0.0000\n",
      "Epoch [140/1000] train=0.4372 val=0.6781 l0=0.0000\n",
      "Epoch [150/1000] train=0.4289 val=0.7269 l0=0.0000\n",
      "Epoch [160/1000] train=0.4240 val=0.7336 l0=0.0000\n",
      "Epoch [170/1000] train=0.4194 val=0.7522 l0=0.0000\n",
      "Epoch [180/1000] train=0.4101 val=0.7679 l0=0.0000\n",
      "Epoch [190/1000] train=0.4029 val=0.7844 l0=0.0000\n",
      "Epoch [200/1000] train=0.3990 val=0.8032 l0=0.0000\n",
      "Epoch [210/1000] train=0.3941 val=0.8221 l0=0.0000\n",
      "Early stopping at epoch 216 (best val=0.5360)\n",
      "Restored best model (val=0.5360)\n",
      "NousNet (Fixed) Accuracy: 0.7156 | AUC: 0.7866\n",
      "Epoch [1/1000] train=0.6219 val=0.5598 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5708 val=0.5497 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.5689 val=0.5486 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.5663 val=0.5438 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.5660 val=0.5401 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.5609 val=0.5362 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5607 val=0.5350 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5569 val=0.5472 l0=0.0000\n",
      "Epoch [15/1000] train=0.5601 val=0.5340 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.5556 val=0.5367 l0=0.0000\n",
      "Epoch [30/1000] train=0.5511 val=0.5334 l0=0.0000 (*)\n",
      "Epoch [34/1000] train=0.5523 val=0.5327 l0=0.0000 (*)\n",
      "Epoch [38/1000] train=0.5563 val=0.5318 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.5477 val=0.5304 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.5457 val=0.5342 l0=0.0000\n",
      "Epoch [59/1000] train=0.5428 val=0.5289 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.5440 val=0.5373 l0=0.0000\n",
      "Epoch [70/1000] train=0.5420 val=0.5320 l0=0.0000\n",
      "Epoch [72/1000] train=0.5410 val=0.5272 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.5376 val=0.5353 l0=0.0000\n",
      "Epoch [90/1000] train=0.5356 val=0.5363 l0=0.0000\n",
      "Epoch [100/1000] train=0.5351 val=0.5414 l0=0.0000\n",
      "Epoch [110/1000] train=0.5328 val=0.5371 l0=0.0000\n",
      "Epoch [120/1000] train=0.5288 val=0.5374 l0=0.0000\n",
      "Epoch [130/1000] train=0.5215 val=0.5507 l0=0.0000\n",
      "Epoch [140/1000] train=0.5205 val=0.5546 l0=0.0000\n",
      "Epoch [150/1000] train=0.5139 val=0.5533 l0=0.0000\n",
      "Epoch [160/1000] train=0.5119 val=0.5789 l0=0.0000\n",
      "Epoch [170/1000] train=0.5062 val=0.5642 l0=0.0000\n",
      "Epoch [180/1000] train=0.5030 val=0.5782 l0=0.0000\n",
      "Epoch [190/1000] train=0.4976 val=0.5752 l0=0.0000\n",
      "Epoch [200/1000] train=0.4959 val=0.5800 l0=0.0000\n",
      "Epoch [210/1000] train=0.4882 val=0.6052 l0=0.0000\n",
      "Epoch [220/1000] train=0.4850 val=0.5997 l0=0.0000\n",
      "Epoch [230/1000] train=0.4794 val=0.6003 l0=0.0000\n",
      "Epoch [240/1000] train=0.4782 val=0.6022 l0=0.0000\n",
      "Epoch [250/1000] train=0.4693 val=0.6015 l0=0.0000\n",
      "Epoch [260/1000] train=0.4639 val=0.6217 l0=0.0000\n",
      "Epoch [270/1000] train=0.4628 val=0.6165 l0=0.0000\n",
      "Early stopping at epoch 272 (best val=0.5272)\n",
      "Restored best model (val=0.5272)\n",
      "NousNet (Softmax+Proto) Accuracy: 0.7271 | AUC: 0.7931\n",
      "XGBoost Accuracy: 0.7208 | AUC: 0.7954\n",
      "EBM Accuracy: 0.7290 | AUC: 0.7973\n",
      "MLP Accuracy: 0.7113 | AUC: 0.7908\n",
      "KAN Accuracy: 0.7256 | AUC: 0.7946\n",
      "KAN top-10 global features:\n",
      "  MSinceMostRecentInqexcl7days: 12.0006\n",
      "  ExternalRiskEstimate: 6.7751\n",
      "  AverageMInFile: 6.4706\n",
      "  NumBank2NatlTradesWHighUtilization: 5.5750\n",
      "  NetFractionRevolvingBurden: 5.1842\n",
      "  NumSatisfactoryTrades: 4.5393\n",
      "  PercentInstallTrades: 3.4615\n",
      "  MaxDelq2PublicRecLast12M: 3.4304\n",
      "  MSinceMostRecentDelq: 3.1479\n",
      "  PercentTradesNeverDelq: 2.4327\n",
      "\n",
      "— NousNet (Softmax+Proto) explanation (held-out sample) —\n",
      "MODEL: SOFTMAX rules | TASK: CLASSIFICATION\n",
      "SAMPLE PREDICTION: Bad\n",
      "  - Confidence: 0.842\n",
      "  - Ground Truth: Bad\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B2/R2: Δmargin=-0.274 [-] | 0.39 AND + 0.30 OR + 0.31 k-of-n\n",
      "    F1, F10  →  [F1] β( [L−R](x̃) = (0.48·MSinceOldestTradeOpen + 0.42·MSinceMostRecentDelq + 0.36·ExternalRiskEstimate + 0.36·PercentTradesWBalance) − (0.20·NetFractionInstallBurden + 0.17·PercentInstallTrades + 0.10·MaxDelq2PublicRecLast12M + 0.09·PercentTradesNeverDelq) > -0.15; k=1.04, ν=0.67 )  where x̃ are calibrated features | [F10] β( [L−R](x̃) = (0.57·AverageMInFile + 0.40·MSinceMostRecentInqexcl7days + 0.34·MSinceMostRecentDelq + 0.26·PercentTradesNeverDelq) − (0.55·PercentTradesWBalance + 0.36·NumInqLast6M + 0.22·NumTrades90Ever2DerogPubRec + 0.21·PercentInstallTrades) > -0.17; k=1.03, ν=0.74 )  where x̃ are calibrated features\n",
      "  • B1/R13: Δmargin=-0.094 [-] | 0.32 AND + 0.31 OR + 0.37 k-of-n\n",
      "    F12, F3  →  [F12] β( [L−R](x̃) = (0.26·NetFractionRevolvingBurden + 0.23·NumRevolvingTradesWBalance + 0.18·PercentInstallTrades + 0.14·NumTrades90Ever2DerogPubRec) − (0.30·MSinceMostRecentDelq + 0.28·ExternalRiskEstimate + 0.28·NumSatisfactoryTrades + 0.27·MSinceMostRecentTradeOpen) > 0.14; k=0.94, ν=0.69 )  where x̃ are calibrated features | [F3] β( [L−R](x̃) = (0.47·NumTotalTrades + 0.44·AverageMInFile + 0.43·NumInstallTradesWBalance + 0.40·NumInqLast6Mexcl7days) − (0.52·NetFractionRevolvingBurden + 0.49·NumSatisfactoryTrades + 0.29·NumBank2NatlTradesWHighUtilization + 0.23·NumRevolvingTradesWBalance) > -0.01; k=1.25, ν=0.73 )  where x̃ are calibrated features\n",
      "  • B2/R8: Δmargin=+0.090 [+] | 0.31 AND + 0.30 OR + 0.39 k-of-n\n",
      "    F11, F14  →  [F11] β( [L−R](x̃) = (0.56·AverageMInFile + 0.47·NetFractionInstallBurden + 0.46·MaxDelqEver + 0.32·NumSatisfactoryTrades) − (0.59·MSinceMostRecentInqexcl7days + 0.52·NumInqLast6Mexcl7days + 0.46·MSinceMostRecentDelq + 0.39·NumInqLast6M) > 0.07; k=1.19, ν=0.80 )  where x̃ are calibrated features | [F14] β( [L−R](x̃) = (0.33·NetFractionRevolvingBurden + 0.26·NetFractionInstallBurden + 0.17·NumTotalTrades + 0.12·MSinceMostRecentTradeOpen) − (0.40·ExternalRiskEstimate + 0.31·NumSatisfactoryTrades + 0.28·PercentTradesNeverDelq + 0.17·NumRevolvingTradesWBalance) > 0.10; k=0.98, ν=0.68 )  where x̃ are calibrated features\n",
      "  • B2/R7: Δmargin=+0.089 [+] | 0.31 AND + 0.30 OR + 0.38 k-of-n\n",
      "    F2, F12  →  [F2] β( [L−R](x̃) = (0.27·PercentInstallTrades + 0.26·NetFractionRevolvingBurden + 0.15·NumBank2NatlTradesWHighUtilization + 0.13·NumRevolvingTradesWBalance) − (0.39·NumTrades90Ever2DerogPubRec + 0.35·MaxDelqEver + 0.28·AverageMInFile + 0.27·ExternalRiskEstimate) > 0.02; k=0.96, ν=0.69 )  where x̃ are calibrated features | [F12] β( [L−R](x̃) = (0.26·NetFractionRevolvingBurden + 0.23·NumRevolvingTradesWBalance + 0.18·PercentInstallTrades + 0.14·NumTrades90Ever2DerogPubRec) − (0.30·MSinceMostRecentDelq + 0.28·ExternalRiskEstimate + 0.28·NumSatisfactoryTrades + 0.27·MSinceMostRecentTradeOpen) > 0.14; k=0.94, ν=0.69 )  where x̃ are calibrated features\n",
      "  • B2/R5: Δmargin=+0.077 [+] | 0.28 AND + 0.41 OR + 0.31 k-of-n\n",
      "    F13, F1  →  [F13] β( [L−R](x̃) = (0.45·NumTotalTrades + 0.34·PercentInstallTrades + 0.33·NumBank2NatlTradesWHighUtilization + 0.04·MSinceMostRecentDelq) − (0.88·AverageMInFile + 0.45·NumInqLast6Mexcl7days + 0.30·PercentTradesNeverDelq + 0.29·PercentTradesWBalance) > -0.01; k=1.04, ν=0.71 )  where x̃ are calibrated features | [F1] β( [L−R](x̃) = (0.48·MSinceOldestTradeOpen + 0.42·MSinceMostRecentDelq + 0.36·ExternalRiskEstimate + 0.36·PercentTradesWBalance) − (0.20·NetFractionInstallBurden + 0.17·PercentInstallTrades + 0.10·MaxDelq2PublicRecLast12M + 0.09·PercentTradesNeverDelq) > -0.15; k=1.04, ν=0.67 )  where x̃ are calibrated features\n",
      "  ... and 11 more active rules.\n",
      "\n",
      "With pruning:\n",
      "MODEL: SOFTMAX rules | TASK: CLASSIFICATION\n",
      "SAMPLE PREDICTION: Bad\n",
      "  - Confidence: 0.841\n",
      "  - Ground Truth: Bad\n",
      "  - Pruning: |act| >= 0.1476 (forward uses pruned activations)\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B2/R2: Δmargin=-0.264 [-] | 0.39 AND + 0.30 OR + 0.31 k-of-n\n",
      "    F1, F10  →  [F1] β( [L−R](x̃) = (0.48·MSinceOldestTradeOpen + 0.42·MSinceMostRecentDelq + 0.36·ExternalRiskEstimate + 0.36·PercentTradesWBalance) − (0.20·NetFractionInstallBurden + 0.17·PercentInstallTrades + 0.10·MaxDelq2PublicRecLast12M + 0.09·PercentTradesNeverDelq) > -0.15; k=1.04, ν=0.67 )  where x̃ are calibrated features | [F10] β( [L−R](x̃) = (0.57·AverageMInFile + 0.40·MSinceMostRecentInqexcl7days + 0.34·MSinceMostRecentDelq + 0.26·PercentTradesNeverDelq) − (0.55·PercentTradesWBalance + 0.36·NumInqLast6M + 0.22·NumTrades90Ever2DerogPubRec + 0.21·PercentInstallTrades) > -0.17; k=1.03, ν=0.74 )  where x̃ are calibrated features\n",
      "  • B1/R12: Δmargin=+0.117 [+] | 0.32 AND + 0.31 OR + 0.37 k-of-n\n",
      "    F8, F5  →  [F8] β( [L−R](x̃) = (0.31·NumRevolvingTradesWBalance + 0.25·NumTrades90Ever2DerogPubRec + 0.25·NumBank2NatlTradesWHighUtilization + 0.23·PercentInstallTrades) − (0.66·AverageMInFile + 0.35·NumInqLast6M + 0.31·NumTrades60Ever2DerogPubRec + 0.23·NumSatisfactoryTrades) > -0.02; k=1.01, ν=0.70 )  where x̃ are calibrated features | [F5] β( [L−R](x̃) = (0.57·MSinceOldestTradeOpen + 0.52·NumSatisfactoryTrades + 0.44·AverageMInFile + 0.31·NetFractionInstallBurden) − (0.55·PercentTradesWBalance + 0.26·NumBank2NatlTradesWHighUtilization + 0.26·NetFractionRevolvingBurden + 0.26·PercentInstallTrades) > -0.10; k=1.17, ν=0.73 )  where x̃ are calibrated features\n",
      "  • B1/R7: Δmargin=+0.114 [+] | 0.37 AND + 0.31 OR + 0.32 k-of-n\n",
      "    F19, F29  →  [F19] β( [L−R](x̃) = (0.33·AverageMInFile + 0.25·NetFractionInstallBurden + 0.25·ExternalRiskEstimate + 0.24·NumBank2NatlTradesWHighUtilization) − (0.65·PercentTradesNeverDelq + 0.54·NumTradesOpeninLast12M + 0.46·MSinceOldestTradeOpen + 0.37·MaxDelq2PublicRecLast12M) > -0.04; k=1.01, ν=0.72 )  where x̃ are calibrated features | [F29] β( [L−R](x̃) = (0.47·NumRevolvingTradesWBalance + 0.40·NumBank2NatlTradesWHighUtilization + 0.28·NumInqLast6M + 0.27·NumTrades90Ever2DerogPubRec) − (0.46·ExternalRiskEstimate + 0.44·NumSatisfactoryTrades + 0.30·MSinceMostRecentInqexcl7days + 0.30·PercentTradesNeverDelq) > -0.09; k=0.99, ν=0.69 )  where x̃ are calibrated features\n",
      "  • B1/R16: Δmargin=+0.100 [+] | 0.31 AND + 0.34 OR + 0.35 k-of-n\n",
      "    F24, F29  →  [F24] β( [L−R](x̃) = (0.50·NumInqLast6M + 0.49·NumRevolvingTradesWBalance + 0.35·PercentTradesWBalance + 0.26·NumSatisfactoryTrades) − (0.62·NumTotalTrades + 0.34·AverageMInFile + 0.31·NumInstallTradesWBalance + 0.27·ExternalRiskEstimate) > 0.04; k=1.07, ν=0.73 )  where x̃ are calibrated features | [F29] β( [L−R](x̃) = (0.47·NumRevolvingTradesWBalance + 0.40·NumBank2NatlTradesWHighUtilization + 0.28·NumInqLast6M + 0.27·NumTrades90Ever2DerogPubRec) − (0.46·ExternalRiskEstimate + 0.44·NumSatisfactoryTrades + 0.30·MSinceMostRecentInqexcl7days + 0.30·PercentTradesNeverDelq) > -0.09; k=0.99, ν=0.69 )  where x̃ are calibrated features\n",
      "  • B2/R8: Δmargin=+0.091 [+] | 0.31 AND + 0.30 OR + 0.39 k-of-n\n",
      "    F11, F14  →  [F11] β( [L−R](x̃) = (0.56·AverageMInFile + 0.47·NetFractionInstallBurden + 0.46·MaxDelqEver + 0.32·NumSatisfactoryTrades) − (0.59·MSinceMostRecentInqexcl7days + 0.52·NumInqLast6Mexcl7days + 0.46·MSinceMostRecentDelq + 0.39·NumInqLast6M) > 0.07; k=1.19, ν=0.80 )  where x̃ are calibrated features | [F14] β( [L−R](x̃) = (0.33·NetFractionRevolvingBurden + 0.26·NetFractionInstallBurden + 0.17·NumTotalTrades + 0.12·MSinceMostRecentTradeOpen) − (0.40·ExternalRiskEstimate + 0.31·NumSatisfactoryTrades + 0.28·PercentTradesNeverDelq + 0.17·NumRevolvingTradesWBalance) > 0.10; k=0.98, ν=0.68 )  where x̃ are calibrated features\n",
      "  ... and 7 more active rules.\n",
      "\n",
      "NousNet explanation fidelity metrics: {'base_margin': 1.6744287014007568, 'sufficiency_margin': 1.9521119594573975, 'comprehensiveness_margin': 1.228771686553955, 'kept_size': 11.0}\n",
      "\n",
      "Prototype report (top by top1_count):\n",
      " proto primary_class  class_entropy  mean_activation  top1_count\n",
      "     6           Bad       0.478466         0.286715        3502\n",
      "     2          Good       0.403603         0.294361        2664\n",
      "     5          Good       0.393646         0.287987         302\n",
      "     3           Bad       0.569993         0.245349         215\n",
      "     4           Bad       0.675750         0.256965          10\n",
      "\n",
      "Describe best prototype:\n",
      "Prototype #6\n",
      "  Primary class: Bad | probs=[0.1850000023841858, 0.8149999976158142] | entropy=0.478\n",
      "  Top rules:\n",
      "    • R4: weight=-0.180 | 0.34 AND + 0.31 OR + 0.35 k-of-n\n",
      "        [F7] β( [L−R](x̃) = (0.50·PercentTradesWBalance + 0.40·MSinceMostRecentInqexcl7days + 0.34·ExternalRiskEstimate + 0.23·NumSatisfactoryTrades) − (0.41·NumTotalTrades + 0.38·NetFractionRevolvingBurden + 0.36·NumBank2NatlTradesWHighUtilization + 0.33·NumInqLast6M) > 0.03; k=1.17, ν=0.76 )  where x̃ are calibrated features | [F8] β( [L−R](x̃) = (0.31·NumRevolvingTradesWBalance + 0.25·NumTrades90Ever2DerogPubRec + 0.25·NumBank2NatlTradesWHighUtilization + 0.23·PercentInstallTrades) − (0.66·AverageMInFile + 0.35·NumInqLast6M + 0.31·NumTrades60Ever2DerogPubRec + 0.23·NumSatisfactoryTrades) > -0.02; k=1.01, ν=0.70 )  where x̃ are calibrated features\n",
      "    • R3: weight=+0.139 | 0.31 AND + 0.34 OR + 0.35 k-of-n\n",
      "        [F24] β( [L−R](x̃) = (0.50·NumInqLast6M + 0.49·NumRevolvingTradesWBalance + 0.35·PercentTradesWBalance + 0.26·NumSatisfactoryTrades) − (0.62·NumTotalTrades + 0.34·AverageMInFile + 0.31·NumInstallTradesWBalance + 0.27·ExternalRiskEstimate) > 0.04; k=1.07, ν=0.73 )  where x̃ are calibrated features | [F25] β( [L−R](x̃) = (0.58·MSinceMostRecentDelq + 0.41·MaxDelq2PublicRecLast12M + 0.34·MSinceOldestTradeOpen + 0.25·PercentTradesWBalance) − (0.63·MSinceMostRecentInqexcl7days + 0.56·NetFractionRevolvingBurden + 0.43·PercentTradesNeverDelq + 0.42·NumInstallTradesWBalance) > 0.01; k=1.07, ν=0.75 )  where x̃ are calibrated features\n",
      "    • R7: weight=+0.134 | 0.31 AND + 0.30 OR + 0.38 k-of-n\n",
      "        [F5] β( [L−R](x̃) = (0.57·MSinceOldestTradeOpen + 0.52·NumSatisfactoryTrades + 0.44·AverageMInFile + 0.31·NetFractionInstallBurden) − (0.55·PercentTradesWBalance + 0.26·NumBank2NatlTradesWHighUtilization + 0.26·NetFractionRevolvingBurden + 0.26·PercentInstallTrades) > -0.10; k=1.17, ν=0.73 )  where x̃ are calibrated features | [F8] β( [L−R](x̃) = (0.31·NumRevolvingTradesWBalance + 0.25·NumTrades90Ever2DerogPubRec + 0.25·NumBank2NatlTradesWHighUtilization + 0.23·PercentInstallTrades) − (0.66·AverageMInFile + 0.35·NumInqLast6M + 0.31·NumTrades60Ever2DerogPubRec + 0.23·NumSatisfactoryTrades) > -0.02; k=1.01, ν=0.70 )  where x̃ are calibrated features\n",
      "    • R2: weight=-0.099 | 0.39 AND + 0.30 OR + 0.31 k-of-n\n",
      "        [F2] β( [L−R](x̃) = (0.27·PercentInstallTrades + 0.26·NetFractionRevolvingBurden + 0.15·NumBank2NatlTradesWHighUtilization + 0.13·NumRevolvingTradesWBalance) − (0.39·NumTrades90Ever2DerogPubRec + 0.35·MaxDelqEver + 0.28·AverageMInFile + 0.27·ExternalRiskEstimate) > 0.02; k=0.96, ν=0.69 )  where x̃ are calibrated features | [F15] β( [L−R](x̃) = (0.60·MSinceOldestTradeOpen + 0.34·NumTradesOpeninLast12M + 0.30·ExternalRiskEstimate + 0.22·NumInstallTradesWBalance) − (0.95·NumBank2NatlTradesWHighUtilization + 0.59·NumSatisfactoryTrades + 0.43·PercentInstallTrades + 0.37·NumTrades60Ever2DerogPubRec) > -0.18; k=1.09, ν=0.74 )  where x̃ are calibrated features\n",
      "    • R1: weight=+0.088 | 0.30 AND + 0.40 OR + 0.30 k-of-n\n",
      "        [F7] β( [L−R](x̃) = (0.50·PercentTradesWBalance + 0.40·MSinceMostRecentInqexcl7days + 0.34·ExternalRiskEstimate + 0.23·NumSatisfactoryTrades) − (0.41·NumTotalTrades + 0.38·NetFractionRevolvingBurden + 0.36·NumBank2NatlTradesWHighUtilization + 0.33·NumInqLast6M) > 0.03; k=1.17, ν=0.76 )  where x̃ are calibrated features | [F8] β( [L−R](x̃) = (0.31·NumRevolvingTradesWBalance + 0.25·NumTrades90Ever2DerogPubRec + 0.25·NumBank2NatlTradesWHighUtilization + 0.23·PercentInstallTrades) − (0.66·AverageMInFile + 0.35·NumInqLast6M + 0.31·NumTrades60Ever2DerogPubRec + 0.23·NumSatisfactoryTrades) > -0.02; k=1.01, ν=0.70 )  where x̃ are calibrated features\n",
      "    • R8: weight=-0.074 | 0.31 AND + 0.30 OR + 0.39 k-of-n\n",
      "        [F9] β( [L−R](x̃) = (0.71·MSinceMostRecentDelq + 0.39·MaxDelq2PublicRecLast12M + 0.39·ExternalRiskEstimate + 0.18·AverageMInFile) − (0.39·PercentTradesWBalance + 0.38·NumInqLast6Mexcl7days + 0.29·NetFractionRevolvingBurden + 0.28·MaxDelqEver) > 0.09; k=1.11, ν=0.77 )  where x̃ are calibrated features | [F18] β( [L−R](x̃) = (0.49·AverageMInFile + 0.26·NumTrades90Ever2DerogPubRec + 0.13·NetFractionRevolvingBurden + 0.11·NumTotalTrades) − (0.67·NumSatisfactoryTrades + 0.42·MSinceOldestTradeOpen + 0.40·PercentTradesWBalance + 0.35·PercentInstallTrades) > -0.25; k=1.00, ν=0.71 )  where x̃ are calibrated features\n",
      "    • R5: weight=-0.069 | 0.28 AND + 0.41 OR + 0.31 k-of-n\n",
      "        [F3] β( [L−R](x̃) = (0.47·NumTotalTrades + 0.44·AverageMInFile + 0.43·NumInstallTradesWBalance + 0.40·NumInqLast6Mexcl7days) − (0.52·NetFractionRevolvingBurden + 0.49·NumSatisfactoryTrades + 0.29·NumBank2NatlTradesWHighUtilization + 0.23·NumRevolvingTradesWBalance) > -0.01; k=1.25, ν=0.73 )  where x̃ are calibrated features | [F12] β( [L−R](x̃) = (0.26·NetFractionRevolvingBurden + 0.23·NumRevolvingTradesWBalance + 0.18·PercentInstallTrades + 0.14·NumTrades90Ever2DerogPubRec) − (0.30·MSinceMostRecentDelq + 0.28·ExternalRiskEstimate + 0.28·NumSatisfactoryTrades + 0.27·MSinceMostRecentTradeOpen) > 0.14; k=0.94, ν=0.69 )  where x̃ are calibrated features\n",
      "    • R6: weight=+0.016 | 0.37 AND + 0.31 OR + 0.32 k-of-n\n",
      "        [F14] β( [L−R](x̃) = (0.33·NetFractionRevolvingBurden + 0.26·NetFractionInstallBurden + 0.17·NumTotalTrades + 0.12·MSinceMostRecentTradeOpen) − (0.40·ExternalRiskEstimate + 0.31·NumSatisfactoryTrades + 0.28·PercentTradesNeverDelq + 0.17·NumRevolvingTradesWBalance) > 0.10; k=0.98, ν=0.68 )  where x̃ are calibrated features | [F19] β( [L−R](x̃) = (0.33·AverageMInFile + 0.25·NetFractionInstallBurden + 0.25·ExternalRiskEstimate + 0.24·NumBank2NatlTradesWHighUtilization) − (0.65·PercentTradesNeverDelq + 0.54·NumTradesOpeninLast12M + 0.46·MSinceOldestTradeOpen + 0.37·MaxDelq2PublicRecLast12M) > -0.04; k=1.01, ν=0.72 )  where x̃ are calibrated features\n",
      "\n",
      "Prototype contributions for the sample:\n",
      " proto  distance  activation       w_c  contribution primary_class  entropy\n",
      "     6  0.524178    0.504670  1.996593      1.007622           Bad 0.478466\n",
      "     4  0.655064    0.425451  0.644615      0.274252           Bad 0.675750\n",
      "     1  0.729295    0.386181 -0.586562     -0.226519           Bad 0.629359\n",
      "     2  1.737970    0.103583 -2.026086     -0.209869          Good 0.403603\n",
      "     3  0.563687    0.479317 -0.255676     -0.122550           Bad 0.569993\n",
      "\n",
      "— XGBoost SHAP (global) —\n",
      "Top-10 global features by mean |SHAP|:\n",
      "  ExternalRiskEstimate: 0.5482\n",
      "  MSinceMostRecentInqexcl7days: 0.4613\n",
      "  NetFractionRevolvingBurden: 0.3732\n",
      "  AverageMInFile: 0.3565\n",
      "  PercentTradesNeverDelq: 0.3489\n",
      "  PercentInstallTrades: 0.3054\n",
      "  NumSatisfactoryTrades: 0.2837\n",
      "  MSinceOldestTradeOpen: 0.2490\n",
      "  NumRevolvingTradesWBalance: 0.2090\n",
      "  NumTotalTrades: 0.1776\n",
      "\n",
      "— EBM (global & local) —\n",
      "EBM global explain failed: 'overall'\n",
      "Top-10 local EBM contributions:\n",
      "  feature_0007: +0.4126\n",
      "  feature_0004: +0.3194\n",
      "  feature_0000: +0.3088\n",
      "  feature_0014: +0.1985\n",
      "  feature_0011: +0.1271\n",
      "  feature_0021: -0.1242\n",
      "  feature_0002: -0.1163\n",
      "  feature_0010: +0.1085\n",
      "  feature_0009: +0.0858\n",
      "  feature_0017: +0.0726\n",
      "\n",
      "— MLP permutation importance (global) —\n",
      "Top-10 features by permutation importance (ΔAccuracy):\n",
      "  ExternalRiskEstimate: 0.0538\n",
      "  PercentTradesNeverDelq: 0.0215\n",
      "  MaxDelqEver: 0.0102\n",
      "  NetFractionRevolvingBurden: 0.0078\n",
      "  NumSatisfactoryTrades: 0.0072\n",
      "  NumRevolvingTradesWBalance: 0.0048\n",
      "  MSinceOldestTradeOpen: 0.0042\n",
      "  MaxDelq2PublicRecLast12M: 0.0036\n",
      "  PercentInstallTrades: 0.0030\n",
      "  NumInqLast6Mexcl7days: 0.0030\n",
      "\n",
      "====================================================================================================\n",
      "Interpretability Showcase — Adult (classification)\n",
      "====================================================================================================\n",
      "Epoch [1/1000] train=0.3597 val=0.3177 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.3166 val=0.3069 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3125 val=0.3045 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.3104 val=0.3031 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.3040 val=0.3010 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3010 val=0.3075 l0=0.0000\n",
      "Epoch [20/1000] train=0.2938 val=0.3075 l0=0.0000\n",
      "Epoch [30/1000] train=0.2858 val=0.3169 l0=0.0000\n",
      "Epoch [40/1000] train=0.2772 val=0.3141 l0=0.0000\n",
      "Epoch [50/1000] train=0.2692 val=0.3253 l0=0.0000\n",
      "Epoch [60/1000] train=0.2611 val=0.3454 l0=0.0000\n",
      "Epoch [70/1000] train=0.2548 val=0.3600 l0=0.0000\n",
      "Epoch [80/1000] train=0.2486 val=0.3819 l0=0.0000\n",
      "Epoch [90/1000] train=0.2436 val=0.3867 l0=0.0000\n",
      "Epoch [100/1000] train=0.2394 val=0.4201 l0=0.0000\n",
      "Epoch [110/1000] train=0.2350 val=0.4412 l0=0.0000\n",
      "Epoch [120/1000] train=0.2319 val=0.4468 l0=0.0000\n",
      "Epoch [130/1000] train=0.2278 val=0.4548 l0=0.0000\n",
      "Epoch [140/1000] train=0.2251 val=0.4802 l0=0.0000\n",
      "Epoch [150/1000] train=0.2228 val=0.4796 l0=0.0000\n",
      "Epoch [160/1000] train=0.2203 val=0.5085 l0=0.0000\n",
      "Epoch [170/1000] train=0.2171 val=0.5359 l0=0.0000\n",
      "Epoch [180/1000] train=0.2161 val=0.5565 l0=0.0000\n",
      "Epoch [190/1000] train=0.2139 val=0.5578 l0=0.0000\n",
      "Epoch [200/1000] train=0.2118 val=0.5902 l0=0.0000\n",
      "Early stopping at epoch 208 (best val=0.3010)\n",
      "Restored best model (val=0.3010)\n",
      "NousNet (Fixed) Accuracy: 0.8580 | AUC: 0.9125\n",
      "Epoch [1/1000] train=0.4370 val=0.3705 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.3683 val=0.3271 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3435 val=0.3183 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.3303 val=0.3160 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.3284 val=0.3113 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.3283 val=0.3090 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.3230 val=0.3082 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.3222 val=0.3076 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.3236 val=0.3070 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.3268 val=0.3283 l0=0.0000\n",
      "Epoch [21/1000] train=0.3208 val=0.3050 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.3158 val=0.3040 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.3141 val=0.3052 l0=0.0000\n",
      "Epoch [38/1000] train=0.3120 val=0.3021 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.3117 val=0.3027 l0=0.0000\n",
      "Epoch [43/1000] train=0.3115 val=0.3019 l0=0.0000 (*)\n",
      "Epoch [44/1000] train=0.3130 val=0.3011 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.3090 val=0.3055 l0=0.0000\n",
      "Epoch [60/1000] train=0.3056 val=0.2988 l0=0.0000 (*)\n",
      "Epoch [64/1000] train=0.3057 val=0.2987 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.3046 val=0.2994 l0=0.0000\n",
      "Epoch [80/1000] train=0.3028 val=0.3025 l0=0.0000\n",
      "Epoch [90/1000] train=0.3015 val=0.3050 l0=0.0000\n",
      "Epoch [99/1000] train=0.3010 val=0.2986 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.3007 val=0.3020 l0=0.0000\n",
      "Epoch [101/1000] train=0.3002 val=0.2982 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.2990 val=0.3007 l0=0.0000\n",
      "Epoch [120/1000] train=0.2963 val=0.3028 l0=0.0000\n",
      "Epoch [130/1000] train=0.2948 val=0.3004 l0=0.0000\n",
      "Epoch [140/1000] train=0.2925 val=0.3146 l0=0.0000\n",
      "Epoch [150/1000] train=0.2913 val=0.3071 l0=0.0000\n",
      "Epoch [160/1000] train=0.2876 val=0.3250 l0=0.0000\n",
      "Epoch [170/1000] train=0.2849 val=0.3153 l0=0.0000\n",
      "Epoch [180/1000] train=0.2820 val=0.3146 l0=0.0000\n",
      "Epoch [190/1000] train=0.2792 val=0.3214 l0=0.0000\n",
      "Epoch [200/1000] train=0.2757 val=0.3250 l0=0.0000\n",
      "Epoch [210/1000] train=0.2736 val=0.3274 l0=0.0000\n",
      "Epoch [220/1000] train=0.2697 val=0.3360 l0=0.0000\n",
      "Epoch [230/1000] train=0.2676 val=0.3377 l0=0.0000\n",
      "Epoch [240/1000] train=0.2653 val=0.3442 l0=0.0000\n",
      "Epoch [250/1000] train=0.2619 val=0.3484 l0=0.0000\n",
      "Epoch [260/1000] train=0.2591 val=0.3549 l0=0.0000\n",
      "Epoch [270/1000] train=0.2556 val=0.3603 l0=0.0000\n",
      "Epoch [280/1000] train=0.2538 val=0.3698 l0=0.0000\n",
      "Epoch [290/1000] train=0.2520 val=0.3838 l0=0.0000\n",
      "Epoch [300/1000] train=0.2500 val=0.3826 l0=0.0000\n",
      "Early stopping at epoch 301 (best val=0.2982)\n",
      "Restored best model (val=0.2982)\n",
      "NousNet (Softmax+Proto) Accuracy: 0.8613 | AUC: 0.9144\n",
      "XGBoost Accuracy: 0.8761 | AUC: 0.9298\n",
      "EBM Accuracy: 0.8741 | AUC: 0.9262\n",
      "MLP Accuracy: 0.8573 | AUC: 0.9112\n",
      "KAN Accuracy: 0.8576 | AUC: 0.9142\n",
      "KAN top-10 global features:\n",
      "  capital-gain: 44.1838\n",
      "  capital-loss: 16.6676\n",
      "  education-num: 16.4515\n",
      "  age: 12.7687\n",
      "  hours-per-week: 9.1268\n",
      "  relationship_Wife: 7.2212\n",
      "  fnlwgt: 6.6368\n",
      "  education_Preschool: 6.5906\n",
      "  marital-status_Married-civ-spouse: 6.4133\n",
      "  native-country_Dominican-Republic: 5.6134\n",
      "\n",
      "— NousNet (Softmax+Proto) explanation (held-out sample) —\n",
      "MODEL: SOFTMAX rules | TASK: CLASSIFICATION\n",
      "SAMPLE PREDICTION: <=50K\n",
      "  - Confidence: 0.826\n",
      "  - Ground Truth: <=50K\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B1/R5: Δmargin=-0.161 [-] | 0.27 AND + 0.47 OR + 0.26 k-of-n\n",
      "    F28, F7  →  [F28] β( [L−R](x̃) = (0.85·age + 0.57·education_5th-6th + 0.51·workclass_Self-emp-not-inc + 0.46·hours-per-week) − (0.83·native-country_Taiwan + 0.81·capital-gain + 0.68·native-country_Canada + 0.51·relationship_Other-relative) > -0.09; k=0.91, ν=0.71 )  where x̃ are calibrated features | [F7] β( [L−R](x̃) = (0.53·education-num + 0.37·hours-per-week + 0.30·age + 0.27·education_Some-college) − (0.36·education_5th-6th + 0.33·education_7th-8th + 0.27·occupation_Craft-repair + 0.24·occupation_Sales) > 0.11; k=0.97, ν=0.69 )  where x̃ are calibrated features\n",
      "  • B2/R1: Δmargin=+0.111 [+] | 0.21 AND + 0.56 OR + 0.23 k-of-n\n",
      "    F13, F4  →  [F13] β( [L−R](x̃) = (1.40·education_Doctorate + 1.09·workclass_Private + 1.07·capital-gain + 0.72·education_Bachelors) − (0.84·workclass_Self-emp-not-inc + 0.61·occupation_Farming-fishing + 0.41·native-country_South + 0.40·education_HS-grad) > 0.10; k=1.11, ν=0.79 )  where x̃ are calibrated features | [F4] β( [L−R](x̃) = (1.31·capital-gain + 1.22·native-country_Yugoslavia + 1.17·capital-loss + 1.06·native-country_Germany) − (1.00·education_5th-6th + 0.78·relationship_Not-in-family + 0.75·occupation_Craft-repair + 0.72·education_Doctorate) > -0.06; k=0.77, ν=0.67 )  where x̃ are calibrated features\n",
      "  • B2/R7: Δmargin=+0.102 [+] | 0.24 AND + 0.22 OR + 0.55 k-of-n\n",
      "    F2, F1  →  [F2] β( [L−R](x̃) = (0.88·native-country_Jamaica + 0.85·workclass_Self-emp-not-inc + 0.78·education_5th-6th + 0.70·native-country_South) − (1.29·native-country_Canada + 1.08·capital-gain + 0.98·education-num + 0.82·capital-loss) > -0.12; k=0.85, ν=0.67 )  where x̃ are calibrated features | [F1] β( [L−R](x̃) = (0.61·age + 0.30·race_Asian-Pac-Islander + 0.27·workclass_Self-emp-not-inc + 0.27·marital-status_Married-civ-spouse) − (0.19·marital-status_Never-married + 0.12·sex_Female + 0.10·native-country_Germany + 0.08·workclass_Private) > -0.21; k=0.90, ν=0.67 )  where x̃ are calibrated features\n",
      "  • B2/R4: Δmargin=-0.098 [-] | 0.60 AND + 0.20 OR + 0.20 k-of-n\n",
      "    F2, F15  →  [F2] β( [L−R](x̃) = (0.88·native-country_Jamaica + 0.85·workclass_Self-emp-not-inc + 0.78·education_5th-6th + 0.70·native-country_South) − (1.29·native-country_Canada + 1.08·capital-gain + 0.98·education-num + 0.82·capital-loss) > -0.12; k=0.85, ν=0.67 )  where x̃ are calibrated features | [F15] β( [L−R](x̃) = (0.55·capital-gain + 0.15·relationship_Other-relative) − (0.59·age + 0.38·capital-loss + 0.37·marital-status_Married-civ-spouse + 0.35·native-country_Laos) > 0.10; k=0.99, ν=0.71 )  where x̃ are calibrated features\n",
      "  • B1/R6: Δmargin=-0.092 [-] | 0.40 AND + 0.29 OR + 0.31 k-of-n\n",
      "    F2, F3  →  [F2] β( [L−R](x̃) = (0.88·native-country_Jamaica + 0.85·workclass_Self-emp-not-inc + 0.78·education_5th-6th + 0.70·native-country_South) − (1.29·native-country_Canada + 1.08·capital-gain + 0.98·education-num + 0.82·capital-loss) > -0.12; k=0.85, ν=0.67 )  where x̃ are calibrated features | [F3] β( [L−R](x̃) = (0.25·workclass_Self-emp-not-inc + 0.17·occupation_Transport-moving + 0.16·hours-per-week + 0.12·relationship_Wife) − (0.38·capital-gain + 0.36·education-num + 0.32·relationship_Husband + 0.31·occupation_Priv-house-serv) > 0.17; k=1.06, ν=0.76 )  where x̃ are calibrated features\n",
      "  ... and 11 more active rules.\n",
      "\n",
      "With pruning:\n",
      "MODEL: SOFTMAX rules | TASK: CLASSIFICATION\n",
      "SAMPLE PREDICTION: <=50K\n",
      "  - Confidence: 0.840\n",
      "  - Ground Truth: <=50K\n",
      "  - Pruning: |act| >= 0.2358 (forward uses pruned activations)\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B1/R5: Δmargin=-0.159 [-] | 0.27 AND + 0.47 OR + 0.26 k-of-n\n",
      "    F28, F7  →  [F28] β( [L−R](x̃) = (0.85·age + 0.57·education_5th-6th + 0.51·workclass_Self-emp-not-inc + 0.46·hours-per-week) − (0.83·native-country_Taiwan + 0.81·capital-gain + 0.68·native-country_Canada + 0.51·relationship_Other-relative) > -0.09; k=0.91, ν=0.71 )  where x̃ are calibrated features | [F7] β( [L−R](x̃) = (0.53·education-num + 0.37·hours-per-week + 0.30·age + 0.27·education_Some-college) − (0.36·education_5th-6th + 0.33·education_7th-8th + 0.27·occupation_Craft-repair + 0.24·occupation_Sales) > 0.11; k=0.97, ν=0.69 )  where x̃ are calibrated features\n",
      "  • B2/R7: Δmargin=+0.106 [+] | 0.24 AND + 0.22 OR + 0.55 k-of-n\n",
      "    F2, F1  →  [F2] β( [L−R](x̃) = (0.88·native-country_Jamaica + 0.85·workclass_Self-emp-not-inc + 0.78·education_5th-6th + 0.70·native-country_South) − (1.29·native-country_Canada + 1.08·capital-gain + 0.98·education-num + 0.82·capital-loss) > -0.12; k=0.85, ν=0.67 )  where x̃ are calibrated features | [F1] β( [L−R](x̃) = (0.61·age + 0.30·race_Asian-Pac-Islander + 0.27·workclass_Self-emp-not-inc + 0.27·marital-status_Married-civ-spouse) − (0.19·marital-status_Never-married + 0.12·sex_Female + 0.10·native-country_Germany + 0.08·workclass_Private) > -0.21; k=0.90, ν=0.67 )  where x̃ are calibrated features\n",
      "  • B2/R1: Δmargin=+0.106 [+] | 0.21 AND + 0.56 OR + 0.23 k-of-n\n",
      "    F13, F4  →  [F13] β( [L−R](x̃) = (1.40·education_Doctorate + 1.09·workclass_Private + 1.07·capital-gain + 0.72·education_Bachelors) − (0.84·workclass_Self-emp-not-inc + 0.61·occupation_Farming-fishing + 0.41·native-country_South + 0.40·education_HS-grad) > 0.10; k=1.11, ν=0.79 )  where x̃ are calibrated features | [F4] β( [L−R](x̃) = (1.31·capital-gain + 1.22·native-country_Yugoslavia + 1.17·capital-loss + 1.06·native-country_Germany) − (1.00·education_5th-6th + 0.78·relationship_Not-in-family + 0.75·occupation_Craft-repair + 0.72·education_Doctorate) > -0.06; k=0.77, ν=0.67 )  where x̃ are calibrated features\n",
      "  • B1/R11: Δmargin=+0.081 [+] | 0.08 AND + 0.06 OR + 0.86 k-of-n\n",
      "    F3, F11  →  [F3] β( [L−R](x̃) = (0.25·workclass_Self-emp-not-inc + 0.17·occupation_Transport-moving + 0.16·hours-per-week + 0.12·relationship_Wife) − (0.38·capital-gain + 0.36·education-num + 0.32·relationship_Husband + 0.31·occupation_Priv-house-serv) > 0.17; k=1.06, ν=0.76 )  where x̃ are calibrated features | [F11] β( [L−R](x̃) = (0.27·occupation_Other-service + 0.26·native-country_Haiti + 0.26·workclass_State-gov + 0.26·native-country_Japan) − (0.06·education_Prof-school + 0.05·native-country_France + 0.05·education_Some-college + 0.04·native-country_Iran) > -0.11; k=1.01, ν=0.68 )  where x̃ are calibrated features\n",
      "  • B1/R4: Δmargin=+0.071 [+] | 0.41 AND + 0.28 OR + 0.31 k-of-n\n",
      "    F2, F11  →  [F2] β( [L−R](x̃) = (0.88·native-country_Jamaica + 0.85·workclass_Self-emp-not-inc + 0.78·education_5th-6th + 0.70·native-country_South) − (1.29·native-country_Canada + 1.08·capital-gain + 0.98·education-num + 0.82·capital-loss) > -0.12; k=0.85, ν=0.67 )  where x̃ are calibrated features | [F11] β( [L−R](x̃) = (0.27·occupation_Other-service + 0.26·native-country_Haiti + 0.26·workclass_State-gov + 0.26·native-country_Japan) − (0.06·education_Prof-school + 0.05·native-country_France + 0.05·education_Some-college + 0.04·native-country_Iran) > -0.11; k=1.01, ν=0.68 )  where x̃ are calibrated features\n",
      "  ... and 4 more active rules.\n",
      "\n",
      "NousNet explanation fidelity metrics: {'base_margin': 1.5575230121612549, 'sufficiency_margin': 1.8581607341766357, 'comprehensiveness_margin': 1.2540538311004639, 'kept_size': 7.0}\n",
      "\n",
      "Prototype report (top by top1_count):\n",
      " proto primary_class  class_entropy  mean_activation  top1_count\n",
      "     8         <=50K       0.304212         0.392341       24226\n",
      "     0          >50K       0.160219         0.112254        5954\n",
      "     2          >50K       0.556464         0.111721         808\n",
      "     6         <=50K       0.492855         0.388447         205\n",
      "     3         <=50K       0.600990         0.362829          53\n",
      "\n",
      "Describe best prototype:\n",
      "Prototype #8\n",
      "  Primary class: <=50K | probs=[0.9089999794960022, 0.09099999815225601] | entropy=0.304\n",
      "  Top rules:\n",
      "    • R7: weight=+0.358 | 0.24 AND + 0.22 OR + 0.55 k-of-n\n",
      "        [F5] β( [L−R](x̃) = (0.99·capital-gain + 0.70·education_Bachelors + 0.55·workclass_Private + 0.40·native-country_Canada) − (0.75·workclass_Self-emp-not-inc + 0.54·age + 0.45·native-country_Scotland + 0.42·education_5th-6th) > 0.10; k=1.18, ν=0.80 )  where x̃ are calibrated features | [F6] β( [L−R](x̃) = (2.26·native-country_Canada + 1.01·education_5th-6th + 0.98·native-country_Cuba + 0.93·native-country_Taiwan) − (1.28·age + 0.88·marital-status_Married-civ-spouse + 0.79·hours-per-week + 0.68·capital-loss) > -0.09; k=0.98, ν=0.67 )  where x̃ are calibrated features\n",
      "    • R1: weight=+0.288 | 0.21 AND + 0.56 OR + 0.23 k-of-n\n",
      "        [F1] β( [L−R](x̃) = (0.61·age + 0.30·race_Asian-Pac-Islander + 0.27·workclass_Self-emp-not-inc + 0.27·marital-status_Married-civ-spouse) − (0.19·marital-status_Never-married + 0.12·sex_Female + 0.10·native-country_Germany + 0.08·workclass_Private) > -0.21; k=0.90, ν=0.67 )  where x̃ are calibrated features | [F2] β( [L−R](x̃) = (0.88·native-country_Jamaica + 0.85·workclass_Self-emp-not-inc + 0.78·education_5th-6th + 0.70·native-country_South) − (1.29·native-country_Canada + 1.08·capital-gain + 0.98·education-num + 0.82·capital-loss) > -0.12; k=0.85, ν=0.67 )  where x̃ are calibrated features\n",
      "    • R4: weight=-0.237 | 0.60 AND + 0.20 OR + 0.20 k-of-n\n",
      "        [F4] β( [L−R](x̃) = (1.31·capital-gain + 1.22·native-country_Yugoslavia + 1.17·capital-loss + 1.06·native-country_Germany) − (1.00·education_5th-6th + 0.78·relationship_Not-in-family + 0.75·occupation_Craft-repair + 0.72·education_Doctorate) > -0.06; k=0.77, ν=0.67 )  where x̃ are calibrated features | [F6] β( [L−R](x̃) = (2.26·native-country_Canada + 1.01·education_5th-6th + 0.98·native-country_Cuba + 0.93·native-country_Taiwan) − (1.28·age + 0.88·marital-status_Married-civ-spouse + 0.79·hours-per-week + 0.68·capital-loss) > -0.09; k=0.98, ν=0.67 )  where x̃ are calibrated features\n",
      "    • R5: weight=-0.218 | 0.71 AND + 0.15 OR + 0.14 k-of-n\n",
      "        [F2] β( [L−R](x̃) = (0.88·native-country_Jamaica + 0.85·workclass_Self-emp-not-inc + 0.78·education_5th-6th + 0.70·native-country_South) − (1.29·native-country_Canada + 1.08·capital-gain + 0.98·education-num + 0.82·capital-loss) > -0.12; k=0.85, ν=0.67 )  where x̃ are calibrated features | [F3] β( [L−R](x̃) = (0.25·workclass_Self-emp-not-inc + 0.17·occupation_Transport-moving + 0.16·hours-per-week + 0.12·relationship_Wife) − (0.38·capital-gain + 0.36·education-num + 0.32·relationship_Husband + 0.31·occupation_Priv-house-serv) > 0.17; k=1.06, ν=0.76 )  where x̃ are calibrated features\n",
      "    • R6: weight=-0.210 | 0.26 AND + 0.41 OR + 0.33 k-of-n\n",
      "        [F10] β( [L−R](x̃) = (0.16·workclass_Private + 0.15·relationship_Wife + 0.09·occupation_Prof-specialty + 0.08·education_Some-college) − (0.60·education-num + 0.41·occupation_Exec-managerial + 0.40·capital-gain + 0.28·workclass_State-gov) > 0.01; k=1.05, ν=0.76 )  where x̃ are calibrated features | [F12] β( [L−R](x̃) = (0.73·workclass_Private + 0.70·education_Bachelors + 0.62·education-num + 0.49·fnlwgt) − (1.27·education_5th-6th + 0.70·capital-gain + 0.59·workclass_Self-emp-not-inc + 0.55·native-country_Yugoslavia) > -0.11; k=0.90, ν=0.69 )  where x̃ are calibrated features\n",
      "    • R2: weight=+0.156 | 0.31 AND + 0.34 OR + 0.35 k-of-n\n",
      "        [F1] β( [L−R](x̃) = (0.61·age + 0.30·race_Asian-Pac-Islander + 0.27·workclass_Self-emp-not-inc + 0.27·marital-status_Married-civ-spouse) − (0.19·marital-status_Never-married + 0.12·sex_Female + 0.10·native-country_Germany + 0.08·workclass_Private) > -0.21; k=0.90, ν=0.67 )  where x̃ are calibrated features | [F20] β( [L−R](x̃) = (1.42·native-country_Cambodia + 1.26·native-country_Scotland + 0.71·education_Doctorate + 0.65·native-country_Dominican-Republic) − (1.56·age + 1.04·native-country_Yugoslavia + 0.82·marital-status_Married-civ-spouse + 0.82·race_Asian-Pac-Islander) > 0.03; k=0.94, ν=0.65 )  where x̃ are calibrated features\n",
      "    • R3: weight=-0.076 | 0.42 AND + 0.24 OR + 0.34 k-of-n\n",
      "        [F3] β( [L−R](x̃) = (0.25·workclass_Self-emp-not-inc + 0.17·occupation_Transport-moving + 0.16·hours-per-week + 0.12·relationship_Wife) − (0.38·capital-gain + 0.36·education-num + 0.32·relationship_Husband + 0.31·occupation_Priv-house-serv) > 0.17; k=1.06, ν=0.76 )  where x̃ are calibrated features | [F6] β( [L−R](x̃) = (2.26·native-country_Canada + 1.01·education_5th-6th + 0.98·native-country_Cuba + 0.93·native-country_Taiwan) − (1.28·age + 0.88·marital-status_Married-civ-spouse + 0.79·hours-per-week + 0.68·capital-loss) > -0.09; k=0.98, ν=0.67 )  where x̃ are calibrated features\n",
      "    • R8: weight=-0.007 | 0.17 AND + 0.64 OR + 0.19 k-of-n\n",
      "        [F5] β( [L−R](x̃) = (0.99·capital-gain + 0.70·education_Bachelors + 0.55·workclass_Private + 0.40·native-country_Canada) − (0.75·workclass_Self-emp-not-inc + 0.54·age + 0.45·native-country_Scotland + 0.42·education_5th-6th) > 0.10; k=1.18, ν=0.80 )  where x̃ are calibrated features | [F7] β( [L−R](x̃) = (0.53·education-num + 0.37·hours-per-week + 0.30·age + 0.27·education_Some-college) − (0.36·education_5th-6th + 0.33·education_7th-8th + 0.27·occupation_Craft-repair + 0.24·occupation_Sales) > 0.11; k=0.97, ν=0.69 )  where x̃ are calibrated features\n",
      "\n",
      "Prototype contributions for the sample:\n",
      " proto  distance  activation       w_c  contribution primary_class  entropy\n",
      "     7  0.820169    0.235198  1.370183      0.322264         <=50K 0.138801\n",
      "     8  0.819493    0.235478  0.923391      0.217438         <=50K 0.304212\n",
      "     3  0.863204    0.217997  0.646577      0.140952         <=50K 0.600990\n",
      "     5  1.800586    0.041692 -1.462930     -0.060992          >50K 0.239280\n",
      "     9  1.599805    0.059420  1.002012      0.059539         <=50K 0.651722\n",
      "\n",
      "— XGBoost SHAP (global) —\n",
      "Top-10 global features by mean |SHAP|:\n",
      "  marital-status_Married-civ-spouse: 1.1265\n",
      "  age: 0.7965\n",
      "  capital-gain: 0.5941\n",
      "  education-num: 0.5352\n",
      "  hours-per-week: 0.3870\n",
      "  fnlwgt: 0.2476\n",
      "  capital-loss: 0.1864\n",
      "  sex_Female: 0.1643\n",
      "  occupation_Exec-managerial: 0.1348\n",
      "  marital-status_Never-married: 0.1211\n",
      "\n",
      "— EBM (global & local) —\n",
      "EBM global explain failed: 'overall'\n",
      "Top-10 local EBM contributions:\n",
      "  feature_0056: +1.1895\n",
      "  feature_0044: -0.8409\n",
      "  feature_0000: +0.7521\n",
      "  feature_0032: +0.4219\n",
      "  feature_0003: -0.2592\n",
      "  feature_0034: +0.2499\n",
      "  feature_0025: -0.2272\n",
      "  feature_0059: -0.1907\n",
      "  feature_0063: -0.1887\n",
      "  feature_0062: -0.1855\n",
      "\n",
      "— MLP permutation importance (global) —\n",
      "Top-10 features by permutation importance (ΔAccuracy):\n",
      "  capital-gain: 0.0340\n",
      "  education-num: 0.0203\n",
      "  age: 0.0157\n",
      "  marital-status_Married-civ-spouse: 0.0077\n",
      "  occupation_Exec-managerial: 0.0061\n",
      "  hours-per-week: 0.0060\n",
      "  marital-status_Never-married: 0.0049\n",
      "  capital-loss: 0.0038\n",
      "  occupation_Farming-fishing: 0.0033\n",
      "  relationship_Wife: 0.0032\n",
      "\n",
      "====================================================================================================\n",
      "Interpretability Showcase — Breast Cancer (classification)\n",
      "====================================================================================================\n",
      "Epoch [1/1000] train=0.6846 val=0.5979 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.5362 val=0.4789 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.4030 val=0.3566 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.3075 val=0.2736 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.2497 val=0.2254 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.2091 val=0.1931 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1725 val=0.1656 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.1471 val=0.1516 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.1276 val=0.1413 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1150 val=0.1328 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.1063 val=0.1288 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.1022 val=0.1271 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.0935 val=0.1135 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.0733 val=0.1057 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.0582 val=0.1114 l0=0.0000\n",
      "Epoch [23/1000] train=0.0566 val=0.1033 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.0475 val=0.1022 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.0456 val=0.1010 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.0408 val=0.1002 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.0408 val=0.1036 l0=0.0000\n",
      "Epoch [35/1000] train=0.0350 val=0.0993 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.0334 val=0.0992 l0=0.0000 (*)\n",
      "Epoch [37/1000] train=0.0390 val=0.0913 l0=0.0000 (*)\n",
      "Epoch [38/1000] train=0.0321 val=0.0894 l0=0.0000 (*)\n",
      "Epoch [39/1000] train=0.0365 val=0.0883 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0327 val=0.0806 l0=0.0000 (*)\n",
      "Epoch [41/1000] train=0.0334 val=0.0775 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.0357 val=0.0752 l0=0.0000 (*)\n",
      "Epoch [48/1000] train=0.0249 val=0.0709 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0300 val=0.0706 l0=0.0000 (*)\n",
      "Epoch [53/1000] train=0.0227 val=0.0697 l0=0.0000 (*)\n",
      "Epoch [56/1000] train=0.0234 val=0.0646 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.0222 val=0.0696 l0=0.0000\n",
      "Epoch [70/1000] train=0.0189 val=0.0689 l0=0.0000\n",
      "Epoch [80/1000] train=0.0174 val=0.0659 l0=0.0000\n",
      "Epoch [85/1000] train=0.0163 val=0.0638 l0=0.0000 (*)\n",
      "Epoch [86/1000] train=0.0162 val=0.0633 l0=0.0000 (*)\n",
      "Epoch [87/1000] train=0.0159 val=0.0627 l0=0.0000 (*)\n",
      "Epoch [88/1000] train=0.0156 val=0.0621 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.0157 val=0.0621 l0=0.0000\n",
      "Epoch [91/1000] train=0.0158 val=0.0620 l0=0.0000 (*)\n",
      "Epoch [93/1000] train=0.0173 val=0.0554 l0=0.0000 (*)\n",
      "Epoch [94/1000] train=0.0162 val=0.0476 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.0112 val=0.0632 l0=0.0000\n",
      "Epoch [110/1000] train=0.0061 val=0.0659 l0=0.0000\n",
      "Epoch [120/1000] train=0.0038 val=0.0552 l0=0.0000\n",
      "Epoch [130/1000] train=0.0020 val=0.0765 l0=0.0000\n",
      "Epoch [140/1000] train=0.0015 val=0.0855 l0=0.0000\n",
      "Epoch [150/1000] train=0.0012 val=0.0892 l0=0.0000\n",
      "Epoch [160/1000] train=0.0011 val=0.0958 l0=0.0000\n",
      "Epoch [170/1000] train=0.0009 val=0.1017 l0=0.0000\n",
      "Epoch [180/1000] train=0.0008 val=0.1061 l0=0.0000\n",
      "Epoch [190/1000] train=0.0007 val=0.1109 l0=0.0000\n",
      "Epoch [200/1000] train=0.0006 val=0.1143 l0=0.0000\n",
      "Epoch [210/1000] train=0.0005 val=0.1182 l0=0.0000\n",
      "Epoch [220/1000] train=0.0005 val=0.1220 l0=0.0000\n",
      "Epoch [230/1000] train=0.0004 val=0.1255 l0=0.0000\n",
      "Epoch [240/1000] train=0.0004 val=0.1288 l0=0.0000\n",
      "Epoch [250/1000] train=0.0004 val=0.1317 l0=0.0000\n",
      "Epoch [260/1000] train=0.0003 val=0.1344 l0=0.0000\n",
      "Epoch [270/1000] train=0.0003 val=0.1372 l0=0.0000\n",
      "Epoch [280/1000] train=0.0003 val=0.1402 l0=0.0000\n",
      "Epoch [290/1000] train=0.0003 val=0.1417 l0=0.0000\n",
      "Early stopping at epoch 294 (best val=0.0476)\n",
      "Restored best model (val=0.0476)\n",
      "NousNet (Fixed) Accuracy: 0.9474 | AUC: 0.9778\n",
      "Epoch [1/1000] train=0.8074 val=0.7677 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.7402 val=0.7118 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.6916 val=0.6646 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.6479 val=0.6444 l0=0.0000 (*)\n",
      "Epoch [5/1000] train=0.6408 val=0.6266 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.6327 val=0.6205 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.6029 val=0.5844 l0=0.0000 (*)\n",
      "Epoch [8/1000] train=0.5642 val=0.5422 l0=0.0000 (*)\n",
      "Epoch [9/1000] train=0.5246 val=0.4999 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.5022 val=0.4823 l0=0.0000 (*)\n",
      "Epoch [11/1000] train=0.4773 val=0.4577 l0=0.0000 (*)\n",
      "Epoch [12/1000] train=0.4296 val=0.4049 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.3824 val=0.3640 l0=0.0000 (*)\n",
      "Epoch [14/1000] train=0.3415 val=0.3280 l0=0.0000 (*)\n",
      "Epoch [15/1000] train=0.3024 val=0.3051 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.2774 val=0.2840 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.2497 val=0.2558 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.2279 val=0.2370 l0=0.0000 (*)\n",
      "Epoch [19/1000] train=0.2005 val=0.2202 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1864 val=0.2002 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.1729 val=0.1932 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.1669 val=0.1731 l0=0.0000 (*)\n",
      "Epoch [23/1000] train=0.1538 val=0.1695 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.1461 val=0.1654 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1317 val=0.1527 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.1230 val=0.1385 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.1097 val=0.1325 l0=0.0000 (*)\n",
      "Epoch [29/1000] train=0.1097 val=0.1323 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1234 val=0.1309 l0=0.0000 (*)\n",
      "Epoch [31/1000] train=0.1020 val=0.1227 l0=0.0000 (*)\n",
      "Epoch [34/1000] train=0.0966 val=0.1137 l0=0.0000 (*)\n",
      "Epoch [36/1000] train=0.0812 val=0.1100 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.0850 val=0.1202 l0=0.0000\n",
      "Epoch [43/1000] train=0.0656 val=0.1033 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.0679 val=0.1021 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.0581 val=0.1086 l0=0.0000\n",
      "Epoch [60/1000] train=0.0534 val=0.1298 l0=0.0000\n",
      "Epoch [62/1000] train=0.0597 val=0.0966 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.0584 val=0.0929 l0=0.0000 (*)\n",
      "Epoch [79/1000] train=0.0540 val=0.0914 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.0483 val=0.1329 l0=0.0000\n",
      "Epoch [90/1000] train=0.0583 val=0.1262 l0=0.0000\n",
      "Epoch [100/1000] train=0.0795 val=0.1575 l0=0.0000\n",
      "Epoch [110/1000] train=0.0623 val=0.1275 l0=0.0000\n",
      "Epoch [120/1000] train=0.0373 val=0.1516 l0=0.0000\n",
      "Epoch [130/1000] train=0.0353 val=0.1449 l0=0.0000\n",
      "Epoch [140/1000] train=0.0297 val=0.1607 l0=0.0000\n",
      "Epoch [150/1000] train=0.0194 val=0.1694 l0=0.0000\n",
      "Epoch [160/1000] train=0.0098 val=0.2402 l0=0.0000\n",
      "Epoch [170/1000] train=0.0134 val=0.1905 l0=0.0000\n",
      "Epoch [180/1000] train=0.0062 val=0.2139 l0=0.0000\n",
      "Epoch [190/1000] train=0.0230 val=0.1679 l0=0.0000\n",
      "Epoch [200/1000] train=0.0213 val=0.2157 l0=0.0000\n",
      "Epoch [210/1000] train=0.0164 val=0.1656 l0=0.0000\n",
      "Epoch [220/1000] train=0.0113 val=0.2539 l0=0.0000\n",
      "Epoch [230/1000] train=0.0164 val=0.1782 l0=0.0000\n",
      "Epoch [240/1000] train=0.0549 val=0.2633 l0=0.0000\n",
      "Epoch [250/1000] train=0.0029 val=0.2561 l0=0.0000\n",
      "Epoch [260/1000] train=0.0291 val=0.3138 l0=0.0000\n",
      "Epoch [270/1000] train=0.0014 val=0.2806 l0=0.0000\n",
      "Early stopping at epoch 279 (best val=0.0914)\n",
      "Restored best model (val=0.0914)\n",
      "NousNet (Softmax+Proto) Accuracy: 0.9825 | AUC: 0.9841\n",
      "XGBoost Accuracy: 0.9474 | AUC: 0.9917\n",
      "EBM Accuracy: 0.9649 | AUC: 0.9931\n",
      "MLP Accuracy: 0.9561 | AUC: 0.9960\n",
      "KAN Accuracy: 0.9474 | AUC: 0.9917\n",
      "KAN top-10 global features:\n",
      "  worst texture: 12.9533\n",
      "  worst smoothness: 12.4788\n",
      "  worst radius: 11.0473\n",
      "  worst symmetry: 9.6263\n",
      "  worst concave points: 8.9660\n",
      "  mean texture: 8.4018\n",
      "  worst area: 8.2879\n",
      "  worst perimeter: 6.8817\n",
      "  mean perimeter: 6.4293\n",
      "  mean area: 6.2493\n",
      "\n",
      "— NousNet (Softmax+Proto) explanation (held-out sample) —\n",
      "MODEL: SOFTMAX rules | TASK: CLASSIFICATION\n",
      "SAMPLE PREDICTION: malignant\n",
      "  - Confidence: 0.993\n",
      "  - Ground Truth: malignant\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B1/R1: Δmargin=+4.621 [+] | 0.37 AND + 0.32 OR + 0.31 k-of-n\n",
      "    F15, F8  →  [F15] β( [L−R](x̃) = (0.30·mean area + 0.30·worst texture + 0.27·worst smoothness + 0.23·worst symmetry) − (0.28·smoothness error + 0.25·mean perimeter + 0.21·worst fractal dimension + 0.17·mean concave points) > 0.09; k=0.96, ν=0.71 )  where x̃ are calibrated features | [F8] β( [L−R](x̃) = (0.35·worst area + 0.30·worst smoothness + 0.21·worst compactness + 0.18·compactness error) − (0.34·fractal dimension error + 0.22·smoothness error + 0.16·mean smoothness + 0.15·symmetry error) > 0.06; k=1.00, ν=0.73 )  where x̃ are calibrated features\n",
      "  • B1/R2: Δmargin=+1.582 [+] | 0.34 AND + 0.32 OR + 0.33 k-of-n\n",
      "    F24, F29  →  [F24] β( [L−R](x̃) = (0.29·mean concave points + 0.25·mean area + 0.22·worst perimeter + 0.20·mean texture) − (0.30·symmetry error + 0.29·compactness error + 0.27·worst area + 0.25·mean symmetry) > -0.03; k=0.98, ν=0.74 )  where x̃ are calibrated features | [F29] β( [L−R](x̃) = (0.30·worst concave points + 0.30·worst radius + 0.23·mean radius + 0.23·mean fractal dimension) − (0.27·worst fractal dimension + 0.19·worst texture + 0.16·mean perimeter + 0.15·compactness error) > -0.00; k=0.96, ν=0.70 )  where x̃ are calibrated features\n",
      "  • B1/R5: Δmargin=+1.301 [+] | 0.34 AND + 0.32 OR + 0.33 k-of-n\n",
      "    F10, F6  →  [F10] β( [L−R](x̃) = (0.20·symmetry error + 0.13·concave points error + 0.13·worst texture + 0.10·worst smoothness) − (0.35·mean fractal dimension + 0.29·worst compactness + 0.22·smoothness error + 0.20·mean texture) > 0.00; k=0.97, ν=0.71 )  where x̃ are calibrated features | [F6] β( [L−R](x̃) = (0.34·worst symmetry + 0.34·radius error + 0.29·mean smoothness + 0.27·texture error) − (0.21·worst smoothness + 0.20·mean area + 0.11·mean compactness + 0.11·worst texture) > -0.07; k=1.02, ν=0.68 )  where x̃ are calibrated features\n",
      "  • B1/R4: Δmargin=+0.684 [+] | 0.35 AND + 0.32 OR + 0.33 k-of-n\n",
      "    F17, F4  →  [F17] β( [L−R](x̃) = (0.40·mean symmetry + 0.32·mean fractal dimension + 0.30·texture error + 0.29·worst smoothness) − (0.21·worst concavity + 0.12·worst fractal dimension + 0.11·symmetry error + 0.11·worst texture) > -0.42; k=1.00, ν=0.68 )  where x̃ are calibrated features | [F4] β( [L−R](x̃) = (0.24·mean perimeter + 0.22·mean area + 0.19·mean concavity + 0.18·smoothness error) − (0.31·area error + 0.28·worst area + 0.24·mean smoothness + 0.22·symmetry error) > -0.12; k=0.95, ν=0.67 )  where x̃ are calibrated features\n",
      "  • B2/R8: Δmargin=+0.677 [+] | 0.36 AND + 0.32 OR + 0.32 k-of-n\n",
      "    F7, F3  →  [F7] β( [L−R](x̃) = (0.20·symmetry error + 0.15·mean compactness + 0.10·mean texture + 0.08·smoothness error) − (0.28·worst compactness + 0.28·worst radius + 0.26·worst concave points + 0.21·worst texture) > 0.12; k=0.95, ν=0.69 )  where x̃ are calibrated features | [F3] β( [L−R](x̃) = (0.22·worst smoothness + 0.18·perimeter error + 0.15·mean perimeter + 0.11·mean radius) − (0.44·compactness error + 0.32·concavity error + 0.25·smoothness error + 0.23·worst texture) > 0.18; k=0.98, ν=0.74 )  where x̃ are calibrated features\n",
      "  ... and 11 more active rules.\n",
      "\n",
      "With pruning:\n",
      "MODEL: SOFTMAX rules | TASK: CLASSIFICATION\n",
      "SAMPLE PREDICTION: malignant\n",
      "  - Confidence: 0.993\n",
      "  - Ground Truth: malignant\n",
      "  - Pruning: |act| >= 0.1346 (forward uses pruned activations)\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B1/R1: Δmargin=+4.235 [+] | 0.37 AND + 0.32 OR + 0.31 k-of-n\n",
      "    F15, F8  →  [F15] β( [L−R](x̃) = (0.30·mean area + 0.30·worst texture + 0.27·worst smoothness + 0.23·worst symmetry) − (0.28·smoothness error + 0.25·mean perimeter + 0.21·worst fractal dimension + 0.17·mean concave points) > 0.09; k=0.96, ν=0.71 )  where x̃ are calibrated features | [F8] β( [L−R](x̃) = (0.35·worst area + 0.30·worst smoothness + 0.21·worst compactness + 0.18·compactness error) − (0.34·fractal dimension error + 0.22·smoothness error + 0.16·mean smoothness + 0.15·symmetry error) > 0.06; k=1.00, ν=0.73 )  where x̃ are calibrated features\n",
      "  • B1/R5: Δmargin=+1.186 [+] | 0.34 AND + 0.32 OR + 0.33 k-of-n\n",
      "    F10, F6  →  [F10] β( [L−R](x̃) = (0.20·symmetry error + 0.13·concave points error + 0.13·worst texture + 0.10·worst smoothness) − (0.35·mean fractal dimension + 0.29·worst compactness + 0.22·smoothness error + 0.20·mean texture) > 0.00; k=0.97, ν=0.71 )  where x̃ are calibrated features | [F6] β( [L−R](x̃) = (0.34·worst symmetry + 0.34·radius error + 0.29·mean smoothness + 0.27·texture error) − (0.21·worst smoothness + 0.20·mean area + 0.11·mean compactness + 0.11·worst texture) > -0.07; k=1.02, ν=0.68 )  where x̃ are calibrated features\n",
      "  • B1/R2: Δmargin=+1.162 [+] | 0.34 AND + 0.32 OR + 0.33 k-of-n\n",
      "    F24, F29  →  [F24] β( [L−R](x̃) = (0.29·mean concave points + 0.25·mean area + 0.22·worst perimeter + 0.20·mean texture) − (0.30·symmetry error + 0.29·compactness error + 0.27·worst area + 0.25·mean symmetry) > -0.03; k=0.98, ν=0.74 )  where x̃ are calibrated features | [F29] β( [L−R](x̃) = (0.30·worst concave points + 0.30·worst radius + 0.23·mean radius + 0.23·mean fractal dimension) − (0.27·worst fractal dimension + 0.19·worst texture + 0.16·mean perimeter + 0.15·compactness error) > -0.00; k=0.96, ν=0.70 )  where x̃ are calibrated features\n",
      "  • B2/R8: Δmargin=+0.593 [+] | 0.36 AND + 0.32 OR + 0.32 k-of-n\n",
      "    F7, F3  →  [F7] β( [L−R](x̃) = (0.20·symmetry error + 0.15·mean compactness + 0.10·mean texture + 0.08·smoothness error) − (0.28·worst compactness + 0.28·worst radius + 0.26·worst concave points + 0.21·worst texture) > 0.12; k=0.95, ν=0.69 )  where x̃ are calibrated features | [F3] β( [L−R](x̃) = (0.22·worst smoothness + 0.18·perimeter error + 0.15·mean perimeter + 0.11·mean radius) − (0.44·compactness error + 0.32·concavity error + 0.25·smoothness error + 0.23·worst texture) > 0.18; k=0.98, ν=0.74 )  where x̃ are calibrated features\n",
      "  • B1/R4: Δmargin=+0.581 [+] | 0.35 AND + 0.32 OR + 0.33 k-of-n\n",
      "    F17, F4  →  [F17] β( [L−R](x̃) = (0.40·mean symmetry + 0.32·mean fractal dimension + 0.30·texture error + 0.29·worst smoothness) − (0.21·worst concavity + 0.12·worst fractal dimension + 0.11·symmetry error + 0.11·worst texture) > -0.42; k=1.00, ν=0.68 )  where x̃ are calibrated features | [F4] β( [L−R](x̃) = (0.24·mean perimeter + 0.22·mean area + 0.19·mean concavity + 0.18·smoothness error) − (0.31·area error + 0.28·worst area + 0.24·mean smoothness + 0.22·symmetry error) > -0.12; k=0.95, ν=0.67 )  where x̃ are calibrated features\n",
      "  ... and 7 more active rules.\n",
      "\n",
      "NousNet explanation fidelity metrics: {'base_margin': 4.98133659362793, 'sufficiency_margin': 5.058825492858887, 'comprehensiveness_margin': 1.1172003746032715, 'kept_size': 15.0}\n",
      "\n",
      "Prototype report (top by top1_count):\n",
      " proto primary_class  class_entropy  mean_activation  top1_count\n",
      "     5        benign       0.464957         0.548617         203\n",
      "     8     malignant       0.288673         0.323391          72\n",
      "     0     malignant       0.552131         0.315986          58\n",
      "     7        benign       0.271039         0.548123          29\n",
      "     2        benign       0.635909         0.546277           2\n",
      "\n",
      "Describe best prototype:\n",
      "Prototype #5\n",
      "  Primary class: benign | probs=[0.17599999904632568, 0.8240000009536743] | entropy=0.465\n",
      "  Top rules:\n",
      "    • R8: weight=-0.173 | 0.36 AND + 0.32 OR + 0.32 k-of-n\n",
      "        [F7] β( [L−R](x̃) = (0.20·symmetry error + 0.15·mean compactness + 0.10·mean texture + 0.08·smoothness error) − (0.28·worst compactness + 0.28·worst radius + 0.26·worst concave points + 0.21·worst texture) > 0.12; k=0.95, ν=0.69 )  where x̃ are calibrated features | [F10] β( [L−R](x̃) = (0.20·symmetry error + 0.13·concave points error + 0.13·worst texture + 0.10·worst smoothness) − (0.35·mean fractal dimension + 0.29·worst compactness + 0.22·smoothness error + 0.20·mean texture) > 0.00; k=0.97, ν=0.71 )  where x̃ are calibrated features\n",
      "    • R3: weight=+0.153 | 0.33 AND + 0.33 OR + 0.34 k-of-n\n",
      "        [F3] β( [L−R](x̃) = (0.22·worst smoothness + 0.18·perimeter error + 0.15·mean perimeter + 0.11·mean radius) − (0.44·compactness error + 0.32·concavity error + 0.25·smoothness error + 0.23·worst texture) > 0.18; k=0.98, ν=0.74 )  where x̃ are calibrated features | [F11] β( [L−R](x̃) = (0.38·smoothness error + 0.24·mean fractal dimension + 0.11·worst area + 0.10·concave points error) − (0.35·mean smoothness + 0.31·worst concavity + 0.29·worst compactness + 0.24·worst symmetry) > -0.01; k=1.01, ν=0.68 )  where x̃ are calibrated features\n",
      "    • R4: weight=+0.115 | 0.36 AND + 0.32 OR + 0.32 k-of-n\n",
      "        [F4] β( [L−R](x̃) = (0.24·mean perimeter + 0.22·mean area + 0.19·mean concavity + 0.18·smoothness error) − (0.31·area error + 0.28·worst area + 0.24·mean smoothness + 0.22·symmetry error) > -0.12; k=0.95, ν=0.67 )  where x̃ are calibrated features | [F15] β( [L−R](x̃) = (0.30·mean area + 0.30·worst texture + 0.27·worst smoothness + 0.23·worst symmetry) − (0.28·smoothness error + 0.25·mean perimeter + 0.21·worst fractal dimension + 0.17·mean concave points) > 0.09; k=0.96, ν=0.71 )  where x̃ are calibrated features\n",
      "    • R6: weight=-0.103 | 0.34 AND + 0.33 OR + 0.33 k-of-n\n",
      "        [F4] β( [L−R](x̃) = (0.24·mean perimeter + 0.22·mean area + 0.19·mean concavity + 0.18·smoothness error) − (0.31·area error + 0.28·worst area + 0.24·mean smoothness + 0.22·symmetry error) > -0.12; k=0.95, ν=0.67 )  where x̃ are calibrated features | [F17] β( [L−R](x̃) = (0.40·mean symmetry + 0.32·mean fractal dimension + 0.30·texture error + 0.29·worst smoothness) − (0.21·worst concavity + 0.12·worst fractal dimension + 0.11·symmetry error + 0.11·worst texture) > -0.42; k=1.00, ν=0.68 )  where x̃ are calibrated features\n",
      "    • R7: weight=+0.087 | 0.33 AND + 0.33 OR + 0.34 k-of-n\n",
      "        [F11] β( [L−R](x̃) = (0.38·smoothness error + 0.24·mean fractal dimension + 0.11·worst area + 0.10·concave points error) − (0.35·mean smoothness + 0.31·worst concavity + 0.29·worst compactness + 0.24·worst symmetry) > -0.01; k=1.01, ν=0.68 )  where x̃ are calibrated features | [F13] β( [L−R](x̃) = (0.24·worst compactness + 0.23·radius error + 0.20·mean smoothness + 0.20·worst symmetry) − (0.34·perimeter error + 0.32·mean concave points + 0.24·compactness error + 0.23·area error) > 0.11; k=0.99, ν=0.71 )  where x̃ are calibrated features\n",
      "    • R1: weight=-0.085 | 0.32 AND + 0.34 OR + 0.33 k-of-n\n",
      "        [F10] β( [L−R](x̃) = (0.20·symmetry error + 0.13·concave points error + 0.13·worst texture + 0.10·worst smoothness) − (0.35·mean fractal dimension + 0.29·worst compactness + 0.22·smoothness error + 0.20·mean texture) > 0.00; k=0.97, ν=0.71 )  where x̃ are calibrated features | [F14] β( [L−R](x̃) = (0.23·perimeter error + 0.18·worst area + 0.16·worst perimeter + 0.16·texture error) − (0.37·worst texture + 0.33·worst symmetry + 0.27·worst concavity + 0.24·symmetry error) > -0.11; k=0.98, ν=0.70 )  where x̃ are calibrated features\n",
      "    • R5: weight=-0.016 | 0.33 AND + 0.33 OR + 0.35 k-of-n\n",
      "        [F4] β( [L−R](x̃) = (0.24·mean perimeter + 0.22·mean area + 0.19·mean concavity + 0.18·smoothness error) − (0.31·area error + 0.28·worst area + 0.24·mean smoothness + 0.22·symmetry error) > -0.12; k=0.95, ν=0.67 )  where x̃ are calibrated features | [F17] β( [L−R](x̃) = (0.40·mean symmetry + 0.32·mean fractal dimension + 0.30·texture error + 0.29·worst smoothness) − (0.21·worst concavity + 0.12·worst fractal dimension + 0.11·symmetry error + 0.11·worst texture) > -0.42; k=1.00, ν=0.68 )  where x̃ are calibrated features\n",
      "    • R2: weight=-0.009 | 0.33 AND + 0.34 OR + 0.33 k-of-n\n",
      "        [F8] β( [L−R](x̃) = (0.35·worst area + 0.30·worst smoothness + 0.21·worst compactness + 0.18·compactness error) − (0.34·fractal dimension error + 0.22·smoothness error + 0.16·mean smoothness + 0.15·symmetry error) > 0.06; k=1.00, ν=0.73 )  where x̃ are calibrated features | [F11] β( [L−R](x̃) = (0.38·smoothness error + 0.24·mean fractal dimension + 0.11·worst area + 0.10·concave points error) − (0.35·mean smoothness + 0.31·worst concavity + 0.29·worst compactness + 0.24·worst symmetry) > -0.01; k=1.01, ν=0.68 )  where x̃ are calibrated features\n",
      "\n",
      "Prototype contributions for the sample:\n",
      " proto  distance  activation       w_c  contribution primary_class  entropy\n",
      "     8  0.072268    0.909468  1.783056      1.621632     malignant 0.288673\n",
      "     1  0.341377    0.638736  0.730740      0.466750     malignant 0.486514\n",
      "     6  0.139687    0.832415  0.426624      0.355128     malignant 0.568006\n",
      "     4  0.400800    0.590791  0.307438      0.181632     malignant 0.655642\n",
      "     2  1.988359    0.073467 -1.103203     -0.081049        benign 0.635909\n",
      "\n",
      "— XGBoost SHAP (global) —\n",
      "Top-10 global features by mean |SHAP|:\n",
      "  worst concave points: 1.2656\n",
      "  worst texture: 1.1672\n",
      "  worst perimeter: 0.9879\n",
      "  worst area: 0.9431\n",
      "  area error: 0.9330\n",
      "  worst radius: 0.6889\n",
      "  symmetry error: 0.4951\n",
      "  mean smoothness: 0.4788\n",
      "  mean concave points: 0.4531\n",
      "  worst symmetry: 0.4430\n",
      "\n",
      "— EBM (global & local) —\n",
      "EBM global explain failed: 'overall'\n",
      "Top-10 local EBM contributions:\n",
      "  feature_0020: -2.4820\n",
      "  feature_0023: -2.3937\n",
      "  feature_0022: -2.2196\n",
      "  feature_0021: -2.0377\n",
      "  feature_0007: -1.9706\n",
      "  feature_0013: -1.8536\n",
      "  feature_0027: -1.7474\n",
      "  feature_0000: -1.5524\n",
      "  feature_0003: -1.5130\n",
      "  feature_0002: -1.3557\n",
      "\n",
      "— MLP permutation importance (global) —\n",
      "Top-10 features by permutation importance (ΔAccuracy):\n",
      "  mean texture: 0.0220\n",
      "  worst concavity: 0.0220\n",
      "  worst smoothness: 0.0220\n",
      "  radius error: 0.0220\n",
      "  worst fractal dimension: 0.0110\n",
      "  fractal dimension error: 0.0110\n",
      "  area error: 0.0110\n",
      "  perimeter error: 0.0110\n",
      "  worst texture: 0.0110\n",
      "  mean symmetry: 0.0110\n",
      "\n",
      "====================================================================================================\n",
      "Interpretability Showcase — California Housing (regression)\n",
      "====================================================================================================\n",
      "Epoch [1/1000] train=0.2823 val=0.1977 l0=0.0000 (*)\n",
      "Epoch [2/1000] train=0.1808 val=0.1872 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1787 val=0.1826 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1611 val=0.1683 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1565 val=0.1864 l0=0.0000\n",
      "Epoch [11/1000] train=0.1539 val=0.1607 l0=0.0000 (*)\n",
      "Epoch [16/1000] train=0.1469 val=0.1582 l0=0.0000 (*)\n",
      "Epoch [17/1000] train=0.1479 val=0.1569 l0=0.0000 (*)\n",
      "Epoch [18/1000] train=0.1455 val=0.1506 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1434 val=0.1496 l0=0.0000 (*)\n",
      "Epoch [21/1000] train=0.1425 val=0.1445 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.1399 val=0.1435 l0=0.0000 (*)\n",
      "Epoch [25/1000] train=0.1369 val=0.1408 l0=0.0000 (*)\n",
      "Epoch [26/1000] train=0.1358 val=0.1406 l0=0.0000 (*)\n",
      "Epoch [27/1000] train=0.1366 val=0.1371 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1338 val=0.1368 l0=0.0000 (*)\n",
      "Epoch [35/1000] train=0.1305 val=0.1333 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1296 val=0.1325 l0=0.0000 (*)\n",
      "Epoch [42/1000] train=0.1278 val=0.1311 l0=0.0000 (*)\n",
      "Epoch [44/1000] train=0.1278 val=0.1302 l0=0.0000 (*)\n",
      "Epoch [45/1000] train=0.1260 val=0.1297 l0=0.0000 (*)\n",
      "Epoch [46/1000] train=0.1268 val=0.1295 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.1247 val=0.1295 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1257 val=0.1463 l0=0.0000\n",
      "Epoch [54/1000] train=0.1235 val=0.1285 l0=0.0000 (*)\n",
      "Epoch [60/1000] train=0.1223 val=0.1360 l0=0.0000\n",
      "Epoch [61/1000] train=0.1215 val=0.1277 l0=0.0000 (*)\n",
      "Epoch [62/1000] train=0.1222 val=0.1273 l0=0.0000 (*)\n",
      "Epoch [63/1000] train=0.1223 val=0.1263 l0=0.0000 (*)\n",
      "Epoch [64/1000] train=0.1211 val=0.1254 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1201 val=0.1281 l0=0.0000\n",
      "Epoch [80/1000] train=0.1174 val=0.1281 l0=0.0000\n",
      "Epoch [82/1000] train=0.1169 val=0.1248 l0=0.0000 (*)\n",
      "Epoch [87/1000] train=0.1171 val=0.1248 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1157 val=0.1237 l0=0.0000 (*)\n",
      "Epoch [91/1000] train=0.1155 val=0.1224 l0=0.0000 (*)\n",
      "Epoch [100/1000] train=0.1153 val=0.1237 l0=0.0000\n",
      "Epoch [108/1000] train=0.1148 val=0.1216 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1137 val=0.1212 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1126 val=0.1243 l0=0.0000\n",
      "Epoch [121/1000] train=0.1118 val=0.1200 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1111 val=0.1236 l0=0.0000\n",
      "Epoch [140/1000] train=0.1102 val=0.1222 l0=0.0000\n",
      "Epoch [150/1000] train=0.1091 val=0.1223 l0=0.0000\n",
      "Epoch [160/1000] train=0.1071 val=0.1220 l0=0.0000\n",
      "Epoch [170/1000] train=0.1079 val=0.1223 l0=0.0000\n",
      "Epoch [180/1000] train=0.1062 val=0.1243 l0=0.0000\n",
      "Epoch [190/1000] train=0.1052 val=0.1225 l0=0.0000\n",
      "Epoch [200/1000] train=0.1043 val=0.1215 l0=0.0000\n",
      "Epoch [210/1000] train=0.1027 val=0.1251 l0=0.0000\n",
      "Epoch [220/1000] train=0.1022 val=0.1274 l0=0.0000\n",
      "Epoch [230/1000] train=0.1029 val=0.1216 l0=0.0000\n",
      "Epoch [240/1000] train=0.1011 val=0.1212 l0=0.0000\n",
      "Epoch [250/1000] train=0.1002 val=0.1238 l0=0.0000\n",
      "Epoch [260/1000] train=0.0996 val=0.1232 l0=0.0000\n",
      "Epoch [270/1000] train=0.0989 val=0.1273 l0=0.0000\n",
      "Epoch [280/1000] train=0.0991 val=0.1236 l0=0.0000\n",
      "Epoch [290/1000] train=0.0977 val=0.1240 l0=0.0000\n",
      "Epoch [300/1000] train=0.0975 val=0.1233 l0=0.0000\n",
      "Epoch [310/1000] train=0.0963 val=0.1232 l0=0.0000\n",
      "Epoch [320/1000] train=0.0962 val=0.1269 l0=0.0000\n",
      "Early stopping at epoch 321 (best val=0.1200)\n",
      "Restored best model (val=0.1200)\n",
      "NousNet (Fixed): RMSE=0.5204 MAE=0.3480 R²=0.7933\n",
      "Epoch [1/1000] train=0.3303 val=0.2141 l0=0.0000 (*)\n",
      "Epoch [3/1000] train=0.1941 val=0.1986 l0=0.0000 (*)\n",
      "Epoch [4/1000] train=0.1876 val=0.1890 l0=0.0000 (*)\n",
      "Epoch [6/1000] train=0.1814 val=0.1798 l0=0.0000 (*)\n",
      "Epoch [7/1000] train=0.1730 val=0.1775 l0=0.0000 (*)\n",
      "Epoch [10/1000] train=0.1694 val=0.1775 l0=0.0000\n",
      "Epoch [11/1000] train=0.1622 val=0.1672 l0=0.0000 (*)\n",
      "Epoch [13/1000] train=0.1614 val=0.1652 l0=0.0000 (*)\n",
      "Epoch [20/1000] train=0.1565 val=0.1707 l0=0.0000\n",
      "Epoch [21/1000] train=0.1547 val=0.1568 l0=0.0000 (*)\n",
      "Epoch [22/1000] train=0.1575 val=0.1552 l0=0.0000 (*)\n",
      "Epoch [28/1000] train=0.1494 val=0.1526 l0=0.0000 (*)\n",
      "Epoch [30/1000] train=0.1513 val=0.1530 l0=0.0000\n",
      "Epoch [35/1000] train=0.1514 val=0.1525 l0=0.0000 (*)\n",
      "Epoch [40/1000] train=0.1444 val=0.1470 l0=0.0000 (*)\n",
      "Epoch [44/1000] train=0.1457 val=0.1465 l0=0.0000 (*)\n",
      "Epoch [49/1000] train=0.1415 val=0.1452 l0=0.0000 (*)\n",
      "Epoch [50/1000] train=0.1548 val=0.1496 l0=0.0000\n",
      "Epoch [60/1000] train=0.1382 val=0.1435 l0=0.0000 (*)\n",
      "Epoch [67/1000] train=0.1370 val=0.1419 l0=0.0000 (*)\n",
      "Epoch [70/1000] train=0.1367 val=0.1576 l0=0.0000\n",
      "Epoch [75/1000] train=0.1376 val=0.1418 l0=0.0000 (*)\n",
      "Epoch [76/1000] train=0.1353 val=0.1415 l0=0.0000 (*)\n",
      "Epoch [80/1000] train=0.1377 val=0.1416 l0=0.0000\n",
      "Epoch [81/1000] train=0.1377 val=0.1386 l0=0.0000 (*)\n",
      "Epoch [90/1000] train=0.1387 val=0.1408 l0=0.0000\n",
      "Epoch [100/1000] train=0.1344 val=0.1515 l0=0.0000\n",
      "Epoch [101/1000] train=0.1320 val=0.1373 l0=0.0000 (*)\n",
      "Epoch [104/1000] train=0.1363 val=0.1349 l0=0.0000 (*)\n",
      "Epoch [109/1000] train=0.1297 val=0.1344 l0=0.0000 (*)\n",
      "Epoch [110/1000] train=0.1342 val=0.1689 l0=0.0000\n",
      "Epoch [114/1000] train=0.1303 val=0.1343 l0=0.0000 (*)\n",
      "Epoch [120/1000] train=0.1285 val=0.1340 l0=0.0000 (*)\n",
      "Epoch [121/1000] train=0.1291 val=0.1319 l0=0.0000 (*)\n",
      "Epoch [130/1000] train=0.1301 val=0.1329 l0=0.0000\n",
      "Epoch [131/1000] train=0.1315 val=0.1316 l0=0.0000 (*)\n",
      "Epoch [140/1000] train=0.1257 val=0.1468 l0=0.0000\n",
      "Epoch [145/1000] train=0.1230 val=0.1307 l0=0.0000 (*)\n",
      "Epoch [150/1000] train=0.1234 val=0.1315 l0=0.0000\n",
      "Epoch [153/1000] train=0.1241 val=0.1307 l0=0.0000 (*)\n",
      "Epoch [160/1000] train=0.1218 val=0.1320 l0=0.0000\n",
      "Epoch [165/1000] train=0.1209 val=0.1291 l0=0.0000 (*)\n",
      "Epoch [167/1000] train=0.1214 val=0.1283 l0=0.0000 (*)\n",
      "Epoch [170/1000] train=0.1241 val=0.1289 l0=0.0000\n",
      "Epoch [171/1000] train=0.1218 val=0.1267 l0=0.0000 (*)\n",
      "Epoch [180/1000] train=0.1215 val=0.1362 l0=0.0000\n",
      "Epoch [190/1000] train=0.1209 val=0.1286 l0=0.0000\n",
      "Epoch [200/1000] train=0.1185 val=0.1274 l0=0.0000\n",
      "Epoch [209/1000] train=0.1185 val=0.1264 l0=0.0000 (*)\n",
      "Epoch [210/1000] train=0.1184 val=0.1355 l0=0.0000\n",
      "Epoch [220/1000] train=0.1176 val=0.1305 l0=0.0000\n",
      "Epoch [224/1000] train=0.1171 val=0.1255 l0=0.0000 (*)\n",
      "Epoch [230/1000] train=0.1158 val=0.1261 l0=0.0000\n",
      "Epoch [240/1000] train=0.1169 val=0.1266 l0=0.0000\n",
      "Epoch [242/1000] train=0.1169 val=0.1246 l0=0.0000 (*)\n",
      "Epoch [250/1000] train=0.1171 val=0.1288 l0=0.0000\n",
      "Epoch [260/1000] train=0.1164 val=0.1253 l0=0.0000\n",
      "Epoch [270/1000] train=0.1148 val=0.1388 l0=0.0000\n",
      "Epoch [280/1000] train=0.1158 val=0.1301 l0=0.0000\n",
      "Epoch [290/1000] train=0.1137 val=0.1315 l0=0.0000\n",
      "Epoch [298/1000] train=0.1153 val=0.1244 l0=0.0000 (*)\n",
      "Epoch [300/1000] train=0.1121 val=0.1304 l0=0.0000\n",
      "Epoch [303/1000] train=0.1128 val=0.1238 l0=0.0000 (*)\n",
      "Epoch [310/1000] train=0.1133 val=0.1373 l0=0.0000\n",
      "Epoch [320/1000] train=0.1133 val=0.1432 l0=0.0000\n",
      "Epoch [324/1000] train=0.1131 val=0.1235 l0=0.0000 (*)\n",
      "Epoch [330/1000] train=0.1117 val=0.1244 l0=0.0000\n",
      "Epoch [334/1000] train=0.1113 val=0.1234 l0=0.0000 (*)\n",
      "Epoch [339/1000] train=0.1138 val=0.1232 l0=0.0000 (*)\n",
      "Epoch [340/1000] train=0.1115 val=0.1255 l0=0.0000\n",
      "Epoch [350/1000] train=0.1107 val=0.1316 l0=0.0000\n",
      "Epoch [358/1000] train=0.1095 val=0.1230 l0=0.0000 (*)\n",
      "Epoch [360/1000] train=0.1107 val=0.1273 l0=0.0000\n",
      "Epoch [367/1000] train=0.1104 val=0.1229 l0=0.0000 (*)\n",
      "Epoch [370/1000] train=0.1098 val=0.1268 l0=0.0000\n",
      "Epoch [374/1000] train=0.1091 val=0.1227 l0=0.0000 (*)\n",
      "Epoch [380/1000] train=0.1093 val=0.1262 l0=0.0000\n",
      "Epoch [386/1000] train=0.1095 val=0.1227 l0=0.0000 (*)\n",
      "Epoch [388/1000] train=0.1076 val=0.1225 l0=0.0000 (*)\n",
      "Epoch [390/1000] train=0.1082 val=0.1274 l0=0.0000\n",
      "Epoch [394/1000] train=0.1083 val=0.1221 l0=0.0000 (*)\n",
      "Epoch [400/1000] train=0.1081 val=0.1256 l0=0.0000\n",
      "Epoch [407/1000] train=0.1065 val=0.1212 l0=0.0000 (*)\n",
      "Epoch [410/1000] train=0.1081 val=0.1226 l0=0.0000\n",
      "Epoch [420/1000] train=0.1056 val=0.1246 l0=0.0000\n",
      "Epoch [430/1000] train=0.1064 val=0.1282 l0=0.0000\n",
      "Epoch [440/1000] train=0.1050 val=0.1335 l0=0.0000\n",
      "Epoch [450/1000] train=0.1055 val=0.1228 l0=0.0000\n",
      "Epoch [460/1000] train=0.1047 val=0.1331 l0=0.0000\n",
      "Epoch [470/1000] train=0.1051 val=0.1234 l0=0.0000\n",
      "Epoch [480/1000] train=0.1041 val=0.1332 l0=0.0000\n",
      "Epoch [490/1000] train=0.1033 val=0.1268 l0=0.0000\n",
      "Epoch [500/1000] train=0.1024 val=0.1230 l0=0.0000\n",
      "Epoch [510/1000] train=0.1032 val=0.1286 l0=0.0000\n",
      "Epoch [520/1000] train=0.1016 val=0.1229 l0=0.0000\n",
      "Epoch [530/1000] train=0.1013 val=0.1225 l0=0.0000\n",
      "Epoch [540/1000] train=0.1019 val=0.1224 l0=0.0000\n",
      "Epoch [550/1000] train=0.1013 val=0.1240 l0=0.0000\n",
      "Epoch [560/1000] train=0.1004 val=0.1370 l0=0.0000\n",
      "Epoch [570/1000] train=0.1011 val=0.1262 l0=0.0000\n",
      "Epoch [580/1000] train=0.0993 val=0.1243 l0=0.0000\n",
      "Epoch [590/1000] train=0.0997 val=0.1236 l0=0.0000\n",
      "Epoch [600/1000] train=0.0980 val=0.1216 l0=0.0000\n",
      "Early stopping at epoch 607 (best val=0.1212)\n",
      "Restored best model (val=0.1212)\n",
      "NousNet (Softmax): RMSE=0.5179 MAE=0.3357 R²=0.7953\n",
      "XGBoost: RMSE=0.4376 MAE=0.2838 R²=0.8539\n",
      "EBM: RMSE=0.5720 MAE=0.4005 R²=0.7503\n",
      "MLP: RMSE=0.5212 MAE=0.3413 R²=0.7927\n",
      "KAN: RMSE=0.5345 MAE=0.3604 R²=0.7820\n",
      "KAN top-10 global features:\n",
      "  Longitude: 53.6377\n",
      "  Latitude: 28.2501\n",
      "  AveOccup: 19.2261\n",
      "  HouseAge: 14.0352\n",
      "  MedInc: 13.9088\n",
      "  Population: 13.1772\n",
      "  AveBedrms: 13.1749\n",
      "  AveRooms: 4.2894\n",
      "\n",
      "— NousNet (Softmax) explanation (held-out sample) —\n",
      "MODEL: SOFTMAX rules | TASK: REGRESSION\n",
      "SAMPLE PREDICTION: Value: 0.384\n",
      "  - Ground Truth: 0.477\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B2/R2: Δprediction=-0.367 | 0.01 AND + 0.98 OR + 0.01 k-of-n\n",
      "    F9, F7  →  [F9] β( [L−R](x̃) = (0.63·AveRooms + 0.62·Latitude + 0.52·MedInc + 0.45·AveOccup) − (1.00·Population + 0.79·Longitude + 0.50·HouseAge) > 0.38; k=1.35, ν=0.96 )  where x̃ are calibrated features | [F7] β( [L−R](x̃) = (0.57·Latitude + 0.56·AveOccup + 0.43·Longitude + 0.18·AveBedrms) − (1.80·HouseAge + 0.96·Population + 0.91·AveRooms + 0.66·MedInc) > -0.23; k=1.83, ν=1.03 )  where x̃ are calibrated features\n",
      "  • B2/R10: Δprediction=-0.087 | 0.18 AND + 0.61 OR + 0.22 k-of-n\n",
      "    F18, F22  →  [F18] β( [L−R](x̃) = (0.05·AveBedrms) − (1.01·AveOccup + 0.36·Longitude + 0.31·AveRooms + 0.25·Latitude) > -0.31; k=0.92, ν=0.65 )  where x̃ are calibrated features | [F22] β( [L−R](x̃) = (0.25·Latitude + 0.22·Longitude + 0.15·Population + 0.09·MedInc) − (1.28·AveOccup + 0.42·HouseAge + 0.22·AveBedrms) > -0.28; k=1.02, ν=0.70 )  where x̃ are calibrated features\n",
      "  • B2/R3: Δprediction=-0.083 | 0.03 AND + 0.93 OR + 0.04 k-of-n\n",
      "    F5, F15  →  [F5] β( [L−R](x̃) = (0.37·AveOccup + 0.26·AveBedrms + 0.23·HouseAge) − (4.07·Population + 0.73·Latitude + 0.66·AveRooms + 0.10·MedInc) > 0.22; k=1.85, ν=1.08 )  where x̃ are calibrated features | [F15] β( [L−R](x̃) = (0.39·Population + 0.25·MedInc) − (1.19·AveOccup + 0.24·HouseAge + 0.15·AveRooms + 0.13·Longitude) > -0.27; k=0.99, ν=0.68 )  where x̃ are calibrated features\n",
      "  • B2/R15: Δprediction=+0.058 | 0.61 AND + 0.18 OR + 0.21 k-of-n\n",
      "    F22, F24  →  [F22] β( [L−R](x̃) = (0.25·Latitude + 0.22·Longitude + 0.15·Population + 0.09·MedInc) − (1.28·AveOccup + 0.42·HouseAge + 0.22·AveBedrms) > -0.28; k=1.02, ν=0.70 )  where x̃ are calibrated features | [F24] β( [L−R](x̃) = (0.82·AveOccup + 0.59·AveRooms + 0.08·HouseAge) − (1.38·AveBedrms + 1.18·Population + 0.82·MedInc + 0.11·Longitude) > -0.35; k=1.43, ν=0.95 )  where x̃ are calibrated features\n",
      "  • B2/R4: Δprediction=+0.044 | 0.18 AND + 0.55 OR + 0.28 k-of-n\n",
      "    F11, F7  →  [F11] β( [L−R](x̃) = (0.39·Population + 0.35·MedInc + 0.32·AveRooms + 0.16·AveOccup) − (1.98·Longitude + 0.36·Latitude + 0.35·AveBedrms + 0.16·HouseAge) > -0.16; k=1.05, ν=0.77 )  where x̃ are calibrated features | [F7] β( [L−R](x̃) = (0.57·Latitude + 0.56·AveOccup + 0.43·Longitude + 0.18·AveBedrms) − (1.80·HouseAge + 0.96·Population + 0.91·AveRooms + 0.66·MedInc) > -0.23; k=1.83, ν=1.03 )  where x̃ are calibrated features\n",
      "  ... and 11 more active rules.\n",
      "\n",
      "With pruning:\n",
      "MODEL: SOFTMAX rules | TASK: REGRESSION\n",
      "SAMPLE PREDICTION: Value: 0.473\n",
      "  - Ground Truth: 0.477\n",
      "  - Pruning: |act| >= 0.2345 (forward uses pruned activations)\n",
      "------------------------------------------------------------\n",
      "CAUSAL RULE IMPACT (Top 5):\n",
      "  • B2/R2: Δprediction=-0.422 | 0.01 AND + 0.98 OR + 0.01 k-of-n\n",
      "    F9, F7  →  [F9] β( [L−R](x̃) = (0.63·AveRooms + 0.62·Latitude + 0.52·MedInc + 0.45·AveOccup) − (1.00·Population + 0.79·Longitude + 0.50·HouseAge) > 0.38; k=1.35, ν=0.96 )  where x̃ are calibrated features | [F7] β( [L−R](x̃) = (0.57·Latitude + 0.56·AveOccup + 0.43·Longitude + 0.18·AveBedrms) − (1.80·HouseAge + 0.96·Population + 0.91·AveRooms + 0.66·MedInc) > -0.23; k=1.83, ν=1.03 )  where x̃ are calibrated features\n",
      "  • B2/R3: Δprediction=-0.142 | 0.03 AND + 0.93 OR + 0.04 k-of-n\n",
      "    F5, F15  →  [F5] β( [L−R](x̃) = (0.37·AveOccup + 0.26·AveBedrms + 0.23·HouseAge) − (4.07·Population + 0.73·Latitude + 0.66·AveRooms + 0.10·MedInc) > 0.22; k=1.85, ν=1.08 )  where x̃ are calibrated features | [F15] β( [L−R](x̃) = (0.39·Population + 0.25·MedInc) − (1.19·AveOccup + 0.24·HouseAge + 0.15·AveRooms + 0.13·Longitude) > -0.27; k=0.99, ν=0.68 )  where x̃ are calibrated features\n",
      "  • B2/R14: Δprediction=-0.046 | 0.73 AND + 0.14 OR + 0.14 k-of-n\n",
      "    F18, F15  →  [F18] β( [L−R](x̃) = (0.05·AveBedrms) − (1.01·AveOccup + 0.36·Longitude + 0.31·AveRooms + 0.25·Latitude) > -0.31; k=0.92, ν=0.65 )  where x̃ are calibrated features | [F15] β( [L−R](x̃) = (0.39·Population + 0.25·MedInc) − (1.19·AveOccup + 0.24·HouseAge + 0.15·AveRooms + 0.13·Longitude) > -0.27; k=0.99, ν=0.68 )  where x̃ are calibrated features\n",
      "  • B2/R7: Δprediction=-0.043 | 0.09 AND + 0.83 OR + 0.08 k-of-n\n",
      "    F31, F19  →  [F31] β( [L−R](x̃) = (0.19·MedInc + 0.15·AveBedrms + 0.06·Population) − (1.18·AveOccup + 0.34·AveRooms + 0.24·Longitude + 0.20·HouseAge) > -0.31; k=0.97, ν=0.67 )  where x̃ are calibrated features | [F19] β( [L−R](x̃) = (0.26·Latitude + 0.14·HouseAge) − (0.85·AveOccup + 0.52·AveRooms + 0.51·MedInc + 0.51·Population) > 0.20; k=0.83, ν=0.82 )  where x̃ are calibrated features\n",
      "  • B1/R14: Δprediction=+0.018 | 0.27 AND + 0.34 OR + 0.40 k-of-n\n",
      "    F21, F19  →  [F21] β( [L−R](x̃) = (0.58·AveOccup + 0.57·Latitude + 0.39·AveRooms + 0.35·Longitude) − (0.58·AveBedrms + 0.14·Population + 0.12·MedInc) > 0.37; k=0.89, ν=0.87 )  where x̃ are calibrated features | [F19] β( [L−R](x̃) = (0.26·Latitude + 0.14·HouseAge) − (0.85·AveOccup + 0.52·AveRooms + 0.51·MedInc + 0.51·Population) > 0.20; k=0.83, ν=0.82 )  where x̃ are calibrated features\n",
      "  ... and 2 more active rules.\n",
      "\n",
      "— XGBoost SHAP (global) —\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 251/256 [00:13<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 global features by mean |SHAP|:\n",
      "  Latitude: 0.7412\n",
      "  Longitude: 0.6361\n",
      "  MedInc: 0.3137\n",
      "  AveRooms: 0.1703\n",
      "  AveOccup: 0.1684\n",
      "  HouseAge: 0.0513\n",
      "  Population: 0.0345\n",
      "  AveBedrms: 0.0318\n",
      "\n",
      "— EBM (global & local) —\n",
      "EBM global explain failed: 'overall'\n",
      "Top-10 local EBM contributions:\n",
      "  feature_0000: -0.8318\n",
      "  feature_0006: -0.4197\n",
      "  feature_0005: -0.3253\n",
      "  feature_0007: -0.1860\n",
      "  feature_0002: +0.0268\n",
      "  feature_0003: -0.0196\n",
      "  feature_0004: -0.0118\n",
      "  feature_0001: +0.0087\n",
      "\n",
      "— MLP permutation importance —\n",
      "Top-10 features by permutation importance (ΔRMSE positive = worse when permuted):\n",
      "  Population: -0.0372\n",
      "  HouseAge: -0.0997\n",
      "  AveOccup: -0.2666\n",
      "  MedInc: -0.4529\n",
      "  AveBedrms: -0.6337\n",
      "  AveRooms: -0.7554\n",
      "  Longitude: -1.0230\n",
      "  Latitude: -1.1624\n"
     ]
    }
   ],
   "source": [
    "# HELOC\n",
    "try:\n",
    "    Xh, yh, f_h, c_h = load_heloc()\n",
    "    showcase_interpretability_classification(Xh, yh, f_h, c_h, dataset_tag=\"HELOC\")\n",
    "except Exception as e:\n",
    "    print(\"HELOC showcase skipped:\", e)\n",
    "\n",
    "# Adult\n",
    "try:\n",
    "    Xa, ya, f_a, c_a = load_adult()\n",
    "    showcase_interpretability_classification(Xa, ya, f_a, c_a, dataset_tag=\"Adult\")\n",
    "except Exception as e:\n",
    "    print(\"Adult showcase skipped:\", e)\n",
    "\n",
    "# Breast Cancer\n",
    "try:\n",
    "    Xb, yb, f_b, c_b = load_breast_cancer_ds()\n",
    "    showcase_interpretability_classification(Xb, yb, f_b, c_b, dataset_tag=\"Breast Cancer\")\n",
    "except Exception as e:\n",
    "    print(\"Breast Cancer showcase skipped:\", e)\n",
    "\n",
    "# California Housing\n",
    "try:\n",
    "    Xc, yc, f_c = load_california()\n",
    "    showcase_interpretability_regression(Xc, yc, f_c, dataset_tag=\"California Housing\")\n",
    "except Exception as e:\n",
    "    print(\"California showcase skipped:\", e)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-15T20:15:46.809499",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
